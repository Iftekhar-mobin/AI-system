{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '/home/iftekhar/AI-system/retrieval_Model/Page_Ranking_Experiment/methods_collection/')\n",
    "import ranking_help_methods \n",
    "import vectorization\n",
    "import sequence_handler\n",
    "import corpus_handling_methods\n",
    "import pandas as pd\n",
    "import ranking\n",
    "import re\n",
    "import make_question as question_maker\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "TFIDF_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# dataset = pd.read_csv(\"/home/iftekhar/amiebot/Resources/amiebot_dataset/mobicontrol_ver14.csv\")\n",
    "# dataset = dataset.rename(columns={\"text\": \"Data\", \"page\": \"PageID\"})\n",
    "# dataset = corpus_handling_methods.corpus_urlmerger(dataset)\n",
    "# dataset.Data = dataset.Data.apply(lambda x: corpus_handling_methods.mecab_tokenization(x))\n",
    "# dataset.Data = dataset.Data.apply(lambda x: corpus_handling_methods.cleaner(x))\n",
    "# dataset.Data = dataset.Data.apply(lambda x: corpus_handling_methods.single_character_remover(x))\n",
    "# dataset.to_csv('tokenized_cleaned_perpagedata.csv') \n",
    "# dataset.head()\n",
    "\n",
    "# dataset = pd.read_csv(\"tokenized_cleaned_perpagedata.csv\")\n",
    "\n",
    "# # dataset.head()\n",
    "dataset = pd.read_csv(\"../../../Helpers/Title_link_merged_corpus.csv\")\n",
    "dataset = dataset.iloc[:,2:]\n",
    "dataset = dataset.rename(columns={\"Article\": \"Data\"})\n",
    "\n",
    "\n",
    "\n",
    "# URL matcher query with the corpus\n",
    "# for index, col in ques_df.iterrows():\n",
    "#     for m_index, m_col in dataset.iterrows():\n",
    "#         if(str(col['URL']) == str(m_col['URL'])):\n",
    "#             print(col['URL'], m_col['PageID'])\n",
    "#             break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cus_ques = pd.read_csv(\"/home/iftekhar/amiebot/Resources/amiebot_dataset/user_query.csv\")\n",
    "cus_ques.Question = cus_ques.Question.apply(lambda x: corpus_handling_methods.mecab_tokenization(x))\n",
    "cus_ques.Question = cus_ques.Question.apply(lambda x: corpus_handling_methods.single_character_remover(x))\n",
    "cus_ques.Question = cus_ques.Question.apply(lambda x: corpus_handling_methods.cleaner(x))\n",
    "# cus_ques.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>PageID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ドメイン れ いる 赤色 表示 どう</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>れ た 情報 時点 位置 どっち</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>パラメータ 右端 クリック 接続 一 どちら</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>参照 サーバ 情報 受領 始め 何が</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>サーバ 端末 直接 配布 サイレント どうして</td>\n",
       "      <td>437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Question  PageID\n",
       "0       ドメイン れ いる 赤色 表示 どう     356\n",
       "1         れ た 情報 時点 位置 どっち     383\n",
       "2   パラメータ 右端 クリック 接続 一 どちら     283\n",
       "3       参照 サーバ 情報 受領 始め 何が     234\n",
       "4  サーバ 端末 直接 配布 サイレント どうして     437"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_corpus = corpus_handling_methods.corpus_split(dataset, 6)\n",
    "# split_corpus.head()\n",
    "perpage_dataset = corpus_handling_methods.corpus_per_page(split_corpus)\n",
    "# perpage_dataset.head() \n",
    "\n",
    "sample_size = 30\n",
    "questions_samples = question_maker.question_dataframe_generator_1000(split_corpus, sample_size)\n",
    "questions_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = vectorization.vector_fit(TFIDF_vectorizer, split_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[375, ['端末 音 鳴らし 削除 端末 MobiControl', '音 鳴らし 削除 端末 MobiControl 登録']]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_list[1][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['電話 帳', '電話 帳 配信', '帳 配信', '帳 配信 行い', '配信 行い']\n",
      "[393]\n",
      "failed to get ID 電話 帳 配信 行い 279\n",
      "[393]\n"
     ]
    }
   ],
   "source": [
    "sample_count = 0\n",
    "sum_score = 0\n",
    "container = []\n",
    "extra_ranks = 0\n",
    "for index, col in cus_ques.iterrows():\n",
    "    # print(col['Question'], col['PageID'])\n",
    "    query = str(col['Question'])\n",
    "    question_parts = question_maker.making_query_collection(query)\n",
    "    collector = sequence_handler.sequence_searcher(perpage_dataset, question_parts)\n",
    "    print(question_parts)\n",
    "\n",
    "    if not collector:\n",
    "        doc = nlp(query)\n",
    "        for ent in doc.ents:\n",
    "            collector = sequence_searcher(perpage_dataset, [ent.text])\n",
    "    print(collector)\n",
    "    result = []\n",
    "    res = []\n",
    "    for pages_id in collector:\n",
    "        page_data = perpage_dataset[(perpage_dataset['PageID']==pages_id)].Data.values\n",
    "        result.append([sequence_matcher(query_token_remover(query), str(page_data)), pages_id])\n",
    "\n",
    "    for items, ids in result:\n",
    "        contents = [single_character_remover(' '.join(items))]\n",
    "        contents = [temp.strip(' ') for temp in contents]\n",
    "        #print(contents)\n",
    "        if len(' '.join(contents).split()) > 1:\n",
    "            res.append(ids)\n",
    "            \n",
    "    sorted_list = get_sequence_match_ID(res, split_corpus, query)\n",
    "    \n",
    "    temp = sequence_handler.chunks_id_collectors(sorted_list, col['Question'], col['PageID'])\n",
    "    if temp:\n",
    "        print(temp)\n",
    "    \n",
    "    break\n",
    "    \n",
    "    \n",
    "#     perpage_sequence_match = sequence_handler.perpage_sequence_match(collector, \n",
    "#                                         perpage_dataset, split_corpus, query)\n",
    "#     ranks = ranking.crude_ranks(perpage_sequence_match, query, vec)\n",
    "#     print(col['PageID'], \"<=>\", ranks)\n",
    "    \n",
    "# #     if filtered_ranks[0][1] < 0.75:\n",
    "# #         print(\"Calling Pretrained Model\")\n",
    "        \n",
    "#     page_answers = []\n",
    "#     prediction_scores = []\n",
    "#     for ids, score in ranks:\n",
    "#         page_answers.append(ids)\n",
    "#         prediction_scores.append(score)\n",
    "#     MRR_score = ranking.mean_reciprocal_rank_score(col['PageID'], page_answers)\n",
    "#     sum_score += MRR_score\n",
    "#     container.append([MRR_score, col['PageID'], page_answers, prediction_scores, col['Question']])\n",
    "#     sample_count += 1\n",
    "    \n",
    "# result = pd.DataFrame(container, columns=['score', 'actual_answer',\n",
    "# 'page_answers', 'prediction_scores', 'query'])\n",
    "# result.to_csv('seq_matcher_TFIDF_Vectorizer_performance.csv')\n",
    "# score = sum_score/sample_count\n",
    "# score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ランチャー って   156\n",
      "['ランチャー って']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-fe72635f2a3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mquestion_parts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquestion_maker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaking_query_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_parts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcollector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_searcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperpage_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_parts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI-system/retrieval_Model/Page_Ranking_Experiment/pipelines/sequence_handler.py\u001b[0m in \u001b[0;36msequence_searcher\u001b[0;34m(corpus_per_page, question_parts)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcollector\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mcollector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_searcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperpage_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query' is not defined"
     ]
    }
   ],
   "source": [
    "sample_count = 0\n",
    "sum_score = 0\n",
    "container = []\n",
    "extra_ranks = 0\n",
    "for index, col in cus_ques.iterrows():\n",
    "    print(col['Question'], col['PageID'])\n",
    "    query = str(col['Question'])\n",
    "    question_parts = question_maker.making_query_collection(query)\n",
    "    print(question_parts)\n",
    "    collector = sequence_handler.sequence_searcher(perpage_dataset, question_parts, query)\n",
    "    print(collector)\n",
    "    break\n",
    "    \n",
    "    perpage_sequence_match = sequence_handler.perpage_sequence_match(collector, \n",
    "                                        perpage_dataset, split_corpus, query)\n",
    "    \n",
    "    ranks = ranking.crude_ranks(perpage_sequence_match, query, vec)\n",
    "    \n",
    "    print(col['PageID'], \"<=>\", ranks)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249 <=> [(374, array([0.67309471])), (440, array([0.67309471])), (233, array([0.53527663]))]\n",
      "149 <=> [(248, array([0.78843629])), (149, array([0.7114791])), (178, array([0.67613208]))]\n",
      "386 <=> [(1, array([0.98763105])), (430, array([0.98763105])), (367, array([0.9804484]))]\n",
      "368 <=> [(368, array([0.44340855])), (43, array([0.41988964])), (43, array([0.41635997]))]\n",
      "271 <=> [(206, array([0.57344961])), (238, array([0.54471293])), (238, array([0.54471293]))]\n",
      "172 <=> [(95, array([0.58037142])), (138, array([0.57503223])), (231, array([0.5672848]))]\n",
      "252 <=> [(238, array([0.42753095])), (238, array([0.42134464])), (29, array([0.40430138]))]\n",
      "271 <=> [(1, array([0.82196468])), (430, array([0.82196468])), (271, array([0.73772403]))]\n",
      "156 <=> [(156, array([0.81134015])), (156, array([0.63235936])), (187, array([0.62457641]))]\n",
      "230 <=> [(238, array([0.60244209])), (258, array([0.56850784])), (6, array([0.51495109]))]\n",
      "369 <=> [(369, array([0.80485996])), (431, array([0.80485996])), (368, array([0.7911098]))]\n",
      "386 <=> [(386, array([0.82272593])), (387, array([0.82139534])), (367, array([0.75864386]))]\n",
      "239 <=> [(239, array([0.88655624])), (179, array([0.88285979])), (239, array([0.69652439]))]\n",
      "156 <=> []\n",
      "12 <=> [(393, array([0.51001265])), (393, array([0.37770964])), (1, array([0.35721304]))]\n",
      "252 <=> [(11, array([0.76112994])), (425, array([0.76112994])), (386, array([0.73810427]))]\n",
      "171 <=> [(171, array([0.97701941])), (285, array([0.87728442])), (343, array([0.87728442]))]\n",
      "271 <=> []\n",
      "239 <=> []\n",
      "178 <=> [(248, array([0.62525707])), (149, array([0.56422738])), (178, array([0.53619597]))]\n",
      "45 <=> [(132, array([0.75469464])), (90, array([0.71568605])), (91, array([0.71568605]))]\n",
      "367 <=> [(386, array([0.74591752])), (386, array([0.72316734])), (385, array([0.51469481]))]\n",
      "357 <=> [(31, array([0.84740391])), (31, array([0.74513027])), (357, array([0.72611553]))]\n",
      "387 <=> [(43, array([0.79844996])), (385, array([0.63427472])), (387, array([0.63427472]))]\n",
      "386 <=> [(385, array([0.76585456])), (387, array([0.76585456])), (386, array([0.73505327]))]\n",
      "270 <=> [(1, array([0.83756421])), (430, array([0.83756421])), (270, array([0.79289532]))]\n",
      "234 <=> [(90, array([0.73814901])), (91, array([0.73814901])), (234, array([0.73814901]))]\n",
      "260 <=> [(359, array([0.42171094])), (367, array([0.40310522])), (248, array([0.34866878]))]\n",
      "367 <=> [(375, array([0.90329781])), (375, array([0.80156868])), (375, array([0.73135311]))]\n",
      "239 <=> [(239, array([0.95672857])), (229, array([0.94383936])), (382, array([0.94235634]))]\n",
      "244 <=> [(392, array([0.46182888])), (392, array([0.45723324])), (25, array([0.44432397]))]\n",
      "40 <=> [(8, array([0.49761045])), (61, array([0.41503396])), (368, array([0.37541696]))]\n",
      "37 <=> [(35, array([0.42522922])), (35, array([0.31030522])), (37, array([0.19934123]))]\n",
      "201 <=> [(148, array([0.71965096])), (177, array([0.71965096])), (202, array([0.71965096]))]\n",
      "27 <=> [(161, array([0.607319])), (116, array([0.49233078])), (161, array([0.49233078]))]\n",
      "375 <=> [(375, array([0.73456969])), (375, array([0.70681419])), (7, array([0.3208624]))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.30944444444444447"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample_size = 100\n",
    "# questions_samples = question_maker.question_dataframe_generator_1000(split_corpus, sample_size)\n",
    "# questions_samples.head()\n",
    "\n",
    "sample_count = 0\n",
    "sum_score = 0\n",
    "container = []\n",
    "extra_ranks = 0\n",
    "for index, col in cus_ques.iterrows():\n",
    "    # print(col['Question'], col['PageID'])\n",
    "    query = str(col['Question'])\n",
    "    question_parts = question_maker.making_query_collection(query)\n",
    "    collector = sequence_handler.sequence_searcher(perpage_dataset, question_parts)\n",
    "    perpage_sequence_match = sequence_handler.perpage_sequence_match(collector, \n",
    "                                        perpage_dataset, split_corpus, query)\n",
    "    \n",
    "    ranks = ranking.crude_ranks(perpage_sequence_match, query, vec)\n",
    "    \n",
    "    print(col['PageID'], \"<=>\", ranks)\n",
    "    \n",
    "#     if filtered_ranks[0][1] < 0.75:\n",
    "#         print(\"Calling Pretrained Model\")\n",
    "        \n",
    "    page_answers = []\n",
    "    prediction_scores = []\n",
    "    for ids, score in ranks:\n",
    "        page_answers.append(ids)\n",
    "        prediction_scores.append(score)\n",
    "    MRR_score = ranking.mean_reciprocal_rank_score(col['PageID'], page_answers)\n",
    "    sum_score += MRR_score\n",
    "    container.append([MRR_score, col['PageID'], page_answers, prediction_scores, col['Question']])\n",
    "    sample_count += 1\n",
    "    \n",
    "result = pd.DataFrame(container, columns=['score', 'actual_answer',\n",
    "'page_answers', 'prediction_scores', 'query'])\n",
    "result.to_csv('seq_matcher_TFIDF_Vectorizer_performance.csv')\n",
    "score = sum_score/sample_count\n",
    "score\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

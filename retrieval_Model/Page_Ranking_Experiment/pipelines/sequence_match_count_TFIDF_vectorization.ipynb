{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'methods_collection'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-16f768e56d42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/home/iftekhar/AI-system/retrieval_Model/Page_Ranking_Experiment/methods_collection/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mranking_help_methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvectorization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msequence_handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI-system/retrieval_Model/Page_Ranking_Experiment/methods_collection/ranking_help_methods.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#from nltk.tokenize import word_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmethods_collection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'methods_collection'"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '/home/iftekhar/AI-system/retrieval_Model/Page_Ranking_Experiment/methods_collection/')\n",
    "import ranking_help_methods \n",
    "import vectorization\n",
    "import sequence_handler\n",
    "import corpus_handling_methods\n",
    "import pandas as pd\n",
    "import ranking\n",
    "import re\n",
    "import make_question as question_maker\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "TFIDF_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# dataset = pd.read_csv(\"/home/iftekhar/amiebot/Resources/amiebot_dataset/mobicontrol_ver14.csv\")\n",
    "# dataset = dataset.rename(columns={\"text\": \"Data\", \"page\": \"PageID\"})\n",
    "# dataset = corpus_handling_methods.corpus_urlmerger(dataset)\n",
    "# dataset.Data = dataset.Data.apply(lambda x: corpus_handling_methods.mecab_tokenization(x))\n",
    "# dataset.Data = dataset.Data.apply(lambda x: corpus_handling_methods.cleaner(x))\n",
    "# dataset.Data = dataset.Data.apply(lambda x: corpus_handling_methods.single_character_remover(x))\n",
    "# dataset.to_csv('tokenized_cleaned_perpagedata.csv') \n",
    "# dataset.head()\n",
    "\n",
    "# dataset = pd.read_csv(\"tokenized_cleaned_perpagedata.csv\")\n",
    "\n",
    "# # dataset.head()\n",
    "dataset = pd.read_csv(\"../../../Helpers/Title_link_merged_corpus.csv\")\n",
    "dataset = dataset.iloc[:,2:]\n",
    "dataset = dataset.rename(columns={\"Article\": \"Data\"})\n",
    "\n",
    "\n",
    "\n",
    "# URL matcher query with the corpus\n",
    "# for index, col in ques_df.iterrows():\n",
    "#     for m_index, m_col in dataset.iterrows():\n",
    "#         if(str(col['URL']) == str(m_col['URL'])):\n",
    "#             print(col['URL'], m_col['PageID'])\n",
    "#             break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cus_ques = pd.read_csv(\"/home/iftekhar/amiebot/Resources/amiebot_dataset/user_query.csv\")\n",
    "cus_ques.Question = cus_ques.Question.apply(lambda x: corpus_handling_methods.mecab_tokenization(x))\n",
    "cus_ques.Question = cus_ques.Question.apply(lambda x: corpus_handling_methods.single_character_remover(x))\n",
    "cus_ques.Question = cus_ques.Question.apply(lambda x: corpus_handling_methods.cleaner(x))\n",
    "# cus_ques.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>PageID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>構文 enablesystemapp &lt; アプリ バンドル どんな</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>画像 ファイル 保存 フォルダ パス どの</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>殆ど 設定 項目 適用 れ 使うには</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>画面 三 本 線 ン どうですか</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>から 直近 チェックイン 時刻 経過 何が</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Question  PageID\n",
       "0  構文 enablesystemapp < アプリ バンドル どんな     243\n",
       "1              画像 ファイル 保存 フォルダ パス どの     349\n",
       "2                 殆ど 設定 項目 適用 れ 使うには     258\n",
       "3                   画面 三 本 線 ン どうですか      38\n",
       "4              から 直近 チェックイン 時刻 経過 何が      30"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_corpus = corpus_handling_methods.corpus_split(dataset, 6)\n",
    "# split_corpus.head()\n",
    "perpage_dataset = corpus_handling_methods.corpus_per_page(split_corpus)\n",
    "# perpage_dataset.head() \n",
    "\n",
    "sample_size = 30\n",
    "questions_samples = question_maker.question_dataframe_generator_1000(split_corpus, sample_size)\n",
    "questions_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-48808b4a6588>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msplit_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'split_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "split_corpus.Data.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = vectorization.vector_fit(TFIDF_vectorizer, split_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ランチャー って']\n",
      "failed to get ID ランチャー って   156\n",
      "[2, 6, 7, 15, 26, 28, 32, 44, 61, 75, 97, 107, 140, 151, 175, 179, 191, 200, 223, 231, 238, 240, 243, 247, 292, 367, 374, 393, 423, 428, 438, 440]\n"
     ]
    }
   ],
   "source": [
    "sample_count = 0\n",
    "sum_score = 0\n",
    "container = []\n",
    "extra_ranks = 0\n",
    "for index, col in cus_ques.iterrows():\n",
    "    # print(col['Question'], col['PageID'])\n",
    "    query = str(col['Question'])\n",
    "    question_parts = question_maker.making_query_collection(query)\n",
    "    collector = sequence_handler.sequence_searcher(perpage_dataset, question_parts, query)\n",
    "    print(question_parts)\n",
    "    perpage_sequence_match = sequence_handler.perpage_sequence_match(collector, \n",
    "                                    perpage_dataset, split_corpus, query)\n",
    "\n",
    "#     result = []\n",
    "#     res = []\n",
    "#     for pages_id in collector:\n",
    "#         page_data = perpage_dataset[(perpage_dataset['PageID']==pages_id)].Data.values\n",
    "#         result.append([sequence_matcher(query_token_remover(query), str(page_data)), pages_id])\n",
    "\n",
    "#     for items, ids in result:\n",
    "#         contents = [single_character_remover(' '.join(items))]\n",
    "#         contents = [temp.strip(' ') for temp in contents]\n",
    "#         #print(contents)\n",
    "#         if len(' '.join(contents).split()) > 1:\n",
    "#             res.append(ids)\n",
    "            \n",
    "#     sorted_list = get_sequence_match_ID(res, split_corpus, query)\n",
    "    \n",
    "    temp = sequence_handler.chunks_id_collectors(perpage_sequence_match, col['Question'], col['PageID'])\n",
    "    if temp:\n",
    "        print(temp)\n",
    "    \n",
    "    break\n",
    "    \n",
    "    \n",
    "#     perpage_sequence_match = sequence_handler.perpage_sequence_match(collector, \n",
    "#                                         perpage_dataset, split_corpus, query)\n",
    "#     ranks = ranking.crude_ranks(perpage_sequence_match, query, vec)\n",
    "#     print(col['PageID'], \"<=>\", ranks)\n",
    "    \n",
    "# #     if filtered_ranks[0][1] < 0.75:\n",
    "# #         print(\"Calling Pretrained Model\")\n",
    "        \n",
    "#     page_answers = []\n",
    "#     prediction_scores = []\n",
    "#     for ids, score in ranks:\n",
    "#         page_answers.append(ids)\n",
    "#         prediction_scores.append(score)\n",
    "#     MRR_score = ranking.mean_reciprocal_rank_score(col['PageID'], page_answers)\n",
    "#     sum_score += MRR_score\n",
    "#     container.append([MRR_score, col['PageID'], page_answers, prediction_scores, col['Question']])\n",
    "#     sample_count += 1\n",
    "    \n",
    "# result = pd.DataFrame(container, columns=['score', 'actual_answer',\n",
    "# 'page_answers', 'prediction_scores', 'query'])\n",
    "# result.to_csv('seq_matcher_TFIDF_Vectorizer_performance.csv')\n",
    "# score = sum_score/sample_count\n",
    "# score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"ja_ginza\")\n",
    "doc = nlp('ランチャー って')   \n",
    "for items in doc.ents:\n",
    "    print(items.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ランチャー って   156\n",
      "['ランチャー って']\n",
      "156 <=> [[151, 0.5480341106129084], [179, 0.5436338204668313], [97, 0.5158551294160274]]\n"
     ]
    }
   ],
   "source": [
    "sample_count = 0\n",
    "sum_score = 0\n",
    "container = []\n",
    "extra_ranks = 0\n",
    "for index, col in cus_ques.iterrows():\n",
    "    print(col['Question'], col['PageID'])\n",
    "    query = str(col['Question'])\n",
    "    question_parts = question_maker.making_query_collection(query)\n",
    "    print(question_parts)\n",
    "    \n",
    "#     collector = []\n",
    "#     for index, col in perpage_dataset.iterrows():\n",
    "#         for items in question_parts:\n",
    "#              if re.search(items, col['Data']):\n",
    "#                 collector.append(col['PageID'])\n",
    "#     collector = list(set(collector))\n",
    "#     print(collector, len(collector))\n",
    "#     if not collector:\n",
    "#         print(\"Hi\")\n",
    "#         doc = nlp(query)\n",
    "#         print(doc.ents)\n",
    "#         for ent in doc.ents:\n",
    "#             print([ent.text])\n",
    "#             collector = sequence_searcher(perpage_dataset, [ent.text])    \n",
    "# #         if len(collector) < 1:\n",
    "# #             for items in query.split():\n",
    "                \n",
    "    collector = sequence_handler.sequence_searcher(perpage_dataset, question_parts, query)\n",
    "#     print(collector)\n",
    "#     break\n",
    "    \n",
    "    perpage_sequence_match = sequence_handler.perpage_sequence_match(collector, \n",
    "                                        perpage_dataset, split_corpus, query)\n",
    "#     result = []\n",
    "#     res = []\n",
    "#     for pages_id in collector:\n",
    "#         page_data = perpage_dataset[(perpage_dataset['PageID']==pages_id)].Data.values\n",
    "#         result.append([sequence_matcher(query_token_remover(query), str(page_data)), pages_id])\n",
    "# #     print(result)\n",
    "#     for items, ids in result:\n",
    "#         contents = [single_character_remover(' '.join(items))]\n",
    "#         contents = [temp.strip(' ') for temp in contents]\n",
    "# #     print(contents)\n",
    "#         if len(' '.join(contents).split()) > 1:\n",
    "#             res.append(ids)\n",
    "#     #print(res)       \n",
    "# #     return get_sequence_match_ID(res, split_corpus, query)\n",
    "    \n",
    "#     q_parts = making_query_collection(query)\n",
    "#     if len(q_parts) < 2: q_parts = query.split()\n",
    "\n",
    "#     match_collection = []\n",
    "#     for ids in res:\n",
    "#         page_data = split_corpus[(split_corpus['PageID']==ids)].Data.values\n",
    "#         for lines in page_data:\n",
    "#             for parts in q_parts:\n",
    "#                 if lines.find(parts) !=-1:\n",
    "#                     match_collection.append([ids, lines])\n",
    "# #     print(match_collection)\n",
    "#     sorted_list = get_unique_2Dlist(match_collection)\n",
    "#     print(sorted_list)\n",
    "#     print(perpage_sequence_match)\n",
    "#     break\n",
    "    ranks = ranking.crude_ranks(perpage_sequence_match, query, vec)\n",
    "    \n",
    "    print(col['PageID'], \"<=>\", ranks)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import difflib\n",
    "from utility import get_unique_2Dlist\n",
    "from corpus_handling_methods import query_token_remover, single_character_remover\n",
    "import sys\n",
    "sys.path.insert(0,\n",
    "'/home/iftekhar/AI-system/retrieval_Model/Page_Ranking_Experiment/methods_collection/')\n",
    "from make_question import making_query_collection\n",
    "import spacy\n",
    "nlp = spacy.load(\"ja_ginza\")\n",
    "\n",
    "def sequence_searcher(corpus_per_page, question_parts, query):\n",
    "    collector = []\n",
    "    for index, col in corpus_per_page.iterrows():\n",
    "        for items in question_parts:\n",
    "             if re.search(items, col['Data']):\n",
    "                collector.append(col['PageID'])\n",
    "    collector = list(set(collector))\n",
    "    \n",
    "    if not collector:\n",
    "        doc = nlp(query)\n",
    "        for ent in doc.ents:\n",
    "            collector = sequence_searcher(corpus_per_page, [ent.text], query) \n",
    "        if not collector:\n",
    "            collector = sequence_searcher(corpus_per_page, query.split(), query)\n",
    "    return collector\n",
    "\n",
    "def sequence_matcher(sequence1, sequence2):\n",
    "    matcher = difflib.SequenceMatcher(None, sequence1, sequence2)\n",
    "    matches = matcher.get_matching_blocks()\n",
    "    matching_result_collection = []\n",
    "    for match in matches:\n",
    "        if len(sequence1[match.a:match.a + match.size]) > 0:\n",
    "            matching_result_collection.append(sequence1[match.a:match.a + match.size])\n",
    "    return matching_result_collection\n",
    "\n",
    "\n",
    "def perpage_sequence_match(collector, perpage_dataset, split_corpus, query):\n",
    "    result = []\n",
    "    res = []\n",
    "    for pages_id in collector:\n",
    "        page_data = perpage_dataset[(perpage_dataset['PageID']==pages_id)].Data.values\n",
    "        result.append([sequence_matcher(query_token_remover(query), str(page_data)), pages_id])\n",
    "\n",
    "    for items, ids in result:\n",
    "        contents = [single_character_remover(' '.join(items))]\n",
    "        contents = [temp.strip(' ') for temp in contents]\n",
    "        #print(contents)\n",
    "        if len(' '.join(contents).split()) > 1:\n",
    "            res.append(ids)\n",
    "            \n",
    "    return get_sequence_match_ID(res, split_corpus, query)\n",
    "\n",
    "def get_sequence_match_ID(res, split_corpus, query):\n",
    "    q_parts = making_query_collection(query)\n",
    "    if len(q_parts) < 2: q_parts = query.split()\n",
    "\n",
    "    match_collection = []\n",
    "    for ids in res:\n",
    "        page_data = split_corpus[(split_corpus['PageID']==ids)].Data.values\n",
    "        for lines in page_data:\n",
    "            for parts in q_parts:\n",
    "                if lines.find(parts) !=-1:\n",
    "                    match_collection.append([ids, lines])\n",
    "    sorted_list = get_unique_2Dlist(match_collection)\n",
    "    return sorted_list\n",
    "\n",
    "def chunks_id_collectors(sorted_list, Question, PageID):\n",
    "    ids_list = []\n",
    "    end_tag = len(sorted_list)\n",
    "    for ids, items in sorted_list:\n",
    "        ids_list.append(ids)\n",
    "        if(ids == PageID):\n",
    "            print(\"Booster having IDs\", Question, PageID)\n",
    "            break\n",
    "        elif(end_tag<2):\n",
    "            print(\"failed to get ID\", Question, PageID)\n",
    "            return ids_list        \n",
    "        end_tag -=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cus_ques' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1855b255940e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mextra_ranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcus_ques\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# print(col['Question'], col['PageID'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cus_ques' is not defined"
     ]
    }
   ],
   "source": [
    "# sample_size = 100\n",
    "# questions_samples = question_maker.question_dataframe_generator_1000(split_corpus, sample_size)\n",
    "# questions_samples.head()\n",
    "\n",
    "sample_count = 0\n",
    "sum_score = 0\n",
    "container = []\n",
    "extra_ranks = 0\n",
    "for index, col in cus_ques.iterrows():\n",
    "    # print(col['Question'], col['PageID'])\n",
    "    query = str(col['Question'])\n",
    "    question_parts = question_maker.making_query_collection(query)\n",
    "    collector = sequence_handler.sequence_searcher(perpage_dataset, question_parts)\n",
    "    perpage_sequence_match = sequence_handler.perpage_sequence_match(collector, \n",
    "                                        perpage_dataset, split_corpus, query)\n",
    "    \n",
    "    ranks = ranking.crude_ranks(perpage_sequence_match, query, vec)\n",
    "    \n",
    "    print(col['PageID'], \"<=>\", ranks)\n",
    "    \n",
    "#     if filtered_ranks[0][1] < 0.75:\n",
    "#         print(\"Calling Pretrained Model\")\n",
    "        \n",
    "    page_answers = []\n",
    "    prediction_scores = []\n",
    "    for ids, score in ranks:\n",
    "        page_answers.append(ids)\n",
    "        prediction_scores.append(score)\n",
    "    MRR_score = ranking.mean_reciprocal_rank_score(col['PageID'], page_answers)\n",
    "    sum_score += MRR_score\n",
    "    container.append([MRR_score, col['PageID'], page_answers, prediction_scores, col['Question']])\n",
    "    sample_count += 1\n",
    "    \n",
    "result = pd.DataFrame(container, columns=['score', 'actual_answer',\n",
    "'page_answers', 'prediction_scores', 'query'])\n",
    "result.to_csv('seq_matcher_TFIDF_Vectorizer_performance.csv')\n",
    "score = sum_score/sample_count\n",
    "score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_duplicate_ids(ranks):\n",
    "    temp = 0\n",
    "    ids_list = []\n",
    "    filtered = []\n",
    "    for page_id, score in ranks:\n",
    "        if page_id == temp:\n",
    "            continue\n",
    "        else:\n",
    "            ids_list.append(page_id)\n",
    "            filtered.append([page_id, score[0]])\n",
    "            temp = page_id\n",
    "    return filtered, ids_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b66aa5be38d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m368\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.44340855\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m43\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.41988964\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m43\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.41635997\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'array' is not defined"
     ]
    }
   ],
   "source": [
    "ranks = [(368, array([0.44340855])), (43, array([0.41988964])), (43, array([0.41635997]))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Demo\n",
    "######################################################################\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus.reader import WordNetCorpusReader, WordNetICCorpusReader\n",
    "\n",
    "def demo():\n",
    "    import nltk\n",
    "    print('loading wordnet')\n",
    "    wn = WordNetCorpusReader(nltk.data.find('corpora/wordnet'), None)\n",
    "    print('done loading')\n",
    "    S = wn.synset\n",
    "    L = wn.lemma\n",
    "\n",
    "    print('getting a synset for go')\n",
    "    move_synset = S('go.v.21')\n",
    "    print(move_synset.name(), move_synset.pos(), move_synset.lexname())\n",
    "    print(move_synset.lemma_names())\n",
    "    print(move_synset.definition())\n",
    "    print(move_synset.examples())\n",
    "\n",
    "    zap_n = ['zap.n.01']\n",
    "    zap_v = ['zap.v.01', 'zap.v.02', 'nuke.v.01', 'microwave.v.01']\n",
    "\n",
    "    def _get_synsets(synset_strings):\n",
    "        return [S(synset) for synset in synset_strings]\n",
    "\n",
    "    zap_n_synsets = _get_synsets(zap_n)\n",
    "    zap_v_synsets = _get_synsets(zap_v)\n",
    "\n",
    "    print(zap_n_synsets)\n",
    "    print(zap_v_synsets)\n",
    "\n",
    "    print(\"Navigations:\")\n",
    "    print(S('travel.v.01').hypernyms())\n",
    "    print(S('travel.v.02').hypernyms())\n",
    "    print(S('travel.v.03').hypernyms())\n",
    "\n",
    "    print(L('zap.v.03.nuke').derivationally_related_forms())\n",
    "    print(L('zap.v.03.atomize').derivationally_related_forms())\n",
    "    print(L('zap.v.03.atomise').derivationally_related_forms())\n",
    "    print(L('zap.v.03.zap').derivationally_related_forms())\n",
    "\n",
    "    print(S('dog.n.01').member_holonyms())\n",
    "    print(S('dog.n.01').part_meronyms())\n",
    "\n",
    "    print(S('breakfast.n.1').hypernyms())\n",
    "    print(S('meal.n.1').hyponyms())\n",
    "    print(S('Austen.n.1').instance_hypernyms())\n",
    "    print(S('composer.n.1').instance_hyponyms())\n",
    "\n",
    "    print(S('faculty.n.2').member_meronyms())\n",
    "    print(S('copilot.n.1').member_holonyms())\n",
    "\n",
    "    print(S('table.n.2').part_meronyms())\n",
    "    print(S('course.n.7').part_holonyms())\n",
    "\n",
    "    print(S('water.n.1').substance_meronyms())\n",
    "    print(S('gin.n.1').substance_holonyms())\n",
    "\n",
    "    # 以下３行は、エラーが出る\n",
    "    # print(L('leader.n.1.leader'))  # Have some error possibly in data?\n",
    "    # print(L('leader.n.1.leader').antonyms())\n",
    "    # print(L('increase.v.1.increase').antonyms())\n",
    "\n",
    "    print(S('snore.v.1').entailments())\n",
    "    print(S('heavy.a.1').similar_tos())\n",
    "    print(S('light.a.1').attributes())\n",
    "    print(S('heavy.a.1').attributes())\n",
    "\n",
    "    # print(L('English.a.1.English').pertainyms())   # Same error as above\n",
    "\n",
    "    print(S('person.n.01').root_hypernyms())\n",
    "    print(S('sail.v.01').root_hypernyms())\n",
    "    print(S('fall.v.12').root_hypernyms())\n",
    "\n",
    "    print(S('person.n.01').lowest_common_hypernyms(S('dog.n.01')))\n",
    "    print(S('woman.n.01').lowest_common_hypernyms(S('girlfriend.n.02')))\n",
    "\n",
    "    print(S('dog.n.01').path_similarity(S('cat.n.01')))\n",
    "    print(S('dog.n.01').lch_similarity(S('cat.n.01')))\n",
    "    print(S('dog.n.01').wup_similarity(S('cat.n.01')))\n",
    "\n",
    "    wnic = WordNetICCorpusReader(nltk.data.find('corpora/wordnet_ic'),\n",
    "                                 '.*\\.dat')\n",
    "    ic = wnic.ic('ic-brown.dat')\n",
    "    print(S('dog.n.01').jcn_similarity(S('cat.n.01'), ic))\n",
    "\n",
    "    ic = wnic.ic('ic-semcor.dat')\n",
    "    print(S('dog.n.01').lin_similarity(S('cat.n.01'), ic))\n",
    "\n",
    "    print(S('code.n.03').topic_domains())\n",
    "    print(S('pukka.a.01').region_domains())\n",
    "    print(S('freaky.a.01').usage_domains())\n",
    "\n",
    "\n",
    "demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "# wn.synsets('鯨', lang='jpn')\n",
    "# wn.synset('spy.n.01').lemma_names('jpn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.lemmas('犬', lang='jpn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('dog.n.01.犬'), Lemma('spy.n.01.犬')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemmas('犬', lang='jpn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym_list = [u.lemma_names('jpn') for u in list(wn.synsets('犬', lang='jpn'))]\n",
    "\n",
    "synonyms = []\n",
    "for items in sym_list:\n",
    "    for syms in items:\n",
    "        synonyms.append(syms)\n",
    "\n",
    "\n",
    "list(set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [[['アクト', '営み', '営為', '活動', '行', '行い', '行ない', '行動', '行為'], ['仕業', '挙止', '振舞', '振る舞い', '振舞い', '為業', '行い', '行ない', '行動', '行為'], ['品行', '態度', '振る舞い', '振舞い', '素行', '行い', '行ない', '行儀', '行動', '行為'], ['ふるまい', '仕打ち', '仕方', '態度', '所作', '振舞', '振る舞い', '振舞い', '素行', '行い', '行ない', '行儀', '行為', '行状', '言動', '身性', '身持', '身持ち', '身状']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['アクト', '営み', '営為', '活動', '行', '行い', '行ない', '行動', '行為']),\n",
       "       list(['仕業', '挙止', '振舞', '振る舞い', '振舞い', '為業', '行い', '行ない', '行動', '行為']),\n",
       "       list(['品行', '態度', '振る舞い', '振舞い', '素行', '行い', '行ない', '行儀', '行動', '行為']),\n",
       "       list(['ふるまい', '仕打ち', '仕方', '態度', '所作', '振舞', '振る舞い', '振舞い', '素行', '行い', '行ない', '行儀', '行為', '行状', '言動', '身性', '身持', '身持ち', '身状'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "l = np.asarray(l)\n",
    "l.squeeze()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

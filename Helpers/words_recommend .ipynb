{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import MeCab\n",
    "import re\n",
    "import ast\n",
    "#mecab = MeCab.Tagger('-Owakati')\n",
    "from pathlib import Path\n",
    "import difflib\n",
    "# import spacy\n",
    "# nlp = spacy.load('ja_ginza')\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(\"very_cleaned_dataset_mobicontrol.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist = corpus.Data.str.split(expand=True).stack().value_counts()\n",
    "import json\n",
    "file = open(\"/home/iftekhar/amiebot/exp_amiecore/amieCore/amie_core/core/retriever/Page_Ranking_Experiment/pipelines/words_frequency_distribution.json\", \"w\", encoding='utf-8')\n",
    "json.dump(freq_dist.to_dict(), file, ensure_ascii=False)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "context = corpus.Data.values.tolist()\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(context)\n",
    "#print(vectorizer.get_feature_names())\n",
    "# ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
    "# print(X.shape)\n",
    "# (4, 9)\n",
    "\n",
    "cvec_counts = vectorizer.transform(context)\n",
    "\n",
    "occ = np.asarray(cvec_counts.sum(axis=0)).ravel().tolist()\n",
    "counts_df = pd.DataFrame({'term': vectorizer.get_feature_names(), 'occurrences': occ})\n",
    "counts_df.sort_values(by='occurrences', ascending=False).head(20)\n",
    "\n",
    "import numpy as np\n",
    "weights = np.asarray(X.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': vectorizer.get_feature_names(), 'weight': weights})\n",
    "weights_df.sort_values(by='weight', ascending=False).head(20)\n",
    "\n",
    "print('sparse matrix shape:', cvec_counts.shape)\n",
    "print('nonzero count:', cvec_counts.nnz)\n",
    "print('sparsity: %.2f%%' % (100.0 * cvec_counts.nnz / (cvec_counts.shape[0] * cvec_counts.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashTag Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>url</th>\n",
       "      <th>PageID</th>\n",
       "      <th>Analysis</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 1014</th>\n",
       "      <th>Unnamed: 1015</th>\n",
       "      <th>Unnamed: 1016</th>\n",
       "      <th>Unnamed: 1017</th>\n",
       "      <th>Unnamed: 1018</th>\n",
       "      <th>Unnamed: 1019</th>\n",
       "      <th>Unnamed: 1020</th>\n",
       "      <th>Unnamed: 1021</th>\n",
       "      <th>Unnamed: 1022</th>\n",
       "      <th>Unnamed: 1023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iMessage 利用 禁止</td>\n",
       "      <td>https://pol-japan.co.jp/products/help/v14/ios_...</td>\n",
       "      <td>271</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ウィルス 対策 実施</td>\n",
       "      <td>https://pol-japan.co.jp/products/help/v14/plus...</td>\n",
       "      <td>149  178</td>\n",
       "      <td>ウィルス対策を実施したい</td>\n",
       "      <td>https://pol-japan.co.jp/products/help/v14/plus...</td>\n",
       "      <td>149  178</td>\n",
       "      <td>ウィルス対策を実施したい</td>\n",
       "      <td>https://pol-japan.co.jp/products/help/v14/plus...</td>\n",
       "      <td>149  178</td>\n",
       "      <td>ウィルス対策を実施したい</td>\n",
       "      <td>...</td>\n",
       "      <td>ウィルス対策を実施したい</td>\n",
       "      <td>https://pol-japan.co.jp/products/help/v14/plus...</td>\n",
       "      <td>149  178</td>\n",
       "      <td>ウィルス対策を実施したい</td>\n",
       "      <td>https://pol-japan.co.jp/products/help/v14/plus...</td>\n",
       "      <td>149  178</td>\n",
       "      <td>ウィルス対策を実施したい</td>\n",
       "      <td>https://pol-japan.co.jp/products/help/v14/plus...</td>\n",
       "      <td>149  178</td>\n",
       "      <td>ウィルス対策を実施したい</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>デフォルト 禁止 いる アプリ 復活</td>\n",
       "      <td>https://pol-japan.co.jp/products/help/v14/linu...</td>\n",
       "      <td>399  271  335</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>端末 初期</td>\n",
       "      <td>https://pol-japan.co.jp/products/help/v14/plus...</td>\n",
       "      <td>239</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ウィルス 対策 実施</td>\n",
       "      <td>https://pol-japan.co.jp/products/help/v14/plus...</td>\n",
       "      <td>149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Question                                                url  \\\n",
       "0      iMessage 利用 禁止  https://pol-japan.co.jp/products/help/v14/ios_...   \n",
       "1          ウィルス 対策 実施  https://pol-japan.co.jp/products/help/v14/plus...   \n",
       "2  デフォルト 禁止 いる アプリ 復活  https://pol-japan.co.jp/products/help/v14/linu...   \n",
       "3               端末 初期  https://pol-japan.co.jp/products/help/v14/plus...   \n",
       "4          ウィルス 対策 実施  https://pol-japan.co.jp/products/help/v14/plus...   \n",
       "\n",
       "          PageID      Analysis  \\\n",
       "0            271           NaN   \n",
       "1       149  178  ウィルス対策を実施したい   \n",
       "2  399  271  335           NaN   \n",
       "3            239           NaN   \n",
       "4            149           NaN   \n",
       "\n",
       "                                          Unnamed: 4 Unnamed: 5    Unnamed: 6  \\\n",
       "0                                                NaN        NaN           NaN   \n",
       "1  https://pol-japan.co.jp/products/help/v14/plus...   149  178  ウィルス対策を実施したい   \n",
       "2                                                NaN        NaN           NaN   \n",
       "3                                                NaN        NaN           NaN   \n",
       "4                                                NaN        NaN           NaN   \n",
       "\n",
       "                                          Unnamed: 7 Unnamed: 8    Unnamed: 9  \\\n",
       "0                                                NaN        NaN           NaN   \n",
       "1  https://pol-japan.co.jp/products/help/v14/plus...   149  178  ウィルス対策を実施したい   \n",
       "2                                                NaN        NaN           NaN   \n",
       "3                                                NaN        NaN           NaN   \n",
       "4                                                NaN        NaN           NaN   \n",
       "\n",
       "   ... Unnamed: 1014                                      Unnamed: 1015  \\\n",
       "0  ...           NaN                                                NaN   \n",
       "1  ...  ウィルス対策を実施したい  https://pol-japan.co.jp/products/help/v14/plus...   \n",
       "2  ...           NaN                                                NaN   \n",
       "3  ...           NaN                                                NaN   \n",
       "4  ...           NaN                                                NaN   \n",
       "\n",
       "  Unnamed: 1016 Unnamed: 1017  \\\n",
       "0           NaN           NaN   \n",
       "1      149  178  ウィルス対策を実施したい   \n",
       "2           NaN           NaN   \n",
       "3           NaN           NaN   \n",
       "4           NaN           NaN   \n",
       "\n",
       "                                       Unnamed: 1018 Unnamed: 1019  \\\n",
       "0                                                NaN           NaN   \n",
       "1  https://pol-japan.co.jp/products/help/v14/plus...      149  178   \n",
       "2                                                NaN           NaN   \n",
       "3                                                NaN           NaN   \n",
       "4                                                NaN           NaN   \n",
       "\n",
       "  Unnamed: 1020                                      Unnamed: 1021  \\\n",
       "0           NaN                                                NaN   \n",
       "1  ウィルス対策を実施したい  https://pol-japan.co.jp/products/help/v14/plus...   \n",
       "2           NaN                                                NaN   \n",
       "3           NaN                                                NaN   \n",
       "4           NaN                                                NaN   \n",
       "\n",
       "  Unnamed: 1022 Unnamed: 1023  \n",
       "0           NaN           NaN  \n",
       "1      149  178  ウィルス対策を実施したい  \n",
       "2           NaN           NaN  \n",
       "3           NaN           NaN  \n",
       "4           NaN           NaN  \n",
       "\n",
       "[5 rows x 1024 columns]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cus_ques = query_corpus_processing(\n",
    "    \"/home/iftekhar/amiebot/Resources/amiebot_dataset/support_team_question_pure.csv\")\n",
    "cus_ques.head()  \n",
    "# cus_ques.to_csv(\"support_teams_query.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/iftekhar/amiebot/exp_amiecore/amieCore/amie_core/core/retriever/Page_Ranking_Experiment'\n",
    "          '/pipelines/vocabulary.txt') as f:\n",
    "    vocabulary = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_substrings(string):\n",
    "    n = len(string)\n",
    "    return {string[i:j+1] for i in range(n) for j in range(i,n)}\n",
    "\n",
    "\n",
    "def query_corpus_processing(corpus):\n",
    "    cus_ques = pd.read_csv(corpus)\n",
    "    cus_ques.Question = cus_ques.Question.apply(lambda x: mecab_tokenization(x))\n",
    "    cus_ques.Question = cus_ques.Question.apply(lambda x: single_character_remover(x))\n",
    "    cus_ques.Question = cus_ques.Question.apply(lambda x: cleaner(x))\n",
    "    return cus_ques\n",
    "\n",
    "def mecab_tokenization(text):\n",
    "    q = mecab.parse(text)\n",
    "    q_parts = q.split()\n",
    "    return ' '.join([word for word in q_parts if not word in get_stop_word_ja()])\n",
    "\n",
    "\n",
    "def single_character_remover(text):\n",
    "    collector = []\n",
    "    for items in text.split():\n",
    "        if len(items) < 2:\n",
    "            replaced = re.sub(r'[ぁ-んァ-ン]', '', items)\n",
    "            replaced = re.sub(r'[A-Za-z]', '', replaced)\n",
    "            replaced = re.sub(r'[0-9]', '', replaced)\n",
    "            collector.append(replaced)\n",
    "        else:\n",
    "            collector.append(items)\n",
    "\n",
    "    return ' '.join([temp.strip(' ') for temp in collector])\n",
    "\n",
    "def cleaner(text):\n",
    "    collector = []\n",
    "    for items in text.split():\n",
    "        cleaned = clean_text(items)\n",
    "        cleaned = re.sub(r\"\\s+\", '', cleaned)\n",
    "        if cleaned is not '' or cleaned is not ' ':\n",
    "            collector.append(clean_text(items))\n",
    "\n",
    "    return ' '.join(collector)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    replaced = text.replace(\"\\\\\", \"\")\n",
    "    replaced = replaced.replace(\"+\", \"\")\n",
    "    replaced = re.sub('_', '', replaced)\n",
    "    replaced = re.sub('\\W+', ' ', replaced)\n",
    "    replaced = re.sub(r'￥', '', replaced)  # 【】の除去\n",
    "    replaced = re.sub(r'．', '', replaced)  # ・ の除去\n",
    "    replaced = re.sub(r'｣', '', replaced)  # （）の除去\n",
    "    replaced = re.sub(r'｢', '', replaced)  # ［］の除去\n",
    "    replaced = re.sub(r'～', '', replaced)  # メンションの除去\n",
    "    replaced = re.sub(r'｜', '', replaced)  # URLの除去\n",
    "    replaced = re.sub(r'＠', '', replaced)  # 全角空白の除去\n",
    "    replaced = re.sub(r'？', '', replaced)  # 数字の除去\n",
    "    replaced = re.sub(r'％', '', replaced)\n",
    "    replaced = re.sub(r'＝', '', replaced)\n",
    "    replaced = re.sub(r'！', '', replaced)\n",
    "    replaced = re.sub(r'｝', '', replaced)\n",
    "    replaced = re.sub(r'：', '', replaced)\n",
    "    replaced = re.sub(r'－', '', replaced)\n",
    "    replaced = re.sub(r'･', '', replaced)\n",
    "    replaced = re.sub(r'ｔ', '', replaced)\n",
    "    replaced = re.sub(r'ｋ', '', replaced)\n",
    "    replaced = re.sub(r'ｄ', '', replaced)\n",
    "    replaced = re.sub(r'\\d+', '', replaced)\n",
    "\n",
    "    return replaced\n",
    "\n",
    "def longest_seq_search(query, page_data):\n",
    "    m = len(query)\n",
    "    n = len(page_data)\n",
    "    counter = [[0] * (n + 1) for x in range(m + 1)]\n",
    "    longest = 0\n",
    "    lcs_set = set()\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if query[i] == page_data[j]:\n",
    "                c = counter[i][j] + 1\n",
    "                counter[i + 1][j + 1] = c\n",
    "                if c > longest:\n",
    "                    lcs_set = set()\n",
    "                    longest = c\n",
    "                    lcs_set.add(query[i - c + 1:i + 1])\n",
    "                elif c == longest:\n",
    "                    lcs_set.add(query[i - c + 1:i + 1])\n",
    "\n",
    "    return lcs_set\n",
    "\n",
    "def get_stop_word_ja():\n",
    "    stop_word_file = Path(\"/home/iftekhar/AI-system/Helpers/stop_word_ja.txt\")\n",
    "    with open(stop_word_file, encoding='utf-8') as f:\n",
    "        stop_word_list = f.read().splitlines()\n",
    "    return stop_word_list\n",
    "\n",
    "def corpus_split(corpus, sentence_length):\n",
    "    labels = corpus.PageID.unique()\n",
    "    lines = []\n",
    "    all_ids = []\n",
    "    for i in list(labels):\n",
    "        text_list = corpus[corpus.PageID == i].Data.values\n",
    "        split_text = fixed_length_sentence(' '.join(text_list), sentence_length)\n",
    "        ids = [i] * len(split_text)\n",
    "        lines += split_text\n",
    "        all_ids += ids\n",
    "    split_corpus = pd.DataFrame(zip(lines, all_ids), columns=[\"Data\", \"PageID\"])\n",
    "    return split_corpus\n",
    "\n",
    "def fixed_length_sentence(contents, word_limit):\n",
    "    contents_list = contents.split()\n",
    "    end = len(contents_list)\n",
    "    count = 0\n",
    "    collector = []\n",
    "    line = []\n",
    "    for items in contents_list:\n",
    "        if count < word_limit - 1 and end > 1:\n",
    "            collector.append(items)\n",
    "            count += 1\n",
    "        else:\n",
    "            collector.append(items)\n",
    "            line.append(' '.join(collector))\n",
    "            collector = []\n",
    "            count = 0\n",
    "        end -= 1\n",
    "    return line\n",
    "\n",
    "\n",
    "def split_joint_word(text):\n",
    "    pattern = re.compile(\"[A-Z]\")\n",
    "    index_saver = []\n",
    "    start = -1\n",
    "    while True:\n",
    "        m = pattern.search(text, start + 1) \n",
    "        if m == None:\n",
    "            break\n",
    "        start = m.start()\n",
    "        index_saver.append(start)\n",
    "    \n",
    "    sorted_list = sorted(index_saver)\n",
    "    range_list=list(range(min(index_saver), max(index_saver)+1))\n",
    "    if sorted_list != range_list:\n",
    "        temp = 0\n",
    "        flag = False\n",
    "        save = []\n",
    "        for indexes in index_saver:\n",
    "            if flag: \n",
    "                if indexes - temp > 1:\n",
    "                    save.append(indexes)\n",
    "                    temp = indexes\n",
    "            else:\n",
    "                save.append(indexes)\n",
    "                temp = indexes\n",
    "                flag = True\n",
    "\n",
    "        if len(save) > 1:\n",
    "            chunk = text[save[0]:save[1]]\n",
    "            return chunk, single_character_remover(text.replace(chunk, ''))\n",
    "        else:\n",
    "            return None, None\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def english_joint_word_handler(text):\n",
    "    saver = []\n",
    "    while text:\n",
    "        temp = text\n",
    "        chunk, text = split_joint_word(text)\n",
    "        saver.append(chunk)\n",
    "    saver.append(temp)\n",
    "    saver.remove(None)\n",
    "    if len(saver) < 2:\n",
    "        saver = []\n",
    "    return saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Matched:  期限切れ , Noun detected:  期限切れ\n",
      "Closest ['期限', '切れ', '途切れ', '切れる']\n",
      "max_term:  期限\n",
      "Not Matched:  Enterise , Noun detected:  Enterise\n",
      "Closest ['Enterprise', 'Enerprise', 'Entrust', 'OneDrive', 'Internet']\n",
      "max_term:  Enter\n"
     ]
    }
   ],
   "source": [
    "##### Optional argument n (default 3) is the maximum number of close matches to return; \n",
    "# Optional argument cutoff (default 0.6) is a float in the range [0, 1]. \n",
    "\n",
    "for index, col in cus_ques.iterrows():\n",
    "    for items in col['Question'].split():\n",
    "        if (items.find('MobiControl')==-1) and re.match(r'[A-Za-z]', items):\n",
    "            chunks_words = english_joint_word_handler(items)\n",
    "#             print(chunks_words)\n",
    "            if chunks_words:\n",
    "                for words in chunks_words:\n",
    "                    end_flag = len(vocabulary)\n",
    "                    for voc in vocabulary:\n",
    "                        if words == voc:\n",
    "            #                 print(\"matched: \", items)\n",
    "                            break\n",
    "                        elif end_flag < 2:\n",
    "                            doc = nlp(words)\n",
    "                            for np in doc.noun_chunks:\n",
    "                                print(\"Not Matched: \", words, \", Noun detected: \", np)\n",
    "\n",
    "                            best_matches = difflib.get_close_matches(words, vocabulary, n = 5, cutoff = 0.5)\n",
    "                            print(\"Closest\", best_matches)\n",
    "\n",
    "                            longest_content = []\n",
    "                            for content in best_matches: \n",
    "                                longest_content.append(max(all_substrings(content) & all_substrings(items), key=len))\n",
    "                            max_term = max(longest_content, key=len)\n",
    "                            print('max_term: ', max_term)\n",
    "                            #sequences_list = available_sequences(corpus, max_term)\n",
    "                            #print(sequences_list)\n",
    "\n",
    "                            # break\n",
    "                        end_flag -= 1\n",
    "                    # break            \n",
    "                end_flag -= 1\n",
    "            # break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_tag_provider(matched, token_query_word):\n",
    "    tags = []\n",
    "    # print(matched)\n",
    "    for items in matched:\n",
    "        for match in re.finditer(r'# (.*) #', items):\n",
    "            tags.append(items[match.start()+1: match.end()].split('#'))\n",
    "    all_tag = []\n",
    "    for tag_chunk in tags:\n",
    "        for tag in tag_chunk:\n",
    "            if tag is not '':\n",
    "                all_tag.append(tag.strip())\n",
    "    unique_tags = list(OrderedDict.fromkeys(sorted(all_tag, key=all_tag.count, reverse=True)))\n",
    "    unique_tags = list(set(unique_tags))\n",
    "    # print(unique_tags)\n",
    "    \n",
    "    try:\n",
    "        if len(unique_tags) > 1:\n",
    "            unique_tags.remove(token_query_word)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return unique_tags\n",
    "\n",
    "def query_in_middle_position(text, match):\n",
    "    chunk = text[match.start() - 20: match.end() + 20]\n",
    "    chunk_list = chunk.split()\n",
    "    chunk_list.pop(0)\n",
    "    chunk_list.pop(-1)\n",
    "    return chunk_list\n",
    "\n",
    "def unique_recommended_all_tags(pages_tags):\n",
    "    suggest_tags = []\n",
    "    for tags in [x for sublist in pages_tags for x in sublist]:\n",
    "        if tags:\n",
    "            suggest_tags.append(tags)\n",
    "    suggest_tags = list(OrderedDict.fromkeys(sorted(suggest_tags, key=suggest_tags.count, reverse=True)))\n",
    "    recommended_tags = []\n",
    "    for items in suggest_tags:\n",
    "        recommended_tags.append(single_character_remover(items))\n",
    "    return recommended_tags\n",
    "        \n",
    "def query_at_top_at_beginning(text, match):\n",
    "    chunk = text[match.start(): match.end() + 40]\n",
    "    chunk_list = chunk.split()\n",
    "    chunk_list.pop(-1)\n",
    "    return chunk_list\n",
    "\n",
    "def longest_match_within_best_matches(best_matches, items):\n",
    "    longest_content = []\n",
    "    for content in best_matches: \n",
    "        longest_content.append(max(all_substrings(content) & all_substrings(items), key=len))\n",
    "    return max(longest_content, key=len)\n",
    "\n",
    "def tag_chunks(front_seq_word, rear_seq_word):\n",
    "    rear_queue = []\n",
    "    count = 0 \n",
    "    for word in rear_seq_word:\n",
    "        # if re.match(r'[ァ-ン]', word) or re.match(r'[A-Za-z]', word) and count < 3:\n",
    "        if count < 3:\n",
    "            rear_queue.append(word)\n",
    "        else:\n",
    "            break\n",
    "        count += 1\n",
    "\n",
    "    front_queue = []\n",
    "    count = 0 \n",
    "    for word in front_seq_word[::-1]:\n",
    "        # if re.match(r'[ァ-ン]', word) or re.match(r'[A-Za-z]', word) and count < 3:\n",
    "        if count < 3:\n",
    "            front_queue.append(word)\n",
    "        else:\n",
    "            break\n",
    "        count += 1\n",
    "    front_queue.reverse()   \n",
    "    return front_queue, rear_queue \n",
    "\n",
    "def tags_factory(text, match, pattern):\n",
    "    front_seq_word = text[match.start()-30: match.end()].split()                 \n",
    "    rear_seq_word = text[match.start(): match.end() + 30].split()\n",
    "    # print(front_seq_word, rear_seq_word)\n",
    "    front_queue, rear_queue = tag_chunks(front_seq_word, rear_seq_word)\n",
    "    return front_queue, rear_queue\n",
    "\n",
    "\n",
    "def hash_tag_generator(page_corpus, token_query_word):\n",
    "    unique_tags = []\n",
    "    pages_tags = []\n",
    "    collector = []\n",
    "    token_query_word = token_query_word\n",
    "    pattern = token_query_word\n",
    "    for index, col in page_corpus.iterrows():\n",
    "        matched = []\n",
    "        text = col['Data']            \n",
    "        for match in re.finditer(pattern, text):\n",
    "            if match:\n",
    "                # print(match)\n",
    "                if match.start() > 30:\n",
    "                    chunk_list = query_in_middle_position(text, match)\n",
    "                    front_queue, rear_queue = tags_factory(text, match, pattern)\n",
    "                    matched.append(' '.join(chunk_list + [\"#\"] + rear_queue + [\"#\"] + front_queue + [\"#\"]))                    \n",
    "                else:\n",
    "                    matched.append(' '.join(query_at_top_at_beginning(text, match)))\n",
    "        if matched:\n",
    "            unique_tags = unique_tag_provider(matched, token_query_word)\n",
    "            # print(unique_tags)\n",
    "            collector.append([col['PageID'], len(matched), unique_tags, matched])\n",
    "        pages_tags.append(unique_tags)\n",
    "    tags = unique_recommended_all_tags(pages_tags)\n",
    "    return tags, sorted(collector, key=lambda l:l[1], reverse=True)[:10]\n",
    "\n",
    "\n",
    "def making_query_collection(query):\n",
    "    query_parts = query.split()\n",
    "    question_parts = []\n",
    "    for i in range(len(query_parts)):\n",
    "        if len(query_parts) - 1 > i:\n",
    "            question_parts.append(query_parts[i] + \" \" + query_parts[i + 1])\n",
    "            if len(query_parts) - 2 > i:\n",
    "                question_parts.append(query_parts[i] + \" \" + query_parts[i + 1] + \" \" + query_parts[i + 2])\n",
    "    return question_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dictionary():\n",
    "    file = open(\"/home/iftekhar/amiebot/exp_amiecore/amieCore/amie_core/core/retriever/Page_Ranking_Experiment/pipelines/vocabulary_synonyms_all.json\", \"r\")\n",
    "    contents = file.read()\n",
    "    synonyms_dict = ast.literal_eval(contents)\n",
    "    file.close()\n",
    "    return synonyms_dict\n",
    "\n",
    "\n",
    "def query_rewritter_replacing_synonyms(single_token_query, corpus):\n",
    "    # check the synonyms and convert it to base terms\n",
    "    collector = []\n",
    "    for items in single_token_query:\n",
    "        if corpus.find(items) == -1:\n",
    "            dict_synonyms = getKeysByValue(synonyms_dict, items)\n",
    "            if dict_synonyms:\n",
    "                print(\"Input Terms: \", items, ' uttered in corpus ', dict_synonyms)\n",
    "                collector.append(' '.join(dict_synonyms))\n",
    "        else:\n",
    "            collector.append(items)\n",
    "    # rewritten_query = ' '.join([x for sublist in collector for x in sublist])\n",
    "    # print(\"Your input becomes: \", ' '.join(collector))\n",
    "    return collector\n",
    "\n",
    "\n",
    "def handling_spelling_mistakes(question_parts, vocabulary):\n",
    "    # Assumed user has spelling mistakes\n",
    "    collector = []\n",
    "    for items in question_parts:\n",
    "        best_matches = difflib.get_close_matches(items, vocabulary, n = 5, cutoff = 0.6)\n",
    "        if best_matches:\n",
    "            max_term = longest_match_within_best_matches(best_matches, items)\n",
    "            collector.append(max_term)\n",
    "            # print(\"Closest\", best_matches)\n",
    "    return collector\n",
    "\n",
    "\n",
    "def how_long_query_matched(collector, whole_corpus):\n",
    "    not_matched = ''\n",
    "    max_matched = ''\n",
    "    flag = True\n",
    "    for items in collector:\n",
    "        if whole_corpus.find(items) != -1 and flag is True:\n",
    "            max_matched = items\n",
    "            flag = False\n",
    "        elif whole_corpus.find(max_matched + \" \" + items) != -1:\n",
    "            max_matched += \" \" + items\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    not_matched = ' '.join(collector).replace(max_matched, '')\n",
    "    print(\"Maximum Sequence Matched: \", max_matched, 'not_matched: ', not_matched)\n",
    "    return max_matched, not_matched\n",
    "\n",
    "\n",
    "def unknown_word_sequence_handler(input_query, vocabulary, synonyms_dict, corpus):\n",
    "    not_matched = 0\n",
    "    whole_corpus = ' '.join(corpus.Data.values)\n",
    "    single_token_query = input_query.split()\n",
    "    question_parts = [input_query] + making_query_collection(input_query) + single_token_query\n",
    "    # print(question_parts)\n",
    "    collector = query_rewritter_replacing_synonyms(single_token_query, whole_corpus, synonyms_dict)\n",
    "    # print(\"Hi\", collector)\n",
    "    max_matched, not_matched = how_long_query_matched(collector, whole_corpus)\n",
    "    # print('Max_matched, Not_matched: ', max_matched, not_matched)\n",
    "    voc_hints = handling_spelling_mistakes(question_parts, vocabulary)\n",
    "    print(\"Vocab found from corpus: \", list(set(voc_hints)))\n",
    "    \n",
    "    tags, details = hash_tag_generator(corpus, max_matched)\n",
    "    if tags:\n",
    "        # print(\"Suggestions: \", tags)\n",
    "        return tags, not_matched, max_matched\n",
    "                \n",
    "        \n",
    "def getKeysByValue(dictOfElements, valueToFind):\n",
    "    listOfKeys = list()\n",
    "    listOfItems = dictOfElements.items()\n",
    "    for item in listOfItems:\n",
    "        for synonyms in item[1]: \n",
    "            if synonyms == valueToFind:\n",
    "                listOfKeys.append(item[0])\n",
    "    return listOfKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms_dict = load_dictionary()\n",
    "while True:\n",
    "    input_query = input(\"Type your query: \") \n",
    "    if input_query and input_query is not \" \":\n",
    "        tags, details = hash_tag_generator(corpus, input_query)\n",
    "        if tags:\n",
    "            print(\"Suggested Tags: \", tags)\n",
    "        else:\n",
    "            # print('Word/Sequence not found ')\n",
    "            try:\n",
    "                tags, not_matched, max_matched = unknown_word_sequence_handler(input_query, vocabulary, synonyms_dict, corpus)\n",
    "                # print(tags, not_matched)\n",
    "                print(\"Suggested Tags: \", tags, 'not_matched: ', not_matched)\n",
    "            except TypeError:\n",
    "                print(\"Not matched with any Tag\")\n",
    "                pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cus_ques = pd.read_csv('support_teams_query.csv')\n",
    "synonyms_dict = load_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________\n",
      "Input Query:  ランチャー って  \n",
      "Not matched with any Tag\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for index, col in cus_ques.iterrows():\n",
    "    print(\"________\\nInput Query: \", col['Question'])\n",
    "    input_query = col['Question']\n",
    "    tags, details = hash_tag_generator(corpus, input_query)\n",
    "    if tags:\n",
    "        print(\"Suggested Tags: \", tags)\n",
    "    else:\n",
    "        # print('Word/Sequence not found ')\n",
    "        try:\n",
    "            tags, not_matched, max_matched = unknown_word_sequence_handler(\\\n",
    "                input_query, vocabulary, synonyms_dict, corpus)\n",
    "            # print(not_matched)\n",
    "            if tags:\n",
    "                collector = []\n",
    "                for chunks in tags:\n",
    "                    for items in not_matched.split():\n",
    "                        if chunks.find(items) != -1:\n",
    "                            collector.append(chunks)\n",
    "                            # print(\"Not matched Present in: \", chunks)\n",
    "                if collector:\n",
    "                    print(\"Suggested Tags (Not macthed found): \", collector)\n",
    "                else:\n",
    "                    if len(tags) > 10:\n",
    "                        saver = []\n",
    "                        for chunks in tags:\n",
    "                            collector = []\n",
    "                            tag_chunks = chunks.split()\n",
    "                            tag_chunks.remove(max_matched)\n",
    "                            for terms in tag_chunks:\n",
    "                                collector.append(synonyms_dict.get(terms))\n",
    "                            saver.append([chunks, collector])\n",
    "                            break\n",
    "                        print(saver)\n",
    "                    print(\"Suggested Tags (Tag found): \", tags)\n",
    "        except TypeError:\n",
    "            print(\"Not matched with any Tag\")\n",
    "            pass\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'実施'"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = difflib.SequenceMatcher(\n",
    "    None, string1, string2).get_matching_blocks()\n",
    "for match in matches:\n",
    "    print(string1[match.a:match.a + match.size])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "sentence = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'\n",
    "ne_tree = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "\n",
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "sent = preprocess(sentence)\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('European', 'JJ'),\n",
       " ('authorities', 'NNS'),\n",
       " ('fined', 'VBD'),\n",
       " ('Google', 'NNP'),\n",
       " ('a', 'DT'),\n",
       " ('record', 'NN'),\n",
       " ('$', '$'),\n",
       " ('5.1', 'CD'),\n",
       " ('billion', 'CD'),\n",
       " ('on', 'IN'),\n",
       " ('Wednesday', 'NNP'),\n",
       " ('for', 'IN'),\n",
       " ('abusing', 'VBG'),\n",
       " ('its', 'PRP$'),\n",
       " ('power', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('mobile', 'JJ'),\n",
       " ('phone', 'NN'),\n",
       " ('market', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('ordered', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('company', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('alter', 'VB'),\n",
       " ('its', 'PRP$'),\n",
       " ('practices', 'NNS')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sent)\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today DATE\n",
      "first ORDINAL\n",
      "Business Process Review ORG\n",
      "BPR ORG\n",
      "four CARDINAL\n",
      "CSF ORG\n",
      "Program & Project Managements ORG\n"
     ]
    }
   ],
   "source": [
    "#python -m spacy download en_core_web_lg\n",
    "#python -m spacy download en_core_web_sm\n",
    "#python -m spacy download en\n",
    "import spacy\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process whole documents\n",
    "text = (\"Business Analysis To enhance your business’ efficiency in the future, you need a solid understanding of where you are today. A typical first step in our strategic approach to improving your business is a comprehensive Business Process Review (BPR). Optimising simplifying your business processes can substantially improve your company’s performance. While revised processes alone can make a dramatic difference, but the biggest benefits of re-engineering are usually possible only with investment in technology. We can provide process review, redesign and innovation to ensure that there are clear and controlled processes that make effective use of organisational resources to deliver stakeholder value. By evaluating a company’s current practices and measuring its performance against globally competitive criteria, we are able to determine the most effective and efficient way for an organization to perform its functions, removing unnecessary steps and people in the process. A strategic business process review delivery framework is designed to identify opportunities for business process improvement, thorough the following four steps: Analyse Identify Quantify and Implement The information gathered from this evaluation helps us to: Define critical success factors (CSF’s) – areas in which an organization must perform well in order to be successful. Define existing or potential problems within a particular area. Define potential solutions which will help your organization reduce overall costs and provide more efficient use of resources. Program & Project Managements Our project and program managers are thought leaders and bring a wealth of knowledge and innovation to every engagement. Our objective is to maximise the benefit of an IT project or program initiative taken by your organisation. Both projects are constrained by time, budget and scope. It is the discipline of planning, organising, controlling as well as motivating resources to achieve specific goals. Our focus is to guide you in effectively implementing programs and projects which includes: Planning and Scheduling Estimation and Cost Management Resource Management Governance Risk Management Stakeholder Management\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Analyze syntax\n",
    "# print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "#print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business\n",
      "analysis\n",
      "business\n",
      "efficiency\n",
      "future\n",
      "understanding\n",
      "today\n",
      "step\n",
      "approach\n",
      "business\n",
      "Business\n",
      "Process\n",
      "Review\n",
      "BPR\n",
      "business\n",
      "process\n",
      "company\n",
      "performance\n",
      "process\n",
      "difference\n",
      "benefit\n",
      "-\n",
      "engineering\n",
      "investment\n",
      "technology\n",
      "process\n",
      "review\n",
      "redesign\n",
      "innovation\n",
      "process\n",
      "use\n",
      "resource\n",
      "stakeholder\n",
      "value\n",
      "company\n",
      "practice\n",
      "performance\n",
      "criterion\n",
      "way\n",
      "organization\n",
      "function\n",
      "step\n",
      "people\n",
      "process\n",
      "business\n",
      "process\n",
      "review\n",
      "delivery\n",
      "framework\n",
      "opportunity\n",
      "business\n",
      "process\n",
      "improvement\n",
      "step\n",
      "Analyse\n",
      "Identify\n",
      "Quantify\n",
      "information\n",
      "evaluation\n",
      "success\n",
      "factor\n",
      "CSF\n",
      "area\n",
      "organization\n",
      "order\n",
      "problem\n",
      "area\n",
      "solution\n",
      "organization\n",
      "cost\n",
      "use\n",
      "resource\n",
      "Program\n",
      "Project\n",
      "Managements\n",
      "project\n",
      "program\n",
      "manager\n",
      "leader\n",
      "wealth\n",
      "knowledge\n",
      "innovation\n",
      "engagement\n",
      "objective\n",
      "benefit\n",
      "project\n",
      "program\n",
      "initiative\n",
      "organisation\n",
      "project\n",
      "time\n",
      "budget\n",
      "scope\n",
      "discipline\n",
      "planning\n",
      "resource\n",
      "goal\n",
      "focus\n",
      "program\n",
      "project\n",
      "planning\n",
      "Scheduling\n",
      "Estimation\n",
      "Cost\n",
      "Management\n",
      "Resource\n",
      "Management\n",
      "Governance\n",
      "Risk\n",
      "Management\n",
      "Stakeholder\n",
      "Management\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if not token.is_stop and token.pos_ in ['PROPN', 'NOUN']:\n",
    "        print(str(token.lemma_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today',\n",
       " 'first',\n",
       " 'Business Process Review',\n",
       " 'BPR',\n",
       " 'four',\n",
       " 'CSF',\n",
       " 'Program & Project Managements']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[entity.text for entity in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "def get_stop_words_en():\n",
    "    stop_word_file = os.path.join('/home/ifte/alechat_core/assets/stopwords', 'stop_words_en.txt')\n",
    "\n",
    "    with open(stop_word_file, encoding='utf-8') as fr:\n",
    "        stop_words = ast.literal_eval(fr.read())\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "def remove_stopwords(words, stopwords):\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sent.text for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = []\n",
    "for w in [i.split() for i in [str(x).lower() for x in doc.noun_chunks]]:\n",
    "    z = remove_stopwords(w, get_stop_words_en())\n",
    "    if z and len(z) > 1:\n",
    "        collector.append(' '.join(z))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['business analysis',\n",
       " 'business’ efficiency',\n",
       " 'solid understanding',\n",
       " 'typical first step',\n",
       " 'strategic approach',\n",
       " 'comprehensive business process review',\n",
       " 'business processes',\n",
       " 'revised processes',\n",
       " 'dramatic difference',\n",
       " 'biggest benefits',\n",
       " 'process review',\n",
       " 'clear controlled processes',\n",
       " 'effective use',\n",
       " 'organisational resources',\n",
       " 'stakeholder value',\n",
       " 'current practices',\n",
       " 'globally competitive criteria',\n",
       " 'most effective efficient way',\n",
       " 'unnecessary steps',\n",
       " 'strategic business process review delivery framework',\n",
       " 'business process improvement',\n",
       " 'following four steps',\n",
       " 'critical success factors',\n",
       " 'existing potential problems',\n",
       " 'particular area',\n",
       " 'potential solutions',\n",
       " 'overall costs',\n",
       " 'efficient use',\n",
       " 'program managers',\n",
       " 'every engagement',\n",
       " 'program initiative',\n",
       " 'specific goals',\n",
       " 'planning scheduling estimation',\n",
       " 'cost management resource management governance risk management stakeholder management']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "gs = ngrams('hello world boy'.split(), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = []\n",
    "for i,j in [x for x in grams]:\n",
    "    collector.append(i)\n",
    "    collector.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', 'world', 'boy']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 'world'), ('world', 'boy')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "world boy\n"
     ]
    }
   ],
   "source": [
    "gs = ngrams('hello world boy'.split(), 2)\n",
    "for grams in gs:\n",
    "    print(' '.join(grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian \n",
      "Thrun \n",
      "start \n",
      "work \n",
      "self \n",
      "drive \n",
      "car \n",
      "Google \n",
      "people \n",
      "company \n",
      "take \n",
      "seriously \n",
      "not \n",
      "not \n",
      "not \n",
      "will \n",
      "not \n",
      "tell \n",
      "senior \n",
      "ceo \n",
      "major \n",
      "american \n",
      "car \n",
      "company \n",
      "shake \n",
      "hand \n",
      "turn \n",
      "away \n",
      "not \n",
      "worth \n",
      "talk \n",
      "say \n",
      "Thrun \n",
      "interview \n",
      "Recode \n",
      "early \n",
      "week \n"
     ]
    }
   ],
   "source": [
    "FEATURES = ['VERB', 'PROPN', 'ADJ', 'NOUN', 'INTJ', 'ADV']\n",
    "for token in doc:\n",
    "#     print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "#             token.shape_, token.is_alpha, token.is_stop)\n",
    "    \n",
    "        \n",
    "    if not token.is_stop and token.pos_ in FEATURES or token.dep_ is 'neg':\n",
    "        print(token.lemma_ + ' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "def get_stop_words_en():\n",
    "    stop_word_file = os.path.join('/home/ifte/alechat_core/assets/stopwords', 'stop_words_en.txt')\n",
    "\n",
    "    with open(stop_word_file, encoding='utf-8') as fr:\n",
    "        stop_words = ast.literal_eval(fr.read())\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['at',\n",
       " 'his',\n",
       " 'hers',\n",
       " 'himself',\n",
       " 'on',\n",
       " 'in',\n",
       " 'ourselves',\n",
       " 'theirs',\n",
       " 'some',\n",
       " 'up',\n",
       " 'their',\n",
       " 'which',\n",
       " 'they',\n",
       " 'is',\n",
       " 'itself',\n",
       " \"you'd\",\n",
       " 'themselves',\n",
       " 'the',\n",
       " 'before',\n",
       " 'were',\n",
       " 'where',\n",
       " 'was',\n",
       " 'let',\n",
       " 'your',\n",
       " 'have',\n",
       " 'that',\n",
       " 'out',\n",
       " 'until',\n",
       " 'her',\n",
       " 'over',\n",
       " \"she's\",\n",
       " 'doing',\n",
       " 'having',\n",
       " 'me',\n",
       " 'after',\n",
       " 'an',\n",
       " 'between',\n",
       " 'am',\n",
       " 'our',\n",
       " 'myself',\n",
       " 'please',\n",
       " 'by',\n",
       " \"you'll\",\n",
       " 'under',\n",
       " 'here',\n",
       " 'yourselves',\n",
       " 'are',\n",
       " 'it',\n",
       " 're',\n",
       " 'not',\n",
       " 'nor',\n",
       " 'there',\n",
       " 'such',\n",
       " 'into',\n",
       " \"you're\",\n",
       " 'should',\n",
       " 'only',\n",
       " 'for',\n",
       " 'above',\n",
       " \"it's\",\n",
       " 'ours',\n",
       " 'more',\n",
       " \"you've\",\n",
       " 'did',\n",
       " 'few',\n",
       " 'if',\n",
       " 'these',\n",
       " 'what',\n",
       " 'does',\n",
       " 'once',\n",
       " 'would',\n",
       " 'to',\n",
       " 'want',\n",
       " 'most',\n",
       " 'those',\n",
       " 'how',\n",
       " 'again',\n",
       " 'each',\n",
       " 'but',\n",
       " 'through',\n",
       " 'my',\n",
       " 'same',\n",
       " 'no',\n",
       " 'as',\n",
       " 'been',\n",
       " 'with',\n",
       " 'from',\n",
       " 'them',\n",
       " 'just',\n",
       " 'now',\n",
       " 'both',\n",
       " 'she',\n",
       " 'while',\n",
       " 'and',\n",
       " \"should've\",\n",
       " 'very',\n",
       " 'during',\n",
       " \"what's\",\n",
       " 'when',\n",
       " 'other',\n",
       " 'own',\n",
       " 've',\n",
       " 'because',\n",
       " 'about',\n",
       " 'you',\n",
       " 'do',\n",
       " 'so',\n",
       " 'why',\n",
       " 'yours',\n",
       " 'below',\n",
       " 'herself',\n",
       " 'can',\n",
       " 'him',\n",
       " 'be',\n",
       " 'against',\n",
       " 'has',\n",
       " 'll',\n",
       " 'further',\n",
       " 'we',\n",
       " 'don',\n",
       " 'its',\n",
       " 'any',\n",
       " 'all',\n",
       " 'than',\n",
       " 'of',\n",
       " 'being',\n",
       " 'whats',\n",
       " 'off',\n",
       " 'too',\n",
       " 'down',\n",
       " 'had',\n",
       " 'he',\n",
       " 'who',\n",
       " 'this',\n",
       " 'then',\n",
       " \"that'll\",\n",
       " 'yourself',\n",
       " 'whom',\n",
       " 'or',\n",
       " 'will']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stop_words_en()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['hot or cold weather', 'hot', 'cold', 'weather']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cold hot weather hot or cold weather'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(list(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Samsung', 'South Korea']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#text = \"who is Barrack Mama\" \n",
    "text = \"Most of the outlay will be at home. No surprise there, either. While Samsung has expanded overseas, South Korea is still host to most of its factories and research engineers. ..\" \n",
    "word = nltk.word_tokenize(text) \n",
    "pos_tag = nltk.pos_tag(word) \n",
    "chunk = nltk.ne_chunk(pos_tag) \n",
    "NE = [\" \".join(w for w, t in ele) for ele in chunk if isinstance(ele, nltk.Tree)] \n",
    "print(NE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE ['South Korea ']\n",
      "ORG ['Samsung ']\n"
     ]
    }
   ],
   "source": [
    "text = \"Most of the outlay will be at home. No surprise there, either. While Samsung has expanded overseas, South Korea is still host to most of its factories and research engineers. ..\" \n",
    "text = nlp(text) \n",
    "labels = set([w.label_ for w in text.ents]) \n",
    "for label in labels: \n",
    "    entities = [e.string for e in text.ents if label==e.label_]\n",
    "    entities = list(set(entities)) \n",
    "    print(label,entities) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "sentence = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'\n",
    "ne_tree = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "\n",
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "sent = preprocess(sentence)\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('European', 'JJ'),\n",
       " ('authorities', 'NNS'),\n",
       " ('fined', 'VBD'),\n",
       " ('Google', 'NNP'),\n",
       " ('a', 'DT'),\n",
       " ('record', 'NN'),\n",
       " ('$', '$'),\n",
       " ('5.1', 'CD'),\n",
       " ('billion', 'CD'),\n",
       " ('on', 'IN'),\n",
       " ('Wednesday', 'NNP'),\n",
       " ('for', 'IN'),\n",
       " ('abusing', 'VBG'),\n",
       " ('its', 'PRP$'),\n",
       " ('power', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('mobile', 'JJ'),\n",
       " ('phone', 'NN'),\n",
       " ('market', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('ordered', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('company', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('alter', 'VB'),\n",
       " ('its', 'PRP$'),\n",
       " ('practices', 'NNS')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sent)\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Thrun PERSON\n",
      "Google ORG\n",
      "2007 DATE\n",
      "American NORP\n",
      "Thrun PERSON\n",
      "Recode LOC\n",
      "earlier this week DATE\n"
     ]
    }
   ],
   "source": [
    "#python -m spacy download en_core_web_lg\n",
    "#python -m spacy download en_core_web_sm\n",
    "#python -m spacy download en\n",
    "import spacy\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process whole documents\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I hadn't haven't wouldn't won't tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Analyze syntax\n",
    "# print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "#print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sebastian Thrun',\n",
       " 'Google',\n",
       " '2007',\n",
       " 'American',\n",
       " 'Thrun',\n",
       " 'Recode',\n",
       " 'earlier this week']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[entity.text for entity in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "gs = ngrams('hello world boy'.split(), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = []\n",
    "for i,j in [x for x in grams]:\n",
    "    collector.append(i)\n",
    "    collector.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', 'world', 'boy']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 'world'), ('world', 'boy')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "world boy\n"
     ]
    }
   ],
   "source": [
    "gs = ngrams('hello world boy'.split(), 2)\n",
    "for grams in gs:\n",
    "    print(' '.join(grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian \n",
      "Thrun \n",
      "start \n",
      "work \n",
      "self \n",
      "drive \n",
      "car \n",
      "Google \n",
      "people \n",
      "company \n",
      "take \n",
      "seriously \n",
      "not \n",
      "not \n",
      "not \n",
      "will \n",
      "not \n",
      "tell \n",
      "senior \n",
      "ceo \n",
      "major \n",
      "american \n",
      "car \n",
      "company \n",
      "shake \n",
      "hand \n",
      "turn \n",
      "away \n",
      "not \n",
      "worth \n",
      "talk \n",
      "say \n",
      "Thrun \n",
      "interview \n",
      "Recode \n",
      "early \n",
      "week \n"
     ]
    }
   ],
   "source": [
    "FEATURES = ['VERB', 'PROPN', 'ADJ', 'NOUN', 'INTJ', 'ADV']\n",
    "for token in doc:\n",
    "#     print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "#             token.shape_, token.is_alpha, token.is_stop)\n",
    "    \n",
    "        \n",
    "    if not token.is_stop and token.pos_ in FEATURES or token.dep_ is 'neg':\n",
    "        print(token.lemma_ + ' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "def get_stop_words_en():\n",
    "    stop_word_file = os.path.join('/home/ifte/alechat_core/assets/stopwords', 'stop_words_en.txt')\n",
    "\n",
    "    with open(stop_word_file, encoding='utf-8') as fr:\n",
    "        stop_words = ast.literal_eval(fr.read())\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['at',\n",
       " 'his',\n",
       " 'hers',\n",
       " 'himself',\n",
       " 'on',\n",
       " 'in',\n",
       " 'ourselves',\n",
       " 'theirs',\n",
       " 'some',\n",
       " 'up',\n",
       " 'their',\n",
       " 'which',\n",
       " 'they',\n",
       " 'is',\n",
       " 'itself',\n",
       " \"you'd\",\n",
       " 'themselves',\n",
       " 'the',\n",
       " 'before',\n",
       " 'were',\n",
       " 'where',\n",
       " 'was',\n",
       " 'let',\n",
       " 'your',\n",
       " 'have',\n",
       " 'that',\n",
       " 'out',\n",
       " 'until',\n",
       " 'her',\n",
       " 'over',\n",
       " \"she's\",\n",
       " 'doing',\n",
       " 'having',\n",
       " 'me',\n",
       " 'after',\n",
       " 'an',\n",
       " 'between',\n",
       " 'am',\n",
       " 'our',\n",
       " 'myself',\n",
       " 'please',\n",
       " 'by',\n",
       " \"you'll\",\n",
       " 'under',\n",
       " 'here',\n",
       " 'yourselves',\n",
       " 'are',\n",
       " 'it',\n",
       " 're',\n",
       " 'not',\n",
       " 'nor',\n",
       " 'there',\n",
       " 'such',\n",
       " 'into',\n",
       " \"you're\",\n",
       " 'should',\n",
       " 'only',\n",
       " 'for',\n",
       " 'above',\n",
       " \"it's\",\n",
       " 'ours',\n",
       " 'more',\n",
       " \"you've\",\n",
       " 'did',\n",
       " 'few',\n",
       " 'if',\n",
       " 'these',\n",
       " 'what',\n",
       " 'does',\n",
       " 'once',\n",
       " 'would',\n",
       " 'to',\n",
       " 'want',\n",
       " 'most',\n",
       " 'those',\n",
       " 'how',\n",
       " 'again',\n",
       " 'each',\n",
       " 'but',\n",
       " 'through',\n",
       " 'my',\n",
       " 'same',\n",
       " 'no',\n",
       " 'as',\n",
       " 'been',\n",
       " 'with',\n",
       " 'from',\n",
       " 'them',\n",
       " 'just',\n",
       " 'now',\n",
       " 'both',\n",
       " 'she',\n",
       " 'while',\n",
       " 'and',\n",
       " \"should've\",\n",
       " 'very',\n",
       " 'during',\n",
       " \"what's\",\n",
       " 'when',\n",
       " 'other',\n",
       " 'own',\n",
       " 've',\n",
       " 'because',\n",
       " 'about',\n",
       " 'you',\n",
       " 'do',\n",
       " 'so',\n",
       " 'why',\n",
       " 'yours',\n",
       " 'below',\n",
       " 'herself',\n",
       " 'can',\n",
       " 'him',\n",
       " 'be',\n",
       " 'against',\n",
       " 'has',\n",
       " 'll',\n",
       " 'further',\n",
       " 'we',\n",
       " 'don',\n",
       " 'its',\n",
       " 'any',\n",
       " 'all',\n",
       " 'than',\n",
       " 'of',\n",
       " 'being',\n",
       " 'whats',\n",
       " 'off',\n",
       " 'too',\n",
       " 'down',\n",
       " 'had',\n",
       " 'he',\n",
       " 'who',\n",
       " 'this',\n",
       " 'then',\n",
       " \"that'll\",\n",
       " 'yourself',\n",
       " 'whom',\n",
       " 'or',\n",
       " 'will']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stop_words_en()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['hot or cold weather', 'hot', 'cold', 'weather']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cold hot weather hot or cold weather'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(list(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Samsung', 'South Korea']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#text = \"who is Barrack Mama\" \n",
    "text = \"Most of the outlay will be at home. No surprise there, either. While Samsung has expanded overseas, South Korea is still host to most of its factories and research engineers. ..\" \n",
    "word = nltk.word_tokenize(text) \n",
    "pos_tag = nltk.pos_tag(word) \n",
    "chunk = nltk.ne_chunk(pos_tag) \n",
    "NE = [\" \".join(w for w, t in ele) for ele in chunk if isinstance(ele, nltk.Tree)] \n",
    "print(NE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE ['South Korea ']\n",
      "ORG ['Samsung ']\n"
     ]
    }
   ],
   "source": [
    "text = \"Most of the outlay will be at home. No surprise there, either. While Samsung has expanded overseas, South Korea is still host to most of its factories and research engineers. ..\" \n",
    "text = nlp(text) \n",
    "labels = set([w.label_ for w in text.ents]) \n",
    "for label in labels: \n",
    "    entities = [e.string for e in text.ents if label==e.label_]\n",
    "    entities = list(set(entities)) \n",
    "    print(label,entities) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

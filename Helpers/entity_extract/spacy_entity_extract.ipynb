{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously', 'I can tell you very senior CEOs of', 'car companies would shake my hand and turn away because I wasn’t worth talking to', 'said Thrun', 'in an interview with Recode earlier this week']\n",
      "Verbs: []\n",
      ". Product_Other\n",
      "I can tell you very senior CEOs of major American car companies would shake my hand and turn away because I wasn’t worth talking to Product_Other\n",
      "said Thrun Product_Other\n",
      "in an interview with Recode earlier this week. Music\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"ja_ginza\")\n",
    "\n",
    "# Process whole documents\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Product_Other\n",
      "I can tell you very senior CEOs of major American car companies would shake my hand and turn away because I wasn’t worth talking to Product_Other\n",
      "said Thrun Product_Other\n",
      "in an interview with Recode earlier this week. Music\n"
     ]
    }
   ],
   "source": [
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jeffrey Hinton is a Corporation_Other\n",
      "\\ Famous for researching neural networks. Product_Other\n"
     ]
    }
   ],
   "source": [
    "doc  =  nlp(\"Jeffrey Hinton is a British-born researcher in computer science and cognitive psychology. \\ Famous for researching neural networks. Currently working at the University of Toronto and Google.\" )\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('MobiControl', 'ORG'), ('Android', 'OPSYS')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "nlp = English()\n",
    "ruler = EntityRuler(nlp)\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"MobiControl\"},\n",
    "            {\"label\": \"OPSYS\", \"pattern\": \"Android\"}]\n",
    "ruler.add_patterns(patterns)\n",
    "nlp.add_pipe(ruler)\n",
    "\n",
    "doc = nlp(\"Apple 製品 MobiControl v 14 Manual ios overall Android 端末\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('ja_ginza')\n",
    "doc = nlp('※ｄトラベルでは海外発行のクレジットカードのご利用はできません。あらかじめご了承ください。 国内宿泊では、ドコモ払い、クレジットカード払い（dカード、VISA、MasterCard、JCB、AMEX...※ｄトラベルでは海外発行のクレジットカードのご利用はできません。あらかじめご了承ください。 ※JCB、AMEX、ダイナースはご利用いただけません。 Q.利用可能な支払方法について教えてください。（電...\"')\n",
    "# for sent in doc.sents:\n",
    "#     for token in sent:\n",
    "#         print(token.i, token.orth_, token.lemma_, token.pos_, token.tag_, token.dep_, token.head.i)\n",
    "#     print('EOS')\n",
    "#     print(sent)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[銀座, ランチ, ご, PM]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in doc.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('トラベル', 'NP'), ('海外発行', 'NP'), ('クレジットカード', 'NP'), ('ご利用', 'NP'), ('ご', 'NP'), ('JCB', 'NP'), ('AMEX', 'NP'), ('ダイナース', 'NP'), ('ご利用', 'NP'), ('利用', 'NP'), ('支払方法', 'NP'), ('電', 'NP')]\n"
     ]
    }
   ],
   "source": [
    "print([(ent.text, ent.label_) for ent in doc.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ドコモ払い', 'Company'), ('クレジットカード払い', 'Doctrine_Method_Other'), ('VISA', 'Product_Other'), ('MasterCard', 'Product_Other'), ('JCB', 'Product_Other'), ('AMEX...※ｄ', 'Product_Other'), ('JCB', 'Product_Other'), ('AMEX', 'Product_Other'), ('ダイナース', 'Product_Other')]\n"
     ]
    }
   ],
   "source": [
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

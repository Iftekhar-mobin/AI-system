{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import MeCab\n",
    "import re\n",
    "import ast\n",
    "mecab = MeCab.Tagger('-Owakati')\n",
    "from pathlib import Path\n",
    "import difflib\n",
    "# import spacy\n",
    "# nlp = spacy.load('ja_ginza')\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(\"very_cleaned_dataset_mobicontrol.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist = corpus.Data.str.split(expand=True).stack().value_counts()\n",
    "import json\n",
    "file = open(\"/home/iftekhar/amiebot/exp_amiecore/amieCore/amie_core/core/retriever/Page_Ranking_Experiment/pipelines/words_frequency_distribution.json\", \"w\", encoding='utf-8')\n",
    "json.dump(freq_dist.to_dict(), file, ensure_ascii=False)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "context = corpus.Data.values.tolist()\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(context)\n",
    "#print(vectorizer.get_feature_names())\n",
    "# ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
    "# print(X.shape)\n",
    "# (4, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_counts = vectorizer.transform(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>occurrences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1938</th>\n",
       "      <td>ます</td>\n",
       "      <td>98.631002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>端末</td>\n",
       "      <td>66.491236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5258</th>\n",
       "      <td>設定</td>\n",
       "      <td>29.337458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2390</th>\n",
       "      <td>サーバ</td>\n",
       "      <td>24.864351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5174</th>\n",
       "      <td>表示</td>\n",
       "      <td>24.288604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>mobicontrol</td>\n",
       "      <td>23.870831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2958</th>\n",
       "      <td>プロファイル</td>\n",
       "      <td>20.095465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4871</th>\n",
       "      <td>登録</td>\n",
       "      <td>19.552362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043</th>\n",
       "      <td>アプリ</td>\n",
       "      <td>18.319571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5515</th>\n",
       "      <td>選択</td>\n",
       "      <td>17.460408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4590</th>\n",
       "      <td>構成</td>\n",
       "      <td>17.124632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3504</th>\n",
       "      <td>入力</td>\n",
       "      <td>17.060311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>グループ</td>\n",
       "      <td>16.668132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083</th>\n",
       "      <td>ユーザ</td>\n",
       "      <td>16.659110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2810</th>\n",
       "      <td>ファイル</td>\n",
       "      <td>15.692510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1854</th>\n",
       "      <td>でき</td>\n",
       "      <td>15.600358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4839</th>\n",
       "      <td>画面</td>\n",
       "      <td>14.932238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5291</th>\n",
       "      <td>認証</td>\n",
       "      <td>14.743216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4340</th>\n",
       "      <td>接続</td>\n",
       "      <td>14.534583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3152</th>\n",
       "      <td>ルール</td>\n",
       "      <td>13.851898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             term  occurrences\n",
       "1938           ます    98.631002\n",
       "4996           端末    66.491236\n",
       "5258           設定    29.337458\n",
       "2390          サーバ    24.864351\n",
       "5174           表示    24.288604\n",
       "922   mobicontrol    23.870831\n",
       "2958       プロファイル    20.095465\n",
       "4871           登録    19.552362\n",
       "2043          アプリ    18.319571\n",
       "5515           選択    17.460408\n",
       "4590           構成    17.124632\n",
       "3504           入力    17.060311\n",
       "2300         グループ    16.668132\n",
       "3083          ユーザ    16.659110\n",
       "2810         ファイル    15.692510\n",
       "1854           でき    15.600358\n",
       "4839           画面    14.932238\n",
       "5291           認証    14.743216\n",
       "4340           接続    14.534583\n",
       "3152          ルール    13.851898"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occ = np.asarray(cvec_counts.sum(axis=0)).ravel().tolist()\n",
    "counts_df = pd.DataFrame({'term': vectorizer.get_feature_names(), 'occurrences': occ})\n",
    "counts_df.sort_values(by='occurrences', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1938</th>\n",
       "      <td>ます</td>\n",
       "      <td>0.221643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>端末</td>\n",
       "      <td>0.149419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5258</th>\n",
       "      <td>設定</td>\n",
       "      <td>0.065927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2390</th>\n",
       "      <td>サーバ</td>\n",
       "      <td>0.055875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5174</th>\n",
       "      <td>表示</td>\n",
       "      <td>0.054581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>mobicontrol</td>\n",
       "      <td>0.053642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2958</th>\n",
       "      <td>プロファイル</td>\n",
       "      <td>0.045158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4871</th>\n",
       "      <td>登録</td>\n",
       "      <td>0.043938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043</th>\n",
       "      <td>アプリ</td>\n",
       "      <td>0.041168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5515</th>\n",
       "      <td>選択</td>\n",
       "      <td>0.039237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4590</th>\n",
       "      <td>構成</td>\n",
       "      <td>0.038482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3504</th>\n",
       "      <td>入力</td>\n",
       "      <td>0.038338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>グループ</td>\n",
       "      <td>0.037456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083</th>\n",
       "      <td>ユーザ</td>\n",
       "      <td>0.037436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2810</th>\n",
       "      <td>ファイル</td>\n",
       "      <td>0.035264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1854</th>\n",
       "      <td>でき</td>\n",
       "      <td>0.035057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4839</th>\n",
       "      <td>画面</td>\n",
       "      <td>0.033556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5291</th>\n",
       "      <td>認証</td>\n",
       "      <td>0.033131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4340</th>\n",
       "      <td>接続</td>\n",
       "      <td>0.032662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3152</th>\n",
       "      <td>ルール</td>\n",
       "      <td>0.031128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             term    weight\n",
       "1938           ます  0.221643\n",
       "4996           端末  0.149419\n",
       "5258           設定  0.065927\n",
       "2390          サーバ  0.055875\n",
       "5174           表示  0.054581\n",
       "922   mobicontrol  0.053642\n",
       "2958       プロファイル  0.045158\n",
       "4871           登録  0.043938\n",
       "2043          アプリ  0.041168\n",
       "5515           選択  0.039237\n",
       "4590           構成  0.038482\n",
       "3504           入力  0.038338\n",
       "2300         グループ  0.037456\n",
       "3083          ユーザ  0.037436\n",
       "2810         ファイル  0.035264\n",
       "1854           でき  0.035057\n",
       "4839           画面  0.033556\n",
       "5291           認証  0.033131\n",
       "4340           接続  0.032662\n",
       "3152          ルール  0.031128"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "weights = np.asarray(X.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': vectorizer.get_feature_names(), 'weight': weights})\n",
    "weights_df.sort_values(by='weight', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse matrix shape: (445, 5690)\n",
      "nonzero count: 83728\n",
      "sparsity: 3.31%\n"
     ]
    }
   ],
   "source": [
    "print('sparse matrix shape:', cvec_counts.shape)\n",
    "print('nonzero count:', cvec_counts.nnz)\n",
    "print('sparsity: %.2f%%' % (100.0 * cvec_counts.nnz / (cvec_counts.shape[0] * cvec_counts.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashTag Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "cus_ques = query_corpus_processing(\n",
    "    \"/home/iftekhar/amiebot/Resources/amiebot_dataset/support_team_question_pure.csv\")\n",
    "cus_ques.head()  \n",
    "cus_ques.to_csv(\"support_teams_query.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/iftekhar/amiebot/exp_amiecore/amieCore/amie_core/core/retriever/Page_Ranking_Experiment'\n",
    "          '/pipelines/vocabulary.txt') as f:\n",
    "    vocabulary = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_substrings(string):\n",
    "    n = len(string)\n",
    "    return {string[i:j+1] for i in range(n) for j in range(i,n)}\n",
    "\n",
    "\n",
    "def query_corpus_processing(corpus):\n",
    "    cus_ques = pd.read_csv(corpus)\n",
    "    cus_ques.Question = cus_ques.Question.apply(lambda x: mecab_tokenization(x))\n",
    "    cus_ques.Question = cus_ques.Question.apply(lambda x: single_character_remover(x))\n",
    "    cus_ques.Question = cus_ques.Question.apply(lambda x: cleaner(x))\n",
    "    return cus_ques\n",
    "\n",
    "def mecab_tokenization(text):\n",
    "    q = mecab.parse(text)\n",
    "    q_parts = q.split()\n",
    "    return ' '.join([word for word in q_parts if not word in get_stop_word_ja()])\n",
    "\n",
    "\n",
    "def single_character_remover(text):\n",
    "    collector = []\n",
    "    for items in text.split():\n",
    "        if len(items) < 2:\n",
    "            replaced = re.sub(r'[ぁ-んァ-ン]', '', items)\n",
    "            replaced = re.sub(r'[A-Za-z]', '', replaced)\n",
    "            replaced = re.sub(r'[0-9]', '', replaced)\n",
    "            collector.append(replaced)\n",
    "        else:\n",
    "            collector.append(items)\n",
    "\n",
    "    return ' '.join([temp.strip(' ') for temp in collector])\n",
    "\n",
    "def cleaner(text):\n",
    "    collector = []\n",
    "    for items in text.split():\n",
    "        cleaned = clean_text(items)\n",
    "        cleaned = re.sub(r\"\\s+\", '', cleaned)\n",
    "        if cleaned is not '' or cleaned is not ' ':\n",
    "            collector.append(clean_text(items))\n",
    "\n",
    "    return ' '.join(collector)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    replaced = text.replace(\"\\\\\", \"\")\n",
    "    replaced = replaced.replace(\"+\", \"\")\n",
    "    replaced = re.sub('_', '', replaced)\n",
    "    replaced = re.sub('\\W+', ' ', replaced)\n",
    "    replaced = re.sub(r'￥', '', replaced)  # 【】の除去\n",
    "    replaced = re.sub(r'．', '', replaced)  # ・ の除去\n",
    "    replaced = re.sub(r'｣', '', replaced)  # （）の除去\n",
    "    replaced = re.sub(r'｢', '', replaced)  # ［］の除去\n",
    "    replaced = re.sub(r'～', '', replaced)  # メンションの除去\n",
    "    replaced = re.sub(r'｜', '', replaced)  # URLの除去\n",
    "    replaced = re.sub(r'＠', '', replaced)  # 全角空白の除去\n",
    "    replaced = re.sub(r'？', '', replaced)  # 数字の除去\n",
    "    replaced = re.sub(r'％', '', replaced)\n",
    "    replaced = re.sub(r'＝', '', replaced)\n",
    "    replaced = re.sub(r'！', '', replaced)\n",
    "    replaced = re.sub(r'｝', '', replaced)\n",
    "    replaced = re.sub(r'：', '', replaced)\n",
    "    replaced = re.sub(r'－', '', replaced)\n",
    "    replaced = re.sub(r'･', '', replaced)\n",
    "    replaced = re.sub(r'ｔ', '', replaced)\n",
    "    replaced = re.sub(r'ｋ', '', replaced)\n",
    "    replaced = re.sub(r'ｄ', '', replaced)\n",
    "    replaced = re.sub(r'\\d+', '', replaced)\n",
    "\n",
    "    return replaced\n",
    "\n",
    "def longest_seq_search(query, page_data):\n",
    "    m = len(query)\n",
    "    n = len(page_data)\n",
    "    counter = [[0] * (n + 1) for x in range(m + 1)]\n",
    "    longest = 0\n",
    "    lcs_set = set()\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if query[i] == page_data[j]:\n",
    "                c = counter[i][j] + 1\n",
    "                counter[i + 1][j + 1] = c\n",
    "                if c > longest:\n",
    "                    lcs_set = set()\n",
    "                    longest = c\n",
    "                    lcs_set.add(query[i - c + 1:i + 1])\n",
    "                elif c == longest:\n",
    "                    lcs_set.add(query[i - c + 1:i + 1])\n",
    "\n",
    "    return lcs_set\n",
    "\n",
    "def get_stop_word_ja():\n",
    "    stop_word_file = Path(\"/home/iftekhar/AI-system/Helpers/stop_word_ja.txt\")\n",
    "    with open(stop_word_file, encoding='utf-8') as f:\n",
    "        stop_word_list = f.read().splitlines()\n",
    "    return stop_word_list\n",
    "\n",
    "def corpus_split(corpus, sentence_length):\n",
    "    labels = corpus.PageID.unique()\n",
    "    lines = []\n",
    "    all_ids = []\n",
    "    for i in list(labels):\n",
    "        text_list = corpus[corpus.PageID == i].Data.values\n",
    "        split_text = fixed_length_sentence(' '.join(text_list), sentence_length)\n",
    "        ids = [i] * len(split_text)\n",
    "        lines += split_text\n",
    "        all_ids += ids\n",
    "    split_corpus = pd.DataFrame(zip(lines, all_ids), columns=[\"Data\", \"PageID\"])\n",
    "    return split_corpus\n",
    "\n",
    "def fixed_length_sentence(contents, word_limit):\n",
    "    contents_list = contents.split()\n",
    "    end = len(contents_list)\n",
    "    count = 0\n",
    "    collector = []\n",
    "    line = []\n",
    "    for items in contents_list:\n",
    "        if count < word_limit - 1 and end > 1:\n",
    "            collector.append(items)\n",
    "            count += 1\n",
    "        else:\n",
    "            collector.append(items)\n",
    "            line.append(' '.join(collector))\n",
    "            collector = []\n",
    "            count = 0\n",
    "        end -= 1\n",
    "    return line\n",
    "\n",
    "\n",
    "def split_joint_word(text):\n",
    "    pattern = re.compile(\"[A-Z]\")\n",
    "    index_saver = []\n",
    "    start = -1\n",
    "    while True:\n",
    "        m = pattern.search(text, start + 1) \n",
    "        if m == None:\n",
    "            break\n",
    "        start = m.start()\n",
    "        index_saver.append(start)\n",
    "    \n",
    "    sorted_list = sorted(index_saver)\n",
    "    range_list=list(range(min(index_saver), max(index_saver)+1))\n",
    "    if sorted_list != range_list:\n",
    "        temp = 0\n",
    "        flag = False\n",
    "        save = []\n",
    "        for indexes in index_saver:\n",
    "            if flag: \n",
    "                if indexes - temp > 1:\n",
    "                    save.append(indexes)\n",
    "                    temp = indexes\n",
    "            else:\n",
    "                save.append(indexes)\n",
    "                temp = indexes\n",
    "                flag = True\n",
    "\n",
    "        if len(save) > 1:\n",
    "            chunk = text[save[0]:save[1]]\n",
    "            return chunk, single_character_remover(text.replace(chunk, ''))\n",
    "        else:\n",
    "            return None, None\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def english_joint_word_handler(text):\n",
    "    saver = []\n",
    "    while text:\n",
    "        temp = text\n",
    "        chunk, text = split_joint_word(text)\n",
    "        saver.append(chunk)\n",
    "    saver.append(temp)\n",
    "    saver.remove(None)\n",
    "    if len(saver) < 2:\n",
    "        saver = []\n",
    "    return saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Matched:  期限切れ , Noun detected:  期限切れ\n",
      "Closest ['期限', '切れ', '途切れ', '切れる']\n",
      "max_term:  期限\n",
      "Not Matched:  Enterise , Noun detected:  Enterise\n",
      "Closest ['Enterprise', 'Enerprise', 'Entrust', 'OneDrive', 'Internet']\n",
      "max_term:  Enter\n"
     ]
    }
   ],
   "source": [
    "##### Optional argument n (default 3) is the maximum number of close matches to return; \n",
    "# Optional argument cutoff (default 0.6) is a float in the range [0, 1]. \n",
    "\n",
    "for index, col in cus_ques.iterrows():\n",
    "    for items in col['Question'].split():\n",
    "        if (items.find('MobiControl')==-1) and re.match(r'[A-Za-z]', items):\n",
    "            chunks_words = english_joint_word_handler(items)\n",
    "#             print(chunks_words)\n",
    "            if chunks_words:\n",
    "                for words in chunks_words:\n",
    "                    end_flag = len(vocabulary)\n",
    "                    for voc in vocabulary:\n",
    "                        if words == voc:\n",
    "            #                 print(\"matched: \", items)\n",
    "                            break\n",
    "                        elif end_flag < 2:\n",
    "                            doc = nlp(words)\n",
    "                            for np in doc.noun_chunks:\n",
    "                                print(\"Not Matched: \", words, \", Noun detected: \", np)\n",
    "\n",
    "                            best_matches = difflib.get_close_matches(words, vocabulary, n = 5, cutoff = 0.5)\n",
    "                            print(\"Closest\", best_matches)\n",
    "\n",
    "                            longest_content = []\n",
    "                            for content in best_matches: \n",
    "                                longest_content.append(max(all_substrings(content) & all_substrings(items), key=len))\n",
    "                            max_term = max(longest_content, key=len)\n",
    "                            print('max_term: ', max_term)\n",
    "                            #sequences_list = available_sequences(corpus, max_term)\n",
    "                            #print(sequences_list)\n",
    "\n",
    "                            # break\n",
    "                        end_flag -= 1\n",
    "                    # break            \n",
    "        else:    \n",
    "            end_flag = len(vocabulary)\n",
    "            for voc in vocabulary:\n",
    "                if items == voc:\n",
    "    #                 print(\"matched: \", items)\n",
    "                    break\n",
    "                elif end_flag < 2:\n",
    "                    doc = nlp(items)\n",
    "                    for np in doc.noun_chunks:\n",
    "                        print(\"Not Matched: \", items, \", Noun detected: \", np)\n",
    "\n",
    "                    best_matches = difflib.get_close_matches(items, vocabulary, n = 5, cutoff = 0.5)\n",
    "                    print(\"Closest\", best_matches)\n",
    "\n",
    "                    longest_content = []\n",
    "                    for content in best_matches: \n",
    "                        longest_content.append(max(all_substrings(content) & all_substrings(items), key=len))\n",
    "                    max_term = max(longest_content, key=len)\n",
    "                    print('max_term: ', max_term)\n",
    "                    #sequences_list = available_sequences(corpus, max_term)\n",
    "                    #print(sequences_list)\n",
    "\n",
    "                    # break\n",
    "                end_flag -= 1\n",
    "            # break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_tag_provider(matched, token_query_word):\n",
    "    tags = []\n",
    "    for items in matched:\n",
    "        for match in re.finditer(r'# (.*) #', items):\n",
    "            tags.append(items[match.start()+1: match.end()].split('#'))\n",
    "    all_tag = []\n",
    "    for tag_chunk in tags:\n",
    "        for tag in tag_chunk:\n",
    "            if tag is not '':\n",
    "                all_tag.append(tag.strip())\n",
    "    unique_tags = list(OrderedDict.fromkeys(sorted(all_tag, key=all_tag.count, reverse=True)))\n",
    "    try:\n",
    "        unique_tags.remove(token_query_word)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return unique_tags\n",
    "\n",
    "def query_in_middle_position(text, match):\n",
    "    chunk = text[match.start() - 20: match.end() + 20]\n",
    "    chunk_list = chunk.split()\n",
    "    chunk_list.pop(0)\n",
    "    chunk_list.pop(-1)\n",
    "    return chunk_list\n",
    "\n",
    "def unique_recommended_all_tags(pages_tags):\n",
    "    suggest_tags = []\n",
    "    for tags in [x for sublist in pages_tags for x in sublist]:\n",
    "        if tags:\n",
    "            suggest_tags.append(tags)\n",
    "    suggest_tags = list(OrderedDict.fromkeys(sorted(suggest_tags, key=suggest_tags.count, reverse=True)))\n",
    "    recommended_tags = []\n",
    "    for items in suggest_tags:\n",
    "        recommended_tags.append(single_character_remover(items))\n",
    "    return recommended_tags\n",
    "        \n",
    "def query_at_top_at_beginning(text, match):\n",
    "    chunk = text[match.start(): match.end() + 40]\n",
    "    chunk_list = chunk.split()\n",
    "    chunk_list.pop(-1)\n",
    "    return chunk_list\n",
    "\n",
    "def longest_match_within_best_matches(best_matches, items):\n",
    "    longest_content = []\n",
    "    for content in best_matches: \n",
    "        longest_content.append(max(all_substrings(content) & all_substrings(items), key=len))\n",
    "    return max(longest_content, key=len)\n",
    "\n",
    "def tag_chunks(front_seq_word, rear_seq_word):\n",
    "    rear_queue = []\n",
    "    count = 0 \n",
    "    for word in rear_seq_word:\n",
    "        # if re.match(r'[ァ-ン]', word) or re.match(r'[A-Za-z]', word) and count < 3:\n",
    "        if count < 3:\n",
    "            rear_queue.append(word)\n",
    "        else:\n",
    "            break\n",
    "        count += 1\n",
    "\n",
    "    front_queue = []\n",
    "    count = 0 \n",
    "    for word in front_seq_word[::-1]:\n",
    "        # if re.match(r'[ァ-ン]', word) or re.match(r'[A-Za-z]', word) and count < 3:\n",
    "        if count < 3:\n",
    "            front_queue.append(word)\n",
    "        else:\n",
    "            break\n",
    "        count += 1\n",
    "    front_queue.reverse()   \n",
    "    return front_queue, rear_queue \n",
    "\n",
    "def tags_factory(text, match, pattern):\n",
    "    front_seq_word = text[match.start()-30: match.end()].split()                 \n",
    "    rear_seq_word = text[match.start(): match.end() + 30].split()\n",
    "    # print(front_seq_word, rear_seq_word)\n",
    "    front_queue, rear_queue = tag_chunks(front_seq_word, rear_seq_word)\n",
    "    return front_queue, rear_queue\n",
    "\n",
    "\n",
    "def hash_tag_generator(page_corpus, token_query_word):\n",
    "    unique_tags = []\n",
    "    pages_tags = []\n",
    "    collector = []\n",
    "    token_query_word = token_query_word\n",
    "    pattern = token_query_word\n",
    "    for index, col in page_corpus.iterrows():\n",
    "        matched = []\n",
    "        text = col['Data']\n",
    "        \n",
    "#         multiple = text.split()\n",
    "#         if len(text) > 1:\n",
    "#             longest_match_within_best_matches(vocabulary, items)\n",
    "            \n",
    "        for match in re.finditer(pattern, text):\n",
    "            if match:\n",
    "                if match.start() > 30:\n",
    "                    chunk_list = query_in_middle_position(text, match)\n",
    "                    front_queue, rear_queue = tags_factory(text, match, pattern)\n",
    "                    matched.append(' '.join(chunk_list + [\"#\"] + rear_queue + [\"#\"] + front_queue + [\"#\"]))                    \n",
    "                else:\n",
    "                    matched.append(' '.join(query_at_top_at_beginning(text, match)))\n",
    "        if matched:\n",
    "            unique_tags = unique_tag_provider(matched, token_query_word)\n",
    "            collector.append([col['PageID'], len(matched), unique_tags, matched])\n",
    "        pages_tags.append(unique_tags)\n",
    "    tags = unique_recommended_all_tags(pages_tags)\n",
    "\n",
    "    return tags, sorted(collector, key=lambda l:l[1], reverse=True)[:10]\n",
    "\n",
    "def making_query_collection(query):\n",
    "    query_parts = query.split()\n",
    "    question_parts = []\n",
    "    for i in range(len(query_parts)):\n",
    "        if len(query_parts) - 1 > i:\n",
    "            question_parts.append(query_parts[i] + \" \" + query_parts[i + 1])\n",
    "            if len(query_parts) - 2 > i:\n",
    "                question_parts.append(query_parts[i] + \" \" + query_parts[i + 1] + \" \" + query_parts[i + 2])\n",
    "    return question_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dictionary():\n",
    "    file = open(\"/home/iftekhar/amiebot/exp_amiecore/amieCore/amie_core/core/retriever/Page_Ranking_Experiment/pipelines/vocabulary_synonyms_all.json\", \"r\")\n",
    "    contents = file.read()\n",
    "    corpus_dict = ast.literal_eval(contents)\n",
    "    file.close()\n",
    "    return corpus_dict\n",
    "\n",
    "\n",
    "def query_rewritter_replacing_synonyms(single_token_query, corpus):\n",
    "    # check the synonyms and convert it to base terms\n",
    "    collector = []\n",
    "    for items in single_token_query:\n",
    "        if corpus.find(items) == -1:\n",
    "            dict_synonyms = getKeysByValue(load_dictionary(), items)\n",
    "            if dict_synonyms:\n",
    "                print(\"Input Terms: \", items, ' uttered in corpus ', dict_synonyms)\n",
    "                collector.append(' '.join(dict_synonyms))\n",
    "        else:\n",
    "            collector.append(items)\n",
    "    # rewritten_query = ' '.join([x for sublist in collector for x in sublist])\n",
    "    # print(\"Your input becomes: \", ' '.join(collector))\n",
    "    return collector\n",
    "\n",
    "\n",
    "def handling_spelling_mistakes(question_parts, vocabulary):\n",
    "    # Assumed user has spelling mistakes\n",
    "    collector = []\n",
    "    for items in question_parts:\n",
    "        best_matches = difflib.get_close_matches(items, vocabulary, n = 5, cutoff = 0.6)\n",
    "        if best_matches:\n",
    "            max_term = longest_match_within_best_matches(best_matches, items)\n",
    "            collector.append(max_term)\n",
    "            # print(\"Closest\", best_matches)\n",
    "    return collector\n",
    "\n",
    "\n",
    "def how_long_query_matched(collector, whole_corpus):\n",
    "    flag = True\n",
    "    for items in collector:\n",
    "        if whole_corpus.find(items) != -1 and flag is True:\n",
    "            max_matched = items\n",
    "            flag = False\n",
    "        elif whole_corpus.find(max_matched + \" \" + items) != -1:\n",
    "            max_matched += \" \" + items\n",
    "        else:\n",
    "            not_matched = items\n",
    "            break\n",
    "        \n",
    "    print(\"Maximum Sequence Matched: \", max_matched)\n",
    "    return max_matched, not_matched\n",
    "\n",
    "def unknown_word_sequence_handler(input_query, vocabulary, corpus_dict, corpus):\n",
    "    not_matched = 0\n",
    "    whole_corpus = ' '.join(corpus.Data.values)\n",
    "    single_token_query = input_query.split()\n",
    "    question_parts = [input_query] + making_query_collection(input_query) + single_token_query\n",
    "    # print(question_parts)\n",
    "    collector = query_rewritter_replacing_synonyms(single_token_query, whole_corpus)\n",
    "    max_matched, not_matched = how_long_query_matched(collector, whole_corpus)\n",
    "    voc_hints = handling_spelling_mistakes(question_parts, vocabulary)\n",
    "    print(\"Vocab hints from corpus: \", list(set(voc_hints)))\n",
    "    \n",
    "    tags, details = hash_tag_generator(corpus, max_matched)\n",
    "    if tags:\n",
    "        print(\"Suggestions: \", tags)\n",
    "        return tags, not_matched\n",
    "                \n",
    "def getKeysByValue(dictOfElements, valueToFind):\n",
    "    listOfKeys = list()\n",
    "    listOfItems = dictOfElements.items()\n",
    "    for item in listOfItems:\n",
    "        for synonyms in item[1]: \n",
    "            if synonyms == valueToFind:\n",
    "                listOfKeys.append(item[0])\n",
    "    return listOfKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dict = load_dictionary()\n",
    "while True:\n",
    "    input_query = input(\"Type your query: \") \n",
    "    if input_query and input_query is not \" \":\n",
    "        tags, details = hash_tag_generator(corpus, input_query)\n",
    "        if tags:\n",
    "            print(\"Suggested Tags: \", tags)\n",
    "        else:\n",
    "            # print('Word/Sequence not found ')\n",
    "            unknown_word_sequence_handler(input_query, vocabulary, corpus_dict, corpus)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________\n",
      "Input Query:  ウィルス 対策 実施\n",
      "Maximum Sequence Matched:  ウィルス 対策\n",
      "Vocab hints from corpus:  ['実施', 'ウィルス', '対策']\n",
      "Suggestions:  ['ウィルス 対策 定義', 'ウィルス 対策 スキャン', 'ウィルス 対策 検疫', '期間 ウィルス 対策', '説 ウィルス 対策', '社 ウィルス 対策', '表示 ウィルス 対策', 'ウィルス 対策 表', '番号 ウィルス 対策', 'ます ウィルス 対策', 'ウィルス 対策 ソフト', 'ウィルス 対策 端末', 'ウィルス 対策 管理', 'メニュー ウィルス 対策', 'ウィルス 対策 選択', '画面 ウィルス 対策', 'ウィルス 対策 ホワイト', 'ので ウィルス 対策', 'ウィルス 対策 ファイル', 'ない ウィルス 対策', 'ませ ウィルス 対策', 'Enterprise ウィルス 対策', 'ATP ウィルス 対策', 'ステータス ウィルス 対策', 'ID ウィルス 対策', 'バージョン ウィルス 対策', '日時 ウィルス 対策', 'Plus ウィルス 対策', 'ファイル ウィルス 対策', '従って ウィルス 対策', '必ず ウィルス 対策', '攻撃 ウィルス 対策', '検出 ウィルス 対策', 'Protection ウィルス 対策', 'により ウィルス 対策', 'サイト ウィルス 対策', '正常 ウィルス 対策', '問題 ウィルス 対策', '明 ウィルス 対策', 'いる ウィルス 対策', 'SOTI ウィルス 対策', '定期 ウィルス 対策', '端末 ウィルス 対策', '探す ウィルス 対策']\n",
      "ウィルス 対策 定義\n",
      "ウィルス 対策 スキャン\n",
      "ウィルス 対策 検疫\n",
      "期間 ウィルス 対策\n",
      "説 ウィルス 対策\n",
      "社 ウィルス 対策\n",
      "表示 ウィルス 対策\n",
      "ウィルス 対策 表\n",
      "番号 ウィルス 対策\n",
      "ます ウィルス 対策\n",
      "ウィルス 対策 ソフト\n",
      "ウィルス 対策 端末\n",
      "ウィルス 対策 管理\n",
      "メニュー ウィルス 対策\n",
      "ウィルス 対策 選択\n",
      "画面 ウィルス 対策\n",
      "ウィルス 対策 ホワイト\n",
      "ので ウィルス 対策\n",
      "ウィルス 対策 ファイル\n",
      "ない ウィルス 対策\n",
      "ませ ウィルス 対策\n",
      "Enterprise ウィルス 対策\n",
      "ATP ウィルス 対策\n",
      "ステータス ウィルス 対策\n",
      "ID ウィルス 対策\n",
      "バージョン ウィルス 対策\n",
      "日時 ウィルス 対策\n",
      "Plus ウィルス 対策\n",
      "ファイル ウィルス 対策\n",
      "従って ウィルス 対策\n",
      "必ず ウィルス 対策\n",
      "攻撃 ウィルス 対策\n",
      "検出 ウィルス 対策\n",
      "Protection ウィルス 対策\n",
      "により ウィルス 対策\n",
      "サイト ウィルス 対策\n",
      "正常 ウィルス 対策\n",
      "問題 ウィルス 対策\n",
      "明 ウィルス 対策\n",
      "いる ウィルス 対策\n",
      "SOTI ウィルス 対策\n",
      "定期 ウィルス 対策\n",
      "端末 ウィルス 対策\n",
      "探す ウィルス 対策\n"
     ]
    }
   ],
   "source": [
    "for index, col in cus_ques.iterrows():\n",
    "    print(\"________\\nInput Query: \", col['Question'])\n",
    "    input_query = col['Question']\n",
    "    tags, details = hash_tag_generator(corpus, input_query)\n",
    "    if tags:\n",
    "        print(\"Suggested Tags: \", tags)\n",
    "    else:\n",
    "        # print('Word/Sequence not found ')\n",
    "        tags, not_matched = unknown_word_sequence_handler(input_query, vocabulary, corpus_dict, corpus)\n",
    "    \n",
    "    \n",
    "    for chunks in tags:\n",
    "        print(chunks)\n",
    "        if chunks.find(not_matched) != -1:\n",
    "            print(chunks)\n",
    "    break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'実施'"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = difflib.SequenceMatcher(\n",
    "    None, string1, string2).get_matching_blocks()\n",
    "for match in matches:\n",
    "    print(string1[match.a:match.a + match.size])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

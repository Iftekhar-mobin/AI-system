{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UgZHR8TO0lFF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras import layers , activations , models , preprocessing, utils\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "from string import digits\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sxiGOLldKOQD"
   },
   "source": [
    "## 2) Loading Preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data_file = Path(\"/home/iftekhar/AI-system/Mixed/POL_workshop/processed_texts.txt\")\n",
    "with open(data_file, encoding='utf-8') as f:\n",
    "    data_list = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \" \".join(data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(all_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5465"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['多数', '駆除', 'cs', 'エクスプロラツール', '表徴']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(vocab)\n",
    "vocab[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'single_character_remover' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-384a3e967e18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mall_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mall_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msingle_character_remover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# lines = re.sub(r\"\\s+\", \" \", lines)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'single_character_remover' is not defined"
     ]
    }
   ],
   "source": [
    "all_text = \" \".join(data_list)\n",
    "all_text = single_character_remover(all_text)\n",
    "\n",
    "# lines = re.sub(r\"\\s+\", \" \", lines)\n",
    "\n",
    "# sentences = sent_tokenize(lines)\n",
    "# sent = np.asarray(sentences)\n",
    "#sentTwo = sent.reshape(len(sent)//2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_text_split(page_text, word_limit):\n",
    "    page_text = page_text.split()\n",
    "    chunks = [' '.join(page_text[i:i + word_limit]) for i in range(0,\n",
    "                                                                   len(page_text), word_limit)]\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def single_character_remover(text):\n",
    "    collector = []\n",
    "    for items in text.split():\n",
    "        if len(items) < 2:\n",
    "            replaced = re.sub(r'[ぁ-んァ-ン]', '', items)\n",
    "            replaced = re.sub(r'[A-Za-z]', '', replaced)\n",
    "            replaced = re.sub(r'[0-9]', '', replaced)\n",
    "            collector.append(replaced)\n",
    "        else:\n",
    "            collector.append(items)\n",
    "\n",
    "    return ' '.join([temp.strip(' ') for temp in collector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = page_text_split(all_text, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mobicontrol v manual mobicontrol v manual カスタマイザー タイトル 設定 カスタマイザー サブタイトル 設定 mobicontrol 設定 順序 mobicontrol 設定 順序 mobicontrol 設定',\n",
       " '順序 端末 os 対象 オンプレミス mobicontrol サーバ 必要 設定 事項 ios 端末 macos コンピュータ 必須 接続 プロファイル 作成 adds active']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = []\n",
    "for line in lines:\n",
    "    collector.append(line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['mobicontrol',\n",
       "  'v',\n",
       "  'manual',\n",
       "  'mobicontrol',\n",
       "  'v',\n",
       "  'manual',\n",
       "  'カスタマイザー',\n",
       "  'タイトル',\n",
       "  '設定',\n",
       "  'カスタマイザー',\n",
       "  'サブタイトル',\n",
       "  '設定',\n",
       "  'mobicontrol',\n",
       "  '設定',\n",
       "  '順序',\n",
       "  'mobicontrol',\n",
       "  '設定',\n",
       "  '順序',\n",
       "  'mobicontrol',\n",
       "  '設定'],\n",
       " ['順序',\n",
       "  '端末',\n",
       "  'os',\n",
       "  '対象',\n",
       "  'オンプレミス',\n",
       "  'mobicontrol',\n",
       "  'サーバ',\n",
       "  '必要',\n",
       "  '設定',\n",
       "  '事項',\n",
       "  'ios',\n",
       "  '端末',\n",
       "  'macos',\n",
       "  'コンピュータ',\n",
       "  '必須',\n",
       "  '接続',\n",
       "  'プロファイル',\n",
       "  '作成',\n",
       "  'adds',\n",
       "  'active']]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collector[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = np.asarray(lines)\n",
    "\n",
    "if len(sent)%2==0:\n",
    "    sentTwo = sent.reshape(len(sent)//2,2)\n",
    "else:\n",
    "    sent = sent[:-1]\n",
    "    sentTwo = sent.reshape(len(sent)//2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['mobicontrol manual mobicontrol manual カスタマイザー タイトル 設定 カスタマイザー サブタイトル 設定',\n",
       "        'mobicontrol 設定 順序 mobicontrol 設定 順序 mobicontrol 設定 順序 端末'],\n",
       "       ['os 対象 オンプレミス mobicontrol サーバ 必要 設定 事項 ios 端末',\n",
       "        'macos コンピュータ 必須 接続 プロファイル 作成 adds active directory domain']],\n",
       "      dtype='<U134')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentTwo[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9766"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(sentTwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([['mobicontrol manual mobicontrol manual カスタマイザー タイトル 設定 カスタマイザー サブタイトル 設定',\n",
       "         'os 対象 オンプレミス mobicontrol サーバ 必要 設定 事項 ios 端末'],\n",
       "        ['os 対象 オンプレミス mobicontrol サーバ 必要 設定 事項 ios 端末',\n",
       "         'service サーバ 間 ad f・s active directory federation service 間'],\n",
       "        ['service サーバ 間 ad f・s active directory federation service 間',\n",
       "         '端末 ios android 複数 従業員 共用 必須 apple google microsoft'],\n",
       "        ...,\n",
       "        ['コンソール 端末 管理 端末 ユーザ 当該 端末 デバイス pc タブレット',\n",
       "         'mobicontrol 機能 管理対象 端末 端末 ユーザ 自ら 端末 限定 端末'],\n",
       "        ['mobicontrol 機能 管理対象 端末 端末 ユーザ 自ら 端末 限定 端末',\n",
       "         'セルフサービスポータル ログイン microsoft exchange アカウントid パスワード 従業員 私物 端末 pc'],\n",
       "        ['セルフサービスポータル ログイン microsoft exchange アカウントid パスワード 従業員 私物 端末 pc',\n",
       "         'サーバ フロント サーバ exchange active sync サーバ サーバ 設定 スマホ']],\n",
       "       dtype='<U134'),\n",
       " array(['service サーバ 間 ad f・s active directory federation service 間',\n",
       "        '端末 ios android 複数 従業員 共用 必須 apple google microsoft',\n",
       "        '設定 app ストア アプリ 購入 managed google play アプリ アプリカタログ', ...,\n",
       "        'セルフサービスポータル ログイン microsoft exchange アカウントid パスワード 従業員 私物 端末 pc',\n",
       "        'サーバ フロント サーバ exchange active sync サーバ サーバ 設定 スマホ',\n",
       "        '未読 メール 通 数 表示 exchange active sync サーバ add-on'], dtype='<U134'))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_dataset(sentTwo, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame, concat\n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tdf = DataFrame(data)\n",
    "\tcols = []\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i+1))\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i-1))\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mobicontrol', 'v', 'manual', 'mobicontrol', 'v', 'manual', 'カスタマイザー', 'タイトル', '設定', 'カスタマイザー', 'サブタイトル', '設定', 'mobicontrol', '設定', '順序', 'mobicontrol', '設定', '順序', 'mobicontrol', '設定', '順序', '端末', 'os', '対象', 'オンプレミス', 'mobicontrol', 'サーバ', '必要', '設定', '事項']\n"
     ]
    }
   ],
   "source": [
    "question = []\n",
    "answer = []\n",
    "for items in collector:\n",
    "    print(items)\n",
    "    shifted_df = (series_to_supervised(items, 8, 8))\n",
    "    for i in range(len(shifted_df)):\n",
    "        saver = shifted_df.iloc[i].values.tolist()\n",
    "        length = len(saver)//2\n",
    "        question.append(saver[:length])\n",
    "        answer.append(saver[length:]) \n",
    "    break\n",
    "\n",
    "collect_question = []\n",
    "for items in question:\n",
    "    collect_question.append(' '.join(items))\n",
    "collect_answer = []\n",
    "for items in answer:\n",
    "    collect_answer.append(' '.join(items))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mobicontrol v manual mobicontrol v manual カスタマイザー タイトル',\n",
       " 'v manual mobicontrol v manual カスタマイザー タイトル 設定',\n",
       " 'manual mobicontrol v manual カスタマイザー タイトル 設定 カスタマイザー',\n",
       " 'mobicontrol v manual カスタマイザー タイトル 設定 カスタマイザー サブタイトル',\n",
       " 'v manual カスタマイザー タイトル 設定 カスタマイザー サブタイトル 設定']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect_question[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['サブタイトル 設定 mobicontrol 設定 順序 mobicontrol 設定 順序',\n",
       " '設定 mobicontrol 設定 順序 mobicontrol 設定 順序 mobicontrol',\n",
       " 'mobicontrol 設定 順序 mobicontrol 設定 順序 mobicontrol 設定',\n",
       " '設定 順序 mobicontrol 設定 順序 mobicontrol 設定 順序',\n",
       " '順序 mobicontrol 設定 順序 mobicontrol 設定 順序 端末']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect_answer[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mobicontrol v manual mobicontrol v manual カスタマ...</td>\n",
       "      <td>サブタイトル 設定 mobicontrol 設定 順序 mobicontrol 設定 順序</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v manual mobicontrol v manual カスタマイザー タイトル 設定</td>\n",
       "      <td>設定 mobicontrol 設定 順序 mobicontrol 設定 順序 mobicon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>manual mobicontrol v manual カスタマイザー タイトル 設定 カス...</td>\n",
       "      <td>mobicontrol 設定 順序 mobicontrol 設定 順序 mobicontro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mobicontrol v manual カスタマイザー タイトル 設定 カスタマイザー サ...</td>\n",
       "      <td>設定 順序 mobicontrol 設定 順序 mobicontrol 設定 順序</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v manual カスタマイザー タイトル 設定 カスタマイザー サブタイトル 設定</td>\n",
       "      <td>順序 mobicontrol 設定 順序 mobicontrol 設定 順序 端末</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Query  \\\n",
       "0  mobicontrol v manual mobicontrol v manual カスタマ...   \n",
       "1      v manual mobicontrol v manual カスタマイザー タイトル 設定   \n",
       "2  manual mobicontrol v manual カスタマイザー タイトル 設定 カス...   \n",
       "3  mobicontrol v manual カスタマイザー タイトル 設定 カスタマイザー サ...   \n",
       "4         v manual カスタマイザー タイトル 設定 カスタマイザー サブタイトル 設定   \n",
       "\n",
       "                                          Prediction  \n",
       "0      サブタイトル 設定 mobicontrol 設定 順序 mobicontrol 設定 順序  \n",
       "1  設定 mobicontrol 設定 順序 mobicontrol 設定 順序 mobicon...  \n",
       "2  mobicontrol 設定 順序 mobicontrol 設定 順序 mobicontro...  \n",
       "3          設定 順序 mobicontrol 設定 順序 mobicontrol 設定 順序  \n",
       "4          順序 mobicontrol 設定 順序 mobicontrol 設定 順序 端末  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = pd.DataFrame(zip(collect_question, collect_answer), columns=['Query', 'Prediction'])\n",
    "lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58594"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1, col2 = create_dataset(sentTwo)\n",
    "#col1 = col1.reshape(-1)\n",
    "col2 = col2.reshape(-1,1)\n",
    "#col2.shape\n",
    "dt = np.concatenate((col1,col2), axis=1)\n",
    "\n",
    "lines = pd.DataFrame(dt, columns=['Query', 'Prediction'])\n",
    "lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mobicontrol manual mobicontrol manual</td>\n",
       "      <td>START_ カスタマイザー タイトル 設定 カスタマイザー _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>manual mobicontrol manual カスタマイザー</td>\n",
       "      <td>START_ タイトル 設定 カスタマイザー サブタイトル _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mobicontrol manual カスタマイザー タイトル</td>\n",
       "      <td>START_ 設定 カスタマイザー サブタイトル 設定 _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mobicontrol 設定 順序 mobicontrol</td>\n",
       "      <td>START_ 設定 順序 mobicontrol 設定 _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>設定 順序 mobicontrol 設定</td>\n",
       "      <td>START_ 順序 mobicontrol 設定 順序 _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>順序 mobicontrol 設定 順序</td>\n",
       "      <td>START_ mobicontrol 設定 順序 端末 _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>os 対象 オンプレミス mobicontrol</td>\n",
       "      <td>START_ サーバ 必要 設定 事項 _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>対象 オンプレミス mobicontrol サーバ</td>\n",
       "      <td>START_ 必要 設定 事項 ios _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>オンプレミス mobicontrol サーバ 必要</td>\n",
       "      <td>START_ 設定 事項 ios 端末 _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>macos コンピュータ 必須 接続</td>\n",
       "      <td>START_ プロファイル 作成 adds active _END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Query                           Prediction\n",
       "0  mobicontrol manual mobicontrol manual  START_ カスタマイザー タイトル 設定 カスタマイザー _END\n",
       "1      manual mobicontrol manual カスタマイザー   START_ タイトル 設定 カスタマイザー サブタイトル _END\n",
       "2        mobicontrol manual カスタマイザー タイトル     START_ 設定 カスタマイザー サブタイトル 設定 _END\n",
       "3          mobicontrol 設定 順序 mobicontrol     START_ 設定 順序 mobicontrol 設定 _END\n",
       "4                   設定 順序 mobicontrol 設定     START_ 順序 mobicontrol 設定 順序 _END\n",
       "5                   順序 mobicontrol 設定 順序     START_ mobicontrol 設定 順序 端末 _END\n",
       "6               os 対象 オンプレミス mobicontrol             START_ サーバ 必要 設定 事項 _END\n",
       "7              対象 オンプレミス mobicontrol サーバ             START_ 必要 設定 事項 ios _END\n",
       "8              オンプレミス mobicontrol サーバ 必要             START_ 設定 事項 ios 端末 _END\n",
       "9                     macos コンピュータ 必須 接続    START_ プロファイル 作成 adds active _END"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add start and end tokens to target sequences\n",
    "lines.Prediction = lines.Prediction.apply(lambda x : 'START_ '+ x + ' _END')\n",
    "lines.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE : 9227\n"
     ]
    }
   ],
   "source": [
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts( lines.Query + lines.Prediction )\n",
    "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
    "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nF1mDKD_R6Os"
   },
   "source": [
    "### B) Reading the data from the files\n",
    "\n",
    "We parse each of the `.yaml` files.\n",
    "\n",
    "*   Concatenate two or more sentences if the answer has two or more of them.\n",
    "*   Remove unwanted data types which are produced while parsing the data.\n",
    "*   Append `<START>` and `<END>` to all the `answers`.\n",
    "*   Create a `Tokenizer` and load the whole vocabulary ( `questions` + `answers` ) into it.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mzsaO1YvS-M8"
   },
   "source": [
    "\n",
    "### C) Preparing data for Seq2Seq model\n",
    "\n",
    "Our model requires three arrays namely `encoder_input_data`, `decoder_input_data` and `decoder_output_data`.\n",
    "\n",
    "For `encoder_input_data` :\n",
    "* Tokenize the `questions`. Pad them to their maximum lquestionsth.\n",
    "\n",
    "For `decoder_input_data` :\n",
    "* Tokenize the `answers`. Pad them to their maximum lquestionsth.\n",
    "\n",
    "For `decoder_output_data` :\n",
    "\n",
    "* Tokenize the `answers`. Remove the first element from all the `tokenized_answers`. This is the `<START>` element which we added earlier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a5AD9ooQKc33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58594, 6) 6\n",
      "(58594, 8) 8\n",
      "(58594, 8, 9227)\n"
     ]
    }
   ],
   "source": [
    "questions = lines.Query \n",
    "answers = lines.Prediction\n",
    "\n",
    "# encoder_input_data\n",
    "tokenized_questions = tokenizer.texts_to_sequences(questions)\n",
    "maxlen_questions = max([len(x) for x in tokenized_questions])\n",
    "padded_questions = preprocessing.sequence.pad_sequences(tokenized_questions , maxlen=maxlen_questions , padding='post' )\n",
    "encoder_input_data = np.array(padded_questions)\n",
    "print(encoder_input_data.shape, maxlen_questions)\n",
    "\n",
    "# decoder_input_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "maxlen_answers = max([ len(x) for x in tokenized_answers ])\n",
    "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "decoder_input_data = np.array( padded_answers )\n",
    "print( decoder_input_data.shape , maxlen_answers )\n",
    "\n",
    "# decoder_output_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "for i in range(len(tokenized_answers)) :\n",
    "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\n",
    "decoder_output_data = np.array( onehot_answers )\n",
    "print( decoder_output_data.shape )\n",
    "\n",
    "# Saving all the arrays to storage\n",
    "#np.save( 'Saved Arrays/enc_in_data.npy' , encoder_input_data )\n",
    "#np.save( 'Saved Arrays/dec_in_data.npy' , decoder_input_data )\n",
    "#np.save( 'Saved Arrays/dec_tar_data.npy' , decoder_output_data )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4SwY3T139l19"
   },
   "source": [
    "## 3) Defining the Encoder-Decoder model\n",
    "The model will have Embedding, LSTM and Dense layers. The basic configuration is as follows.\n",
    "\n",
    "\n",
    "*   2 Input Layers : One for `encoder_input_data` and another for `decoder_input_data`.\n",
    "*   Embedding layer : For converting token vectors to fix sized dense vectors. **( Note :  Don't forget the `mask_zero=True` argument here )**\n",
    "*   LSTM layer : Provide access to Long-Short Term cells.\n",
    "\n",
    "Working : \n",
    "\n",
    "1.   The `encoder_input_data` comes in the Embedding layer (  `encoder_embedding` ). \n",
    "2.   The output of the Embedding layer goes to the LSTM cell which produces 2 state vectors ( `h` and `c` which are `encoder_states` )\n",
    "3.   These states are set in the LSTM cell of the decoder.\n",
    "4.   The decoder_input_data comes in through the Embedding layer.\n",
    "5.   The Embeddings goes in LSTM cell ( which had the states ) to produce seqeunces.\n",
    "\n",
    "**Important points :**\n",
    "\n",
    "\n",
    "*   `200` is the output of the GloVe embeddings.\n",
    "*   `embedding_matrix` is the GloVe embedding which we downloaded earlier.\n",
    "\n",
    "\n",
    "<center><img style=\"float: center;\" src=\"https://cdn-images-1.medium.com/max/1600/1*bnRvZDDapHF8Gk8soACtCQ.gif\"></center>\n",
    "\n",
    "\n",
    "Image credits to [Hackernoon](https://hackernoon.com/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-gUYtOwv21rt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 100)    622900      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    622900      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 100), (None, 80400       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 100),  80400       embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 6229)   629129      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,035,729\n",
      "Trainable params: 2,035,729\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "shape = 100\n",
    "\n",
    "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, shape , mask_zero=True ) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( shape , return_state=True )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, shape , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM( shape , return_state=True , return_sequences=True )\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n9g_8sR7WWf3"
   },
   "source": [
    "## 4) Training the model\n",
    "We train the model for a number of epochs with `RMSprop` optimizer and `categorical_crossentropy` loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N74NZnfo3Id-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "996/996 [==============================] - 26s 26ms/step - loss: 4.6889\n",
      "Epoch 2/5\n",
      "996/996 [==============================] - 25s 26ms/step - loss: 4.4408\n",
      "Epoch 3/5\n",
      "996/996 [==============================] - 26s 26ms/step - loss: 4.3236\n",
      "Epoch 4/5\n",
      "996/996 [==============================] - 25s 25ms/step - loss: 4.2058\n",
      "Epoch 5/5\n",
      "996/996 [==============================] - 26s 26ms/step - loss: 4.0936\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=10, epochs=5 ) \n",
    "model.save( 'model.h5' ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3sOLQr0M-lAe"
   },
   "source": [
    "## 5) Defining inference models\n",
    "We create inference models which help in predicting answers.\n",
    "\n",
    "**Encoder inference model** : Takes the question as input and outputs LSTM states ( `h` and `c` ).\n",
    "\n",
    "**Decoder inference model** : Takes in 2 inputs, one are the LSTM states ( Output of encoder model ), second are the answer input seqeunces ( ones not having the `<start>` tag ). It will output the answers for the question which we fed to the encoder model and its state values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1u5DE4qo3Mf2"
   },
   "outputs": [],
   "source": [
    "shape = 100\n",
    "\n",
    "def make_inference_models():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( shape ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( shape ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rxZp0ZRy-6dA"
   },
   "source": [
    "## 6) Talking with our Chatbot\n",
    "\n",
    "First, we define a method `str_to_tokens` which converts `str` questions to Integer tokens with padding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5P_wDD554q9O"
   },
   "outputs": [],
   "source": [
    "\n",
    "def str_to_tokens( sentence : str ):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    #return tokens_list\n",
    "    for word in words:\n",
    "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[138, 512,   1,  67,  24,   0,   0,   0,   0,   0,   0,   0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_to_tokens(\"会社 支給 端末 ios android\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "djEPrfJBmZE-"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "1.   First, we take a question as input and predict the state values using `enc_model`.\n",
    "2.   We set the state values in the decoder's LSTM.\n",
    "3.   Then, we generate a sequence which contains the `<start>` element.\n",
    "4.   We input this sequence in the `dec_model`.\n",
    "5.   We replace the `<start>` element with the element which was predicted by the `dec_model` and update the state values.\n",
    "6.   We carry out the above steps iteratively till we hit the `<end>` tag or the maximum answer lquestionsth.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2zBmN8qB3O-e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter question : 会社 支給 端末 ios android\n",
      " 端末 グループ 端末 グループ 端末 グループ 端末 グループ 端末 end\n",
      "Enter question : mobicontrol android setting\n",
      " 端末 ユーザ ユーザ ユーザ パスワード 入力 入力 端末 end\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/amiebot/experiment_env/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/amiebot/experiment_env/lib/python3.6/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 813\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    814\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/amiebot/experiment_env/lib/python3.6/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/amiebot/experiment_env/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-f2c5b220d3db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstates_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mstr_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'Enter question : '\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mempty_target_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mempty_target_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/amiebot/experiment_env/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         )\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/amiebot/experiment_env/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "enc_model , dec_model = make_inference_models()\n",
    "\n",
    "for _ in range(10):\n",
    "    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in tokenizer.word_index.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
    "            stop_condition = True\n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "\n",
    "    print( decoded_translation )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE : 941\n",
      "(1000, 5) 5\n",
      "(1000, 7) 7\n",
      "(1000, 7, 941)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, None, 100)    94100       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, None, 100)    94100       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 100), (None, 80400       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   [(None, None, 100),  80400       embedding_5[0][0]                \n",
      "                                                                 lstm_4[0][1]                     \n",
      "                                                                 lstm_4[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 941)    95041       lstm_5[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 444,041\n",
      "Trainable params: 444,041\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 4.0917\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 3.5444\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 3.3899\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 3.3231\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 3.2798\n",
      "Enter question : mobicontrol manual setting\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'setting'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6bf532e2d253>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mstates_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Enter question : '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0mempty_target_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mempty_target_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-6bf532e2d253>\u001b[0m in \u001b[0;36mstr_to_tokens\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# return tokens_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mtokens_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen_questions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'setting'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras import layers, activations, models, preprocessing, utils, optimizers, activations \n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "from string import digits\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "lines = pd.read_csv('seq2seq_dataframe.csv')\n",
    "lines = lines.head(1000)\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(lines.Query + lines.Prediction)\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "print('VOCAB SIZE : {}'.format(VOCAB_SIZE))\n",
    "\n",
    "questions = lines.Query\n",
    "answers = lines.Prediction\n",
    "\n",
    "# encoder_input_data\n",
    "tokenized_questions = tokenizer.texts_to_sequences(questions)\n",
    "maxlen_questions = max([len(x) for x in tokenized_questions])\n",
    "padded_questions = preprocessing.sequence.pad_sequences(tokenized_questions, maxlen=maxlen_questions, padding='post')\n",
    "encoder_input_data = np.array(padded_questions)\n",
    "print(encoder_input_data.shape, maxlen_questions)\n",
    "\n",
    "# decoder_input_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
    "maxlen_answers = max([len(x) for x in tokenized_answers])\n",
    "padded_answers = preprocessing.sequence.pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
    "decoder_input_data = np.array(padded_answers)\n",
    "print(decoder_input_data.shape, maxlen_answers)\n",
    "\n",
    "# decoder_output_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
    "for i in range(len(tokenized_answers)):\n",
    "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "padded_answers = preprocessing.sequence.pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
    "onehot_answers = utils.to_categorical(padded_answers, VOCAB_SIZE)\n",
    "decoder_output_data = np.array(onehot_answers)\n",
    "print(decoder_output_data.shape)\n",
    "\n",
    "shape = 100\n",
    "\n",
    "encoder_inputs = layers.Input(shape=(None,))\n",
    "encoder_embedding = layers.Embedding(VOCAB_SIZE, shape, mask_zero=True)(encoder_inputs)\n",
    "encoder_outputs, state_h, state_c = layers.LSTM(shape, return_state=True)(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = layers.Input(shape=(None,))\n",
    "decoder_embedding = layers.Embedding(VOCAB_SIZE, shape, mask_zero=True)(decoder_inputs)\n",
    "decoder_lstm = layers.LSTM(shape, return_state=True, return_sequences=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = layers.Dense(VOCAB_SIZE, activation=activations.softmax)\n",
    "output = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = models.Model([encoder_inputs, decoder_inputs], output)\n",
    "model.compile(optimizer=optimizers.RMSprop(), loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_output_data, \\\n",
    "          batch_size=10, epochs=5)\n",
    "model.save('model.h5')\n",
    "\n",
    "\n",
    "def make_inference_models():\n",
    "    encoder_model = models.Model(encoder_inputs, encoder_states)\n",
    "    decoder_state_input_h = layers.Input(shape=(shape,))\n",
    "    decoder_state_input_c = layers.Input(shape=(shape,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "\n",
    "    return encoder_model, decoder_model\n",
    "\n",
    "\n",
    "def str_to_tokens(sentence: str):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    # return tokens_list\n",
    "    for word in words:\n",
    "        tokens_list.append(tokenizer.word_index[word])\n",
    "    return preprocessing.sequence.pad_sequences([tokens_list], maxlen=maxlen_questions, padding='post')\n",
    "\n",
    "\n",
    "enc_model, dec_model = make_inference_models()\n",
    "\n",
    "for _ in range(10):\n",
    "    try:\n",
    "        states_values = enc_model.predict(str_to_tokens(input('Enter question : ')))\n",
    "    except KeyError:\n",
    "        continue\n",
    "        pass\n",
    "    empty_target_seq = np.zeros((1, 1))\n",
    "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition:\n",
    "        dec_outputs, h, c = dec_model.predict([empty_target_seq] + states_values)\n",
    "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
    "        sampled_word = None\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if sampled_word_index == index:\n",
    "                decoded_translation += ' {}'.format(word)\n",
    "                sampled_word = word\n",
    "\n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
    "            stop_condition = True\n",
    "        empty_target_seq = np.zeros((1, 1))\n",
    "        empty_target_seq[0, 0] = sampled_word_index\n",
    "        states_values = [h, c]\n",
    "\n",
    "    print(decoded_translation)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ChatBot_using_seq2seq.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  \n",
    "import numpy as np  \n",
    "import random  \n",
    "import string\n",
    "\n",
    "import bs4 as bs  \n",
    "import urllib.request  \n",
    "import re  \n",
    "\n",
    "raw_html = urllib.request.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')  \n",
    "raw_html = raw_html.read()\n",
    "article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
    "article_paragraphs = article_html.find_all('p')\n",
    "article_text = ''\n",
    "\n",
    "for para in article_paragraphs:  \n",
    "    article_text += para.text\n",
    "\n",
    "corpus = nltk.sent_tokenize(article_text)\n",
    "for i in range(len(corpus )):\n",
    "    corpus [i] = corpus [i].lower()\n",
    "    corpus [i] = re.sub(r'\\W',' ',corpus [i])\n",
    "    corpus [i] = re.sub(r'\\s+',' ',corpus [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural language processing nlp is a subfield of linguistics computer science information engineering and artificial intelligence concerned with the interactions between computers and human natural languages in particular how to program computers to process and analyze large amounts of natural language data ',\n",
       " 'challenges in natural language processing frequently involve speech recognition natural language understanding and natural language generation ',\n",
       " 'the history of natural language processing nlp generally started in the 1950s although work can be found from earlier periods ',\n",
       " 'in 1950 alan turing published an article titled computing machinery and intelligence which proposed what is now called the turing test as a criterion of intelligence clarification needed ',\n",
       " 'the georgetown experiment in 1954 involved fully automatic translation of more than sixty russian sentences into english ',\n",
       " 'the authors claimed that within three or five years machine translation would be a solved problem ',\n",
       " ' 2 however real progress was much slower and after the alpac report in 1966 which found that ten year long research had failed to fulfill the expectations funding for machine translation was dramatically reduced ',\n",
       " 'little further research in machine translation was conducted until the late 1980s when the first statistical machine translation systems were developed ',\n",
       " 'some notably successful natural language processing systems developed in the 1960s were shrdlu a natural language system working in restricted blocks worlds with restricted vocabularies and eliza a simulation of a rogerian psychotherapist written by joseph weizenbaum between 1964 and 1966 ',\n",
       " 'using almost no information about human thought or emotion eliza sometimes provided a startlingly human like interaction ',\n",
       " 'when the patient exceeded the very small knowledge base eliza might provide a generic response for example responding to my head hurts with why do you say your head hurts ',\n",
       " 'during the 1970s many programmers began to write conceptual ontologies which structured real world information into computer understandable data ',\n",
       " 'examples are margie schank 1975 sam cullingford 1978 pam wilensky 1978 talespin meehan 1976 qualm lehnert 1977 politics carbonell 1979 and plot units lehnert 1981 ',\n",
       " 'during this time many chatterbots were written including parry racter and jabberwacky ',\n",
       " 'up to the 1980s most natural language processing systems were based on complex sets of hand written rules ',\n",
       " 'starting in the late 1980s however there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing ',\n",
       " 'this was due to both the steady increase in computational power see moore s law and the gradual lessening of the dominance of chomskyan theories of linguistics e g ',\n",
       " 'transformational grammar whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine learning approach to language processing ',\n",
       " ' 3 some of the earliest used machine learning algorithms such as decision trees produced systems of hard if then rules similar to existing hand written rules ',\n",
       " 'however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data ',\n",
       " 'the cache language models upon which many speech recognition systems now rely are examples of such statistical models ',\n",
       " 'such models are generally more robust when given unfamiliar input especially input that contains errors as is very common for real world data and produce more reliable results when integrated into a larger system comprising multiple subtasks ',\n",
       " 'many of the notable early successes occurred in the field of machine translation due especially to work at ibm research where successively more complicated statistical models were developed ',\n",
       " 'these systems were able to take advantage of existing multilingual textual corpora that had been produced by the parliament of canada and the european union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government ',\n",
       " 'however most other systems depended on corpora specifically developed for the tasks implemented by these systems which was and often continues to be a major limitation in the success of these systems ',\n",
       " 'as a result a great deal of research has gone into methods of more effectively learning from limited amounts of data ',\n",
       " 'recent research has increasingly focused on unsupervised and semi supervised learning algorithms ',\n",
       " 'such algorithms can learn from data that has not been hand annotated with the desired answers or using a combination of annotated and non annotated data ',\n",
       " 'generally this task is much more difficult than supervised learning and typically produces less accurate results for a given amount of input data ',\n",
       " 'however there is an enormous amount of non annotated data available including among other things the entire content of the world wide web which can often make up for the inferior results if the algorithm used has a low enough time complexity to be practical ',\n",
       " 'in the 2010s representation learning and deep neural network style machine learning methods became widespread in natural language processing due in part to a flurry of results showing that such techniques 4 5 can achieve state of the art results in many natural language tasks for example in language modeling 6 parsing 7 8 and many others ',\n",
       " 'popular techniques include the use of word embeddings to capture semantic properties of words and an increase in end to end learning of a higher level task e g question answering instead of relying on a pipeline of separate intermediate tasks e g part of speech tagging and dependency parsing ',\n",
       " 'in some areas this shift has entailed substantial changes in how nlp systems are designed such that deep neural network based approaches may be viewed as a new paradigm distinct from statistical natural language processing ',\n",
       " 'for instance the term neural machine translation nmt emphasizes the fact that deep learning based approaches to machine translation directly learn sequence to sequence transformations obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation smt ',\n",
       " 'in the early days many language processing systems were designed by hand coding a set of rules 9 10 such as by writing grammars or devising heuristic rules for stemming ',\n",
       " 'since the so called statistical revolution 11 12 in the late 1980s and mid 1990s much natural language processing research has relied heavily on machine learning ',\n",
       " 'the machine learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora the plural form of corpus is a set of documents possibly with human or computer annotations of typical real world examples ',\n",
       " 'many different classes of machine learning algorithms have been applied to natural language processing tasks ',\n",
       " 'these algorithms take as input a large set of features that are generated from the input data ',\n",
       " 'some of the earliest used algorithms such as decision trees produced systems of hard if then rules similar to the systems of handwritten rules that were then common ',\n",
       " 'increasingly however research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to each input feature ',\n",
       " 'such models have the advantage that they can express the relative certainty of many different possible answers rather than only one producing more reliable results when such a model is included as a component of a larger system ',\n",
       " 'systems based on machine learning algorithms have many advantages over hand produced rules the following is a list of some of the most commonly researched tasks in natural language processing ',\n",
       " 'some of these tasks have direct real world applications while others more commonly serve as subtasks that are used to aid in solving larger tasks ',\n",
       " 'though natural language processing tasks are closely intertwined they are frequently subdivided into categories for convenience ',\n",
       " 'a coarse division is given below ',\n",
       " 'the first published work by an artificial intelligence was published in 2018 1 the road marketed as a novel contains sixty million words ',\n",
       " 'cognition refers to the mental action or process of acquiring knowledge and understanding through thought experience and the senses ',\n",
       " ' 19 cognitive science is the interdisciplinary scientific study of the mind and its processes ',\n",
       " ' 20 cognitive linguistics is an interdisciplinary branch of linguistics combining knowledge and research from both psychology and linguistics ',\n",
       " ' 21 george lakoff offers a methodology to build natural language processing nlp algorithms through the perspective of cognitive science along with the findings of cognitive linguistics 22 the first defining aspect of this cognitive task of nlp is the application of the theory of conceptual metaphor explained by lakoff as the understanding of one idea in terms of another which provides an idea of the intent of the author 23 ',\n",
       " 'for example consider some of the meanings in english of the word big ',\n",
       " 'when used as a comparative as in that is a big tree a likely inference of the intent of the author is that the author is using the word big to imply a statement about the tree being physically large in comparison to other trees or the authors experience ',\n",
       " 'when used as a stative verb as in tomorrow is a big day a likely inference of the author s intent it that big is being used to imply importance ',\n",
       " 'these examples are not presented to be complete but merely as indicators of the implication of the idea of conceptual metaphor ',\n",
       " 'the intent behind other usages like in she is a big person will remain somewhat ambiguous to a person and a cognitive nlp algorithm alike without additional information ',\n",
       " 'this leads to the second defining aspect of this cognitive task of nlp namely probabilistic context free grammar pcfg which enables cognitive nlp algorithms to assign relative measures of meaning to a word phrase sentence or piece of text based on the information presented before and after the piece of text being analyzed ',\n",
       " 'the mathematical equation for such algorithms is presented in us patent 9269353 where rmm is the relative measure of meaning token is any block of text sentence phrase or word n is the number of tokens being analyzed pmm is the probable measure of meaning based on a corpora n is one less than the number of tokens being analyzed d is the location of the token along the sequence of n tokens pf is the probability function specific to a language']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq = {}\n",
    "for sentence in corpus:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "most_freq = heapq.nlargest(200, wordfreq, key=wordfreq.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'of',\n",
       " 'a',\n",
       " 'to',\n",
       " 'in',\n",
       " 'and',\n",
       " 'language',\n",
       " 'is',\n",
       " 'natural',\n",
       " 'as',\n",
       " 'processing',\n",
       " 'that',\n",
       " 'machine',\n",
       " 'for',\n",
       " 'systems',\n",
       " 'learning',\n",
       " 'such',\n",
       " 'on',\n",
       " 'algorithms',\n",
       " 'data',\n",
       " 'which',\n",
       " 'translation',\n",
       " 'many',\n",
       " 'or',\n",
       " 'research',\n",
       " 'statistical',\n",
       " 'are',\n",
       " 'rules',\n",
       " 'nlp',\n",
       " 'more',\n",
       " 'was',\n",
       " 'were',\n",
       " 'based',\n",
       " 'used',\n",
       " 'models',\n",
       " 'has',\n",
       " 'tasks',\n",
       " 'cognitive',\n",
       " 'linguistics',\n",
       " 'with',\n",
       " 'real',\n",
       " 'when',\n",
       " 'some',\n",
       " 'by',\n",
       " 'this',\n",
       " 'input',\n",
       " 'be',\n",
       " 'from',\n",
       " 'an',\n",
       " 'into',\n",
       " 'however',\n",
       " 'results',\n",
       " 'these',\n",
       " 'word',\n",
       " 'big',\n",
       " 'information',\n",
       " 'can',\n",
       " 'world',\n",
       " 'hand',\n",
       " 'being',\n",
       " 'intelligence',\n",
       " 'human',\n",
       " 'large',\n",
       " 'speech',\n",
       " 'than',\n",
       " '1980s',\n",
       " 'developed',\n",
       " 'written',\n",
       " 'using',\n",
       " 'examples',\n",
       " 'produced',\n",
       " 'corpora',\n",
       " 'other',\n",
       " 'annotated',\n",
       " 'task',\n",
       " 'have',\n",
       " 'intent',\n",
       " 'author',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'understanding',\n",
       " 'generally',\n",
       " 'work',\n",
       " 'published',\n",
       " 'much',\n",
       " 'late',\n",
       " 'first',\n",
       " 'system',\n",
       " 'eliza',\n",
       " 'knowledge',\n",
       " 'example',\n",
       " 'conceptual',\n",
       " 'up',\n",
       " 'most',\n",
       " 'due',\n",
       " 'e',\n",
       " 'g',\n",
       " 'trees',\n",
       " 'if',\n",
       " 'then',\n",
       " 'part',\n",
       " 'increasingly',\n",
       " 'focused',\n",
       " 'make',\n",
       " 'probabilistic',\n",
       " 'given',\n",
       " 'larger',\n",
       " 'been',\n",
       " 'learn',\n",
       " 'deep',\n",
       " 'neural',\n",
       " 'sequence',\n",
       " 'set',\n",
       " 'inference',\n",
       " 'through',\n",
       " 'relative',\n",
       " 'one',\n",
       " 'idea',\n",
       " 'presented',\n",
       " 'meaning',\n",
       " 'text',\n",
       " 'analyzed',\n",
       " 'n',\n",
       " 'tokens',\n",
       " 'artificial',\n",
       " 'between',\n",
       " 'computers',\n",
       " 'languages',\n",
       " 'how',\n",
       " 'process',\n",
       " 'amounts',\n",
       " 'frequently',\n",
       " 'recognition',\n",
       " 'found',\n",
       " 'turing',\n",
       " 'now',\n",
       " 'called',\n",
       " 'sixty',\n",
       " 'english',\n",
       " 'authors',\n",
       " 'after',\n",
       " '1966',\n",
       " 'had',\n",
       " 'restricted',\n",
       " 'about',\n",
       " 'thought',\n",
       " 'like',\n",
       " 'very',\n",
       " 'head',\n",
       " 'hurts',\n",
       " 'during',\n",
       " '1978',\n",
       " 'lehnert',\n",
       " 'time',\n",
       " 'including',\n",
       " 'there',\n",
       " 'revolution',\n",
       " 'both',\n",
       " 'increase',\n",
       " 's',\n",
       " 'grammar',\n",
       " 'corpus',\n",
       " 'earliest',\n",
       " 'decision',\n",
       " 'hard',\n",
       " 'similar',\n",
       " 'existing',\n",
       " 'tagging',\n",
       " 'use',\n",
       " 'soft',\n",
       " 'decisions',\n",
       " 'attaching',\n",
       " 'valued',\n",
       " 'weights',\n",
       " 'features',\n",
       " 'especially',\n",
       " 'contains',\n",
       " 'common',\n",
       " 'reliable',\n",
       " 'subtasks',\n",
       " 'early',\n",
       " 'where',\n",
       " 'take',\n",
       " 'advantage',\n",
       " 'result',\n",
       " 'all',\n",
       " 'often',\n",
       " 'methods',\n",
       " 'supervised',\n",
       " 'not',\n",
       " 'answers',\n",
       " 'non',\n",
       " 'less',\n",
       " 'amount',\n",
       " 'algorithm',\n",
       " 'network',\n",
       " 'techniques',\n",
       " 'modeling',\n",
       " 'parsing',\n",
       " 'others']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'natural': 20,\n",
       " 'language': 26,\n",
       " 'processing': 17,\n",
       " 'nlp': 8,\n",
       " 'is': 26,\n",
       " 'a': 44,\n",
       " 'subfield': 1,\n",
       " 'of': 89,\n",
       " 'linguistics': 7,\n",
       " 'computer': 3,\n",
       " 'science': 3,\n",
       " 'information': 5,\n",
       " 'engineering': 1,\n",
       " 'and': 31,\n",
       " 'artificial': 2,\n",
       " 'intelligence': 4,\n",
       " 'concerned': 1,\n",
       " 'with': 7,\n",
       " 'the': 97,\n",
       " 'interactions': 1,\n",
       " 'between': 2,\n",
       " 'computers': 2,\n",
       " 'human': 4,\n",
       " 'languages': 2,\n",
       " 'in': 35,\n",
       " 'particular': 1,\n",
       " 'how': 2,\n",
       " 'to': 36,\n",
       " 'program': 1,\n",
       " 'process': 2,\n",
       " 'analyze': 1,\n",
       " 'large': 4,\n",
       " 'amounts': 2,\n",
       " 'data': 10,\n",
       " 'challenges': 1,\n",
       " 'frequently': 2,\n",
       " 'involve': 1,\n",
       " 'speech': 4,\n",
       " 'recognition': 2,\n",
       " 'understanding': 3,\n",
       " 'generation': 1,\n",
       " 'history': 1,\n",
       " 'generally': 3,\n",
       " 'started': 1,\n",
       " '1950s': 1,\n",
       " 'although': 1,\n",
       " 'work': 3,\n",
       " 'can': 5,\n",
       " 'be': 6,\n",
       " 'found': 2,\n",
       " 'from': 6,\n",
       " 'earlier': 1,\n",
       " 'periods': 1,\n",
       " '1950': 1,\n",
       " 'alan': 1,\n",
       " 'turing': 2,\n",
       " 'published': 3,\n",
       " 'an': 6,\n",
       " 'article': 1,\n",
       " 'titled': 1,\n",
       " 'computing': 1,\n",
       " 'machinery': 1,\n",
       " 'which': 10,\n",
       " 'proposed': 1,\n",
       " 'what': 1,\n",
       " 'now': 2,\n",
       " 'called': 2,\n",
       " 'test': 1,\n",
       " 'as': 19,\n",
       " 'criterion': 1,\n",
       " 'clarification': 1,\n",
       " 'needed': 1,\n",
       " 'georgetown': 1,\n",
       " 'experiment': 1,\n",
       " '1954': 1,\n",
       " 'involved': 1,\n",
       " 'fully': 1,\n",
       " 'automatic': 1,\n",
       " 'translation': 10,\n",
       " 'more': 8,\n",
       " 'than': 4,\n",
       " 'sixty': 2,\n",
       " 'russian': 1,\n",
       " 'sentences': 1,\n",
       " 'into': 6,\n",
       " 'english': 2,\n",
       " 'authors': 2,\n",
       " 'claimed': 1,\n",
       " 'that': 17,\n",
       " 'within': 1,\n",
       " 'three': 1,\n",
       " 'or': 9,\n",
       " 'five': 1,\n",
       " 'years': 1,\n",
       " 'machine': 16,\n",
       " 'would': 1,\n",
       " 'solved': 1,\n",
       " 'problem': 1,\n",
       " '2': 1,\n",
       " 'however': 6,\n",
       " 'real': 7,\n",
       " 'progress': 1,\n",
       " 'was': 8,\n",
       " 'much': 3,\n",
       " 'slower': 1,\n",
       " 'after': 2,\n",
       " 'alpac': 1,\n",
       " 'report': 1,\n",
       " '1966': 2,\n",
       " 'ten': 1,\n",
       " 'year': 1,\n",
       " 'long': 1,\n",
       " 'research': 9,\n",
       " 'had': 2,\n",
       " 'failed': 1,\n",
       " 'fulfill': 1,\n",
       " 'expectations': 1,\n",
       " 'funding': 1,\n",
       " 'for': 16,\n",
       " 'dramatically': 1,\n",
       " 'reduced': 1,\n",
       " 'little': 1,\n",
       " 'further': 1,\n",
       " 'conducted': 1,\n",
       " 'until': 1,\n",
       " 'late': 3,\n",
       " '1980s': 4,\n",
       " 'when': 7,\n",
       " 'first': 3,\n",
       " 'statistical': 9,\n",
       " 'systems': 15,\n",
       " 'were': 8,\n",
       " 'developed': 4,\n",
       " 'some': 7,\n",
       " 'notably': 1,\n",
       " 'successful': 1,\n",
       " '1960s': 1,\n",
       " 'shrdlu': 1,\n",
       " 'system': 3,\n",
       " 'working': 1,\n",
       " 'restricted': 2,\n",
       " 'blocks': 1,\n",
       " 'worlds': 1,\n",
       " 'vocabularies': 1,\n",
       " 'eliza': 3,\n",
       " 'simulation': 1,\n",
       " 'rogerian': 1,\n",
       " 'psychotherapist': 1,\n",
       " 'written': 4,\n",
       " 'by': 7,\n",
       " 'joseph': 1,\n",
       " 'weizenbaum': 1,\n",
       " '1964': 1,\n",
       " 'using': 4,\n",
       " 'almost': 1,\n",
       " 'no': 1,\n",
       " 'about': 2,\n",
       " 'thought': 2,\n",
       " 'emotion': 1,\n",
       " 'sometimes': 1,\n",
       " 'provided': 1,\n",
       " 'startlingly': 1,\n",
       " 'like': 2,\n",
       " 'interaction': 1,\n",
       " 'patient': 1,\n",
       " 'exceeded': 1,\n",
       " 'very': 2,\n",
       " 'small': 1,\n",
       " 'knowledge': 3,\n",
       " 'base': 1,\n",
       " 'might': 1,\n",
       " 'provide': 1,\n",
       " 'generic': 1,\n",
       " 'response': 1,\n",
       " 'example': 3,\n",
       " 'responding': 1,\n",
       " 'my': 1,\n",
       " 'head': 2,\n",
       " 'hurts': 2,\n",
       " 'why': 1,\n",
       " 'do': 1,\n",
       " 'you': 1,\n",
       " 'say': 1,\n",
       " 'your': 1,\n",
       " 'during': 2,\n",
       " '1970s': 1,\n",
       " 'many': 10,\n",
       " 'programmers': 1,\n",
       " 'began': 1,\n",
       " 'write': 1,\n",
       " 'conceptual': 3,\n",
       " 'ontologies': 1,\n",
       " 'structured': 1,\n",
       " 'world': 5,\n",
       " 'understandable': 1,\n",
       " 'examples': 4,\n",
       " 'are': 9,\n",
       " 'margie': 1,\n",
       " 'schank': 1,\n",
       " '1975': 1,\n",
       " 'sam': 1,\n",
       " 'cullingford': 1,\n",
       " '1978': 2,\n",
       " 'pam': 1,\n",
       " 'wilensky': 1,\n",
       " 'talespin': 1,\n",
       " 'meehan': 1,\n",
       " '1976': 1,\n",
       " 'qualm': 1,\n",
       " 'lehnert': 2,\n",
       " '1977': 1,\n",
       " 'politics': 1,\n",
       " 'carbonell': 1,\n",
       " '1979': 1,\n",
       " 'plot': 1,\n",
       " 'units': 1,\n",
       " '1981': 1,\n",
       " 'this': 7,\n",
       " 'time': 2,\n",
       " 'chatterbots': 1,\n",
       " 'including': 2,\n",
       " 'parry': 1,\n",
       " 'racter': 1,\n",
       " 'jabberwacky': 1,\n",
       " 'up': 3,\n",
       " 'most': 3,\n",
       " 'based': 8,\n",
       " 'on': 12,\n",
       " 'complex': 1,\n",
       " 'sets': 1,\n",
       " 'hand': 5,\n",
       " 'rules': 9,\n",
       " 'starting': 1,\n",
       " 'there': 2,\n",
       " 'revolution': 2,\n",
       " 'introduction': 1,\n",
       " 'learning': 14,\n",
       " 'algorithms': 11,\n",
       " 'due': 3,\n",
       " 'both': 2,\n",
       " 'steady': 1,\n",
       " 'increase': 2,\n",
       " 'computational': 1,\n",
       " 'power': 1,\n",
       " 'see': 1,\n",
       " 'moore': 1,\n",
       " 's': 2,\n",
       " 'law': 1,\n",
       " 'gradual': 1,\n",
       " 'lessening': 1,\n",
       " 'dominance': 1,\n",
       " 'chomskyan': 1,\n",
       " 'theories': 1,\n",
       " 'e': 3,\n",
       " 'g': 3,\n",
       " 'transformational': 1,\n",
       " 'grammar': 2,\n",
       " 'whose': 1,\n",
       " 'theoretical': 1,\n",
       " 'underpinnings': 1,\n",
       " 'discouraged': 1,\n",
       " 'sort': 1,\n",
       " 'corpus': 2,\n",
       " 'underlies': 1,\n",
       " 'approach': 1,\n",
       " '3': 1,\n",
       " 'earliest': 2,\n",
       " 'used': 8,\n",
       " 'such': 13,\n",
       " 'decision': 2,\n",
       " 'trees': 3,\n",
       " 'produced': 4,\n",
       " 'hard': 2,\n",
       " 'if': 3,\n",
       " 'then': 3,\n",
       " 'similar': 2,\n",
       " 'existing': 2,\n",
       " 'part': 3,\n",
       " 'tagging': 2,\n",
       " 'introduced': 1,\n",
       " 'use': 2,\n",
       " 'hidden': 1,\n",
       " 'markov': 1,\n",
       " 'models': 8,\n",
       " 'increasingly': 3,\n",
       " 'has': 8,\n",
       " 'focused': 3,\n",
       " 'make': 3,\n",
       " 'soft': 2,\n",
       " 'probabilistic': 3,\n",
       " 'decisions': 2,\n",
       " 'attaching': 2,\n",
       " 'valued': 2,\n",
       " 'weights': 2,\n",
       " 'features': 2,\n",
       " 'making': 1,\n",
       " 'input': 7,\n",
       " 'cache': 1,\n",
       " 'upon': 1,\n",
       " 'rely': 1,\n",
       " 'robust': 1,\n",
       " 'given': 3,\n",
       " 'unfamiliar': 1,\n",
       " 'especially': 2,\n",
       " 'contains': 2,\n",
       " 'errors': 1,\n",
       " 'common': 2,\n",
       " 'produce': 1,\n",
       " 'reliable': 2,\n",
       " 'results': 6,\n",
       " 'integrated': 1,\n",
       " 'larger': 3,\n",
       " 'comprising': 1,\n",
       " 'multiple': 1,\n",
       " 'subtasks': 2,\n",
       " 'notable': 1,\n",
       " 'early': 2,\n",
       " 'successes': 1,\n",
       " 'occurred': 1,\n",
       " 'field': 1,\n",
       " 'at': 1,\n",
       " 'ibm': 1,\n",
       " 'where': 2,\n",
       " 'successively': 1,\n",
       " 'complicated': 1,\n",
       " 'these': 6,\n",
       " 'able': 1,\n",
       " 'take': 2,\n",
       " 'advantage': 2,\n",
       " 'multilingual': 1,\n",
       " 'textual': 1,\n",
       " 'corpora': 4,\n",
       " 'been': 3,\n",
       " 'parliament': 1,\n",
       " 'canada': 1,\n",
       " 'european': 1,\n",
       " 'union': 1,\n",
       " 'result': 2,\n",
       " 'laws': 1,\n",
       " 'calling': 1,\n",
       " 'all': 2,\n",
       " 'governmental': 1,\n",
       " 'proceedings': 1,\n",
       " 'official': 1,\n",
       " 'corresponding': 1,\n",
       " 'government': 1,\n",
       " 'other': 4,\n",
       " 'depended': 1,\n",
       " 'specifically': 1,\n",
       " 'tasks': 8,\n",
       " 'implemented': 1,\n",
       " 'often': 2,\n",
       " 'continues': 1,\n",
       " 'major': 1,\n",
       " 'limitation': 1,\n",
       " 'success': 1,\n",
       " 'great': 1,\n",
       " 'deal': 1,\n",
       " 'gone': 1,\n",
       " 'methods': 2,\n",
       " 'effectively': 1,\n",
       " 'limited': 1,\n",
       " 'recent': 1,\n",
       " 'unsupervised': 1,\n",
       " 'semi': 1,\n",
       " 'supervised': 2,\n",
       " 'learn': 3,\n",
       " 'not': 2,\n",
       " 'annotated': 4,\n",
       " 'desired': 1,\n",
       " 'answers': 2,\n",
       " 'combination': 1,\n",
       " 'non': 2,\n",
       " 'task': 4,\n",
       " 'difficult': 1,\n",
       " 'typically': 1,\n",
       " 'produces': 1,\n",
       " 'less': 2,\n",
       " 'accurate': 1,\n",
       " 'amount': 2,\n",
       " 'enormous': 1,\n",
       " 'available': 1,\n",
       " 'among': 1,\n",
       " 'things': 1,\n",
       " 'entire': 1,\n",
       " 'content': 1,\n",
       " 'wide': 1,\n",
       " 'web': 1,\n",
       " 'inferior': 1,\n",
       " 'algorithm': 2,\n",
       " 'low': 1,\n",
       " 'enough': 1,\n",
       " 'complexity': 1,\n",
       " 'practical': 1,\n",
       " '2010s': 1,\n",
       " 'representation': 1,\n",
       " 'deep': 3,\n",
       " 'neural': 3,\n",
       " 'network': 2,\n",
       " 'style': 1,\n",
       " 'became': 1,\n",
       " 'widespread': 1,\n",
       " 'flurry': 1,\n",
       " 'showing': 1,\n",
       " 'techniques': 2,\n",
       " '4': 1,\n",
       " '5': 1,\n",
       " 'achieve': 1,\n",
       " 'state': 1,\n",
       " 'art': 1,\n",
       " 'modeling': 2,\n",
       " '6': 1,\n",
       " 'parsing': 2,\n",
       " '7': 1,\n",
       " '8': 1,\n",
       " 'others': 2,\n",
       " 'popular': 1,\n",
       " 'include': 1,\n",
       " 'word': 6,\n",
       " 'embeddings': 1,\n",
       " 'capture': 1,\n",
       " 'semantic': 1,\n",
       " 'properties': 1,\n",
       " 'words': 2,\n",
       " 'end': 2,\n",
       " 'higher': 1,\n",
       " 'level': 1,\n",
       " 'question': 1,\n",
       " 'answering': 1,\n",
       " 'instead': 2,\n",
       " 'relying': 1,\n",
       " 'pipeline': 1,\n",
       " 'separate': 1,\n",
       " 'intermediate': 2,\n",
       " 'dependency': 1,\n",
       " 'areas': 1,\n",
       " 'shift': 1,\n",
       " 'entailed': 1,\n",
       " 'substantial': 1,\n",
       " 'changes': 1,\n",
       " 'designed': 2,\n",
       " 'approaches': 2,\n",
       " 'may': 1,\n",
       " 'viewed': 1,\n",
       " 'new': 1,\n",
       " 'paradigm': 2,\n",
       " 'distinct': 1,\n",
       " 'instance': 1,\n",
       " 'term': 1,\n",
       " 'nmt': 1,\n",
       " 'emphasizes': 1,\n",
       " 'fact': 1,\n",
       " 'directly': 1,\n",
       " 'sequence': 3,\n",
       " 'transformations': 1,\n",
       " 'obviating': 1,\n",
       " 'need': 1,\n",
       " 'steps': 1,\n",
       " 'alignment': 1,\n",
       " 'smt': 1,\n",
       " 'days': 1,\n",
       " 'coding': 1,\n",
       " 'set': 3,\n",
       " '9': 1,\n",
       " '10': 1,\n",
       " 'writing': 1,\n",
       " 'grammars': 1,\n",
       " 'devising': 1,\n",
       " 'heuristic': 1,\n",
       " 'stemming': 1,\n",
       " 'since': 1,\n",
       " 'so': 1,\n",
       " '11': 1,\n",
       " '12': 1,\n",
       " 'mid': 1,\n",
       " '1990s': 1,\n",
       " 'relied': 1,\n",
       " 'heavily': 1,\n",
       " 'calls': 1,\n",
       " 'inference': 3,\n",
       " 'automatically': 1,\n",
       " 'through': 3,\n",
       " 'analysis': 1,\n",
       " 'plural': 1,\n",
       " 'form': 1,\n",
       " 'documents': 1,\n",
       " 'possibly': 1,\n",
       " 'annotations': 1,\n",
       " 'typical': 1,\n",
       " 'different': 2,\n",
       " 'classes': 1,\n",
       " 'have': 4,\n",
       " 'applied': 1,\n",
       " 'generated': 1,\n",
       " 'handwritten': 1,\n",
       " 'each': 1,\n",
       " 'feature': 1,\n",
       " 'they': 2,\n",
       " 'express': 1,\n",
       " 'relative': 3,\n",
       " 'certainty': 1,\n",
       " 'possible': 1,\n",
       " 'rather': 1,\n",
       " 'only': 1,\n",
       " 'one': 3,\n",
       " 'producing': 1,\n",
       " 'model': 1,\n",
       " 'included': 1,\n",
       " 'component': 1,\n",
       " 'advantages': 1,\n",
       " 'over': 1,\n",
       " 'following': 1,\n",
       " 'list': 1,\n",
       " 'commonly': 2,\n",
       " 'researched': 1,\n",
       " 'direct': 1,\n",
       " 'applications': 1,\n",
       " 'while': 1,\n",
       " 'serve': 1,\n",
       " 'aid': 1,\n",
       " 'solving': 1,\n",
       " 'though': 1,\n",
       " 'closely': 1,\n",
       " 'intertwined': 1,\n",
       " 'subdivided': 1,\n",
       " 'categories': 1,\n",
       " 'convenience': 1,\n",
       " 'coarse': 1,\n",
       " 'division': 1,\n",
       " 'below': 1,\n",
       " '2018': 1,\n",
       " '1': 1,\n",
       " 'road': 1,\n",
       " 'marketed': 1,\n",
       " 'novel': 1,\n",
       " 'million': 1,\n",
       " 'cognition': 1,\n",
       " 'refers': 1,\n",
       " 'mental': 1,\n",
       " 'action': 1,\n",
       " 'acquiring': 1,\n",
       " 'experience': 2,\n",
       " 'senses': 1,\n",
       " '19': 1,\n",
       " 'cognitive': 8,\n",
       " 'interdisciplinary': 2,\n",
       " 'scientific': 1,\n",
       " 'study': 1,\n",
       " 'mind': 1,\n",
       " 'its': 1,\n",
       " 'processes': 1,\n",
       " '20': 1,\n",
       " 'branch': 1,\n",
       " 'combining': 1,\n",
       " 'psychology': 1,\n",
       " '21': 1,\n",
       " 'george': 1,\n",
       " 'lakoff': 2,\n",
       " 'offers': 1,\n",
       " 'methodology': 1,\n",
       " 'build': 1,\n",
       " 'perspective': 1,\n",
       " 'along': 2,\n",
       " 'findings': 1,\n",
       " '22': 1,\n",
       " 'defining': 2,\n",
       " 'aspect': 2,\n",
       " 'application': 1,\n",
       " 'theory': 1,\n",
       " 'metaphor': 2,\n",
       " 'explained': 1,\n",
       " 'idea': 3,\n",
       " 'terms': 1,\n",
       " 'another': 1,\n",
       " 'provides': 1,\n",
       " 'intent': 4,\n",
       " 'author': 4,\n",
       " '23': 1,\n",
       " 'consider': 1,\n",
       " 'meanings': 1,\n",
       " 'big': 6,\n",
       " 'comparative': 1,\n",
       " 'tree': 2,\n",
       " 'likely': 2,\n",
       " 'imply': 2,\n",
       " 'statement': 1,\n",
       " 'being': 5,\n",
       " 'physically': 1,\n",
       " 'comparison': 1,\n",
       " 'stative': 1,\n",
       " 'verb': 1,\n",
       " 'tomorrow': 1,\n",
       " 'day': 1,\n",
       " 'it': 1,\n",
       " 'importance': 1,\n",
       " 'presented': 3,\n",
       " 'complete': 1,\n",
       " 'but': 1,\n",
       " 'merely': 1,\n",
       " 'indicators': 1,\n",
       " 'implication': 1,\n",
       " 'behind': 1,\n",
       " 'usages': 1,\n",
       " 'she': 1,\n",
       " 'person': 2,\n",
       " 'will': 1,\n",
       " 'remain': 1,\n",
       " 'somewhat': 1,\n",
       " 'ambiguous': 1,\n",
       " 'alike': 1,\n",
       " 'without': 1,\n",
       " 'additional': 1,\n",
       " 'leads': 1,\n",
       " 'second': 1,\n",
       " 'namely': 1,\n",
       " 'context': 1,\n",
       " 'free': 1,\n",
       " 'pcfg': 1,\n",
       " 'enables': 1,\n",
       " 'assign': 1,\n",
       " 'measures': 1,\n",
       " 'meaning': 3,\n",
       " 'phrase': 2,\n",
       " 'sentence': 2,\n",
       " 'piece': 2,\n",
       " 'text': 3,\n",
       " 'before': 1,\n",
       " 'analyzed': 3,\n",
       " 'mathematical': 1,\n",
       " 'equation': 1,\n",
       " 'us': 1,\n",
       " 'patent': 1,\n",
       " '9269353': 1,\n",
       " 'rmm': 1,\n",
       " 'measure': 2,\n",
       " 'token': 2,\n",
       " 'any': 1,\n",
       " 'block': 1,\n",
       " 'n': 3,\n",
       " 'number': 2,\n",
       " 'tokens': 3,\n",
       " 'pmm': 1,\n",
       " 'probable': 1,\n",
       " 'd': 1,\n",
       " 'location': 1,\n",
       " 'pf': 1,\n",
       " 'probability': 1,\n",
       " 'function': 1,\n",
       " 'specific': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "for sentence in corpus:\n",
    "    sentence_tokens = nltk.word_tokenize(sentence)\n",
    "    sent_vec = []\n",
    "    for token in most_freq:\n",
    "        if token in sentence_tokens:\n",
    "            sent_vec.append(1)\n",
    "        else:\n",
    "            sent_vec.append(0)\n",
    "    sentence_vectors.append(sent_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentence_vectors)\n",
    "len(sentence_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(sentence_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2    3    4    5    6    7    8    9    ...  190  191  192  193  \\\n",
       "0     1    1    1    1    1    1    1    1    1    0  ...    0    0    0    0   \n",
       "1     0    0    0    0    1    1    1    0    1    0  ...    0    0    0    0   \n",
       "2     1    1    0    0    1    0    1    0    1    0  ...    0    0    0    0   \n",
       "3     1    1    1    0    1    1    0    1    0    1  ...    0    0    0    0   \n",
       "4     1    1    0    0    1    0    0    0    0    0  ...    0    0    0    0   \n",
       "5     1    0    1    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "6     1    0    0    1    1    1    0    0    0    0  ...    0    0    0    0   \n",
       "7     1    0    0    0    1    0    0    0    0    0  ...    0    0    0    0   \n",
       "8     1    1    1    0    1    1    1    0    1    0  ...    0    0    0    0   \n",
       "9     0    0    1    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "10    1    0    1    1    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "11    1    0    0    1    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "12    0    0    0    0    0    1    0    0    0    0  ...    0    0    0    0   \n",
       "13    0    0    0    0    0    1    0    0    0    0  ...    0    0    0    0   \n",
       "14    1    1    0    1    0    0    1    0    1    0  ...    0    0    0    0   \n",
       "15    1    1    1    0    1    0    1    0    1    0  ...    0    0    0    0   \n",
       "16    1    1    0    1    1    1    0    0    0    0  ...    0    0    0    0   \n",
       "17    1    1    0    1    0    0    1    0    0    0  ...    0    0    0    0   \n",
       "18    1    1    0    1    0    0    0    0    0    1  ...    0    0    0    0   \n",
       "19    1    1    0    1    0    1    1    0    1    0  ...    0    0    0    0   \n",
       "20    1    1    0    0    0    0    1    0    0    0  ...    0    0    0    0   \n",
       "21    0    0    1    0    0    1    0    1    0    1  ...    0    0    0    0   \n",
       "22    1    1    0    1    1    0    0    0    0    0  ...    0    0    0    0   \n",
       "23    1    1    1    1    0    1    0    0    0    1  ...    0    0    0    0   \n",
       "24    1    1    1    1    1    1    0    0    0    0  ...    0    0    0    0   \n",
       "25    0    1    1    0    0    0    0    0    0    1  ...    0    0    0    0   \n",
       "26    0    0    0    0    0    1    0    0    0    0  ...    0    0    0    0   \n",
       "27    1    1    1    0    0    1    0    0    0    0  ...    1    1    0    0   \n",
       "28    0    1    1    0    0    1    0    1    0    0  ...    0    0    1    1   \n",
       "29    1    1    1    1    0    0    0    1    0    0  ...    0    1    0    1   \n",
       "30    1    1    1    1    1    1    1    0    1    0  ...    0    0    0    0   \n",
       "31    1    1    1    1    1    1    0    0    0    0  ...    0    0    0    0   \n",
       "32    0    0    1    0    1    0    1    0    1    1  ...    0    0    0    0   \n",
       "33    1    0    0    1    1    1    1    0    0    1  ...    0    0    0    0   \n",
       "34    1    1    1    0    1    0    1    0    0    1  ...    0    0    0    0   \n",
       "35    1    0    0    0    1    1    1    0    1    0  ...    0    0    0    0   \n",
       "36    1    1    1    1    0    0    0    1    0    0  ...    0    0    0    0   \n",
       "37    0    1    0    1    0    0    1    0    1    0  ...    0    0    0    0   \n",
       "38    1    1    1    0    0    0    0    0    0    1  ...    0    0    0    0   \n",
       "39    1    1    0    1    0    0    0    0    0    1  ...    0    0    0    0   \n",
       "40    0    0    0    1    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "41    1    1    1    0    0    0    0    1    0    1  ...    1    0    0    0   \n",
       "42    1    1    1    0    1    0    1    1    1    0  ...    0    0    0    0   \n",
       "43    0    1    0    1    1    0    0    0    0    1  ...    0    0    0    0   \n",
       "44    0    0    0    0    0    0    1    0    1    0  ...    0    0    0    0   \n",
       "45    0    0    1    0    0    0    0    1    0    0  ...    0    0    0    0   \n",
       "46    1    0    1    0    1    0    0    0    0    1  ...    0    0    0    0   \n",
       "47    1    1    0    1    0    1    0    0    0    0  ...    0    0    0    0   \n",
       "48    1    1    0    0    0    1    0    1    0    0  ...    0    0    0    0   \n",
       "49    0    1    0    0    0    1    0    1    0    0  ...    0    0    0    0   \n",
       "50    1    1    1    1    1    0    1    1    1    1  ...    0    0    0    0   \n",
       "51    1    1    0    0    1    0    0    0    0    0  ...    0    0    0    0   \n",
       "52    1    1    1    1    1    0    0    1    0    1  ...    0    0    0    0   \n",
       "53    1    1    1    1    1    0    0    1    0    1  ...    0    0    0    0   \n",
       "54    1    1    0    1    0    0    0    0    0    1  ...    0    0    0    0   \n",
       "55    1    0    1    1    1    1    0    1    0    0  ...    0    0    0    0   \n",
       "56    1    1    1    1    0    1    0    0    0    0  ...    0    0    0    0   \n",
       "57    1    1    1    1    1    0    1    1    0    0  ...    0    0    1    0   \n",
       "\n",
       "    194  195  196  197  198  199  \n",
       "0     0    0    0    0    0    0  \n",
       "1     0    0    0    0    0    0  \n",
       "2     0    0    0    0    0    0  \n",
       "3     0    0    0    0    0    0  \n",
       "4     0    0    0    0    0    0  \n",
       "5     0    0    0    0    0    0  \n",
       "6     0    0    0    0    0    0  \n",
       "7     0    0    0    0    0    0  \n",
       "8     0    0    0    0    0    0  \n",
       "9     0    0    0    0    0    0  \n",
       "10    0    0    0    0    0    0  \n",
       "11    0    0    0    0    0    0  \n",
       "12    0    0    0    0    0    0  \n",
       "13    0    0    0    0    0    0  \n",
       "14    0    0    0    0    0    0  \n",
       "15    0    0    0    0    0    0  \n",
       "16    0    0    0    0    0    0  \n",
       "17    0    0    0    0    0    0  \n",
       "18    0    0    0    0    0    0  \n",
       "19    0    0    0    0    0    0  \n",
       "20    0    0    0    0    0    0  \n",
       "21    0    0    0    0    0    0  \n",
       "22    0    0    0    0    0    0  \n",
       "23    0    0    0    0    0    0  \n",
       "24    0    0    0    0    0    0  \n",
       "25    0    0    0    0    0    0  \n",
       "26    0    0    0    0    0    0  \n",
       "27    0    0    0    0    0    0  \n",
       "28    0    0    0    0    0    0  \n",
       "29    1    0    0    0    0    0  \n",
       "30    0    1    1    1    1    1  \n",
       "31    0    0    1    0    1    0  \n",
       "32    0    1    0    0    0    0  \n",
       "33    0    0    0    1    0    0  \n",
       "34    0    0    0    0    0    0  \n",
       "35    0    0    0    0    0    0  \n",
       "36    0    0    0    0    0    0  \n",
       "37    0    0    0    0    0    0  \n",
       "38    0    0    0    0    0    0  \n",
       "39    0    0    0    0    0    0  \n",
       "40    0    0    0    0    0    0  \n",
       "41    0    0    0    0    0    0  \n",
       "42    0    0    0    0    0    0  \n",
       "43    0    0    0    0    0    1  \n",
       "44    0    0    0    0    0    0  \n",
       "45    0    0    0    0    0    0  \n",
       "46    0    0    0    0    0    0  \n",
       "47    0    0    0    0    0    0  \n",
       "48    0    0    0    0    0    0  \n",
       "49    0    0    0    0    0    0  \n",
       "50    0    0    0    0    0    0  \n",
       "51    0    0    0    0    0    0  \n",
       "52    0    0    0    0    0    0  \n",
       "53    0    0    0    0    0    0  \n",
       "54    0    0    0    0    0    0  \n",
       "55    1    0    0    0    0    0  \n",
       "56    0    0    0    0    0    0  \n",
       "57    0    0    0    0    0    0  \n",
       "\n",
       "[58 rows x 200 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 natural language processing nlp is a subfield of linguistics computer science information engineering and artificial intelligence concerned with the interactions between computers and human natural languages in particular how to program computers to process and analyze large amounts of natural language data \n"
     ]
    }
   ],
   "source": [
    "print(len(corpus), corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Code given below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idf_values = {}\n",
    "for token in most_freq:\n",
    "    doc_containing_word = 0\n",
    "    for document in corpus:\n",
    "        if token in nltk.word_tokenize(document):\n",
    "            doc_containing_word += 1\n",
    "    word_idf_values[token] = np.log(len(corpus)/(1 + doc_containing_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0.27625337662815813,\n",
       " 'of': 0.371563556432483,\n",
       " 'a': 0.626455806061273,\n",
       " 'to': 0.659245628884264,\n",
       " 'in': 0.7282385003712155,\n",
       " 'and': 0.8415671856782185,\n",
       " 'language': 1.0647107369924282,\n",
       " 'is': 1.2272296664902032,\n",
       " 'natural': 1.3523928094442093,\n",
       " 'as': 1.1700712526502546,\n",
       " 'processing': 1.2272296664902032,\n",
       " 'that': 1.2878542883066382,\n",
       " 'machine': 1.421385680931161,\n",
       " 'for': 1.2878542883066382,\n",
       " 'systems': 1.575536360758419,\n",
       " 'learning': 1.421385680931161,\n",
       " 'such': 1.4954936530848826,\n",
       " 'on': 1.6625477377480486,\n",
       " 'algorithms': 1.575536360758419,\n",
       " 'data': 1.7578579175523736,\n",
       " 'which': 1.6625477377480486,\n",
       " 'translation': 1.9810014688665833,\n",
       " 'many': 1.7578579175523736,\n",
       " 'or': 1.7578579175523736,\n",
       " 'research': 1.7578579175523736,\n",
       " 'statistical': 1.7578579175523736,\n",
       " 'are': 1.8632184332102,\n",
       " 'rules': 2.114532861491106,\n",
       " 'nlp': 2.114532861491106,\n",
       " 'more': 1.9810014688665833,\n",
       " 'was': 1.9810014688665833,\n",
       " 'were': 1.8632184332102,\n",
       " 'based': 1.8632184332102,\n",
       " 'used': 1.9810014688665833,\n",
       " 'models': 2.114532861491106,\n",
       " 'has': 1.8632184332102,\n",
       " 'tasks': 1.9810014688665833,\n",
       " 'cognitive': 2.268683541318364,\n",
       " 'linguistics': 2.268683541318364,\n",
       " 'with': 1.9810014688665833,\n",
       " 'real': 1.9810014688665833,\n",
       " 'when': 2.114532861491106,\n",
       " 'some': 1.9810014688665833,\n",
       " 'by': 2.114532861491106,\n",
       " 'this': 2.114532861491106,\n",
       " 'input': 2.268683541318364,\n",
       " 'be': 2.114532861491106,\n",
       " 'from': 2.114532861491106,\n",
       " 'an': 2.114532861491106,\n",
       " 'into': 2.114532861491106,\n",
       " 'however': 2.114532861491106,\n",
       " 'results': 2.268683541318364,\n",
       " 'these': 2.268683541318364,\n",
       " 'word': 2.114532861491106,\n",
       " 'big': 2.451005098112319,\n",
       " 'information': 2.268683541318364,\n",
       " 'can': 2.268683541318364,\n",
       " 'world': 2.268683541318364,\n",
       " 'hand': 2.268683541318364,\n",
       " 'being': 2.451005098112319,\n",
       " 'intelligence': 2.6741486494265287,\n",
       " 'human': 2.6741486494265287,\n",
       " 'large': 2.451005098112319,\n",
       " 'speech': 2.451005098112319,\n",
       " 'than': 2.451005098112319,\n",
       " '1980s': 2.451005098112319,\n",
       " 'developed': 2.451005098112319,\n",
       " 'written': 2.451005098112319,\n",
       " 'using': 2.451005098112319,\n",
       " 'examples': 2.451005098112319,\n",
       " 'produced': 2.451005098112319,\n",
       " 'corpora': 2.451005098112319,\n",
       " 'other': 2.451005098112319,\n",
       " 'annotated': 2.9618307218783095,\n",
       " 'task': 2.451005098112319,\n",
       " 'have': 2.451005098112319,\n",
       " 'intent': 2.451005098112319,\n",
       " 'author': 2.6741486494265287,\n",
       " 'computer': 2.6741486494265287,\n",
       " 'science': 2.6741486494265287,\n",
       " 'understanding': 2.6741486494265287,\n",
       " 'generally': 2.6741486494265287,\n",
       " 'work': 2.6741486494265287,\n",
       " 'published': 2.9618307218783095,\n",
       " 'much': 2.6741486494265287,\n",
       " 'late': 2.6741486494265287,\n",
       " 'first': 2.6741486494265287,\n",
       " 'system': 2.6741486494265287,\n",
       " 'eliza': 2.6741486494265287,\n",
       " 'knowledge': 2.6741486494265287,\n",
       " 'example': 2.6741486494265287,\n",
       " 'conceptual': 2.6741486494265287,\n",
       " 'up': 2.6741486494265287,\n",
       " 'most': 2.6741486494265287,\n",
       " 'due': 2.6741486494265287,\n",
       " 'e': 2.9618307218783095,\n",
       " 'g': 2.9618307218783095,\n",
       " 'trees': 2.6741486494265287,\n",
       " 'if': 2.6741486494265287,\n",
       " 'then': 2.9618307218783095,\n",
       " 'part': 2.6741486494265287,\n",
       " 'increasingly': 2.6741486494265287,\n",
       " 'focused': 2.6741486494265287,\n",
       " 'make': 2.6741486494265287,\n",
       " 'probabilistic': 2.6741486494265287,\n",
       " 'given': 2.6741486494265287,\n",
       " 'larger': 2.6741486494265287,\n",
       " 'been': 2.6741486494265287,\n",
       " 'learn': 2.6741486494265287,\n",
       " 'deep': 2.6741486494265287,\n",
       " 'neural': 2.6741486494265287,\n",
       " 'sequence': 2.9618307218783095,\n",
       " 'set': 2.6741486494265287,\n",
       " 'inference': 2.6741486494265287,\n",
       " 'through': 2.6741486494265287,\n",
       " 'relative': 2.6741486494265287,\n",
       " 'one': 2.6741486494265287,\n",
       " 'idea': 2.9618307218783095,\n",
       " 'presented': 2.6741486494265287,\n",
       " 'meaning': 2.9618307218783095,\n",
       " 'text': 2.9618307218783095,\n",
       " 'analyzed': 2.9618307218783095,\n",
       " 'n': 3.367295829986474,\n",
       " 'tokens': 3.367295829986474,\n",
       " 'artificial': 2.9618307218783095,\n",
       " 'between': 2.9618307218783095,\n",
       " 'computers': 3.367295829986474,\n",
       " 'languages': 2.9618307218783095,\n",
       " 'how': 2.9618307218783095,\n",
       " 'process': 2.9618307218783095,\n",
       " 'amounts': 2.9618307218783095,\n",
       " 'frequently': 2.9618307218783095,\n",
       " 'recognition': 2.9618307218783095,\n",
       " 'found': 2.9618307218783095,\n",
       " 'turing': 3.367295829986474,\n",
       " 'now': 2.9618307218783095,\n",
       " 'called': 2.9618307218783095,\n",
       " 'sixty': 2.9618307218783095,\n",
       " 'english': 2.9618307218783095,\n",
       " 'authors': 2.9618307218783095,\n",
       " 'after': 2.9618307218783095,\n",
       " '1966': 2.9618307218783095,\n",
       " 'had': 2.9618307218783095,\n",
       " 'restricted': 3.367295829986474,\n",
       " 'about': 2.9618307218783095,\n",
       " 'thought': 2.9618307218783095,\n",
       " 'like': 2.9618307218783095,\n",
       " 'very': 2.9618307218783095,\n",
       " 'head': 3.367295829986474,\n",
       " 'hurts': 3.367295829986474,\n",
       " 'during': 2.9618307218783095,\n",
       " '1978': 3.367295829986474,\n",
       " 'lehnert': 3.367295829986474,\n",
       " 'time': 2.9618307218783095,\n",
       " 'including': 2.9618307218783095,\n",
       " 'there': 2.9618307218783095,\n",
       " 'revolution': 2.9618307218783095,\n",
       " 'both': 2.9618307218783095,\n",
       " 'increase': 2.9618307218783095,\n",
       " 's': 2.9618307218783095,\n",
       " 'grammar': 2.9618307218783095,\n",
       " 'corpus': 2.9618307218783095,\n",
       " 'earliest': 2.9618307218783095,\n",
       " 'decision': 2.9618307218783095,\n",
       " 'hard': 2.9618307218783095,\n",
       " 'similar': 2.9618307218783095,\n",
       " 'existing': 2.9618307218783095,\n",
       " 'tagging': 2.9618307218783095,\n",
       " 'use': 2.9618307218783095,\n",
       " 'soft': 2.9618307218783095,\n",
       " 'decisions': 2.9618307218783095,\n",
       " 'attaching': 2.9618307218783095,\n",
       " 'valued': 2.9618307218783095,\n",
       " 'weights': 2.9618307218783095,\n",
       " 'features': 2.9618307218783095,\n",
       " 'especially': 2.9618307218783095,\n",
       " 'contains': 2.9618307218783095,\n",
       " 'common': 2.9618307218783095,\n",
       " 'reliable': 2.9618307218783095,\n",
       " 'subtasks': 2.9618307218783095,\n",
       " 'early': 2.9618307218783095,\n",
       " 'where': 2.9618307218783095,\n",
       " 'take': 2.9618307218783095,\n",
       " 'advantage': 2.9618307218783095,\n",
       " 'result': 2.9618307218783095,\n",
       " 'all': 3.367295829986474,\n",
       " 'often': 2.9618307218783095,\n",
       " 'methods': 2.9618307218783095,\n",
       " 'supervised': 2.9618307218783095,\n",
       " 'not': 2.9618307218783095,\n",
       " 'answers': 2.9618307218783095,\n",
       " 'non': 2.9618307218783095,\n",
       " 'less': 2.9618307218783095,\n",
       " 'amount': 2.9618307218783095,\n",
       " 'algorithm': 2.9618307218783095,\n",
       " 'network': 2.9618307218783095,\n",
       " 'techniques': 2.9618307218783095,\n",
       " 'modeling': 2.9618307218783095,\n",
       " 'parsing': 2.9618307218783095,\n",
       " 'others': 2.9618307218783095}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tf_values = {}\n",
    "for token in most_freq:\n",
    "    sent_tf_vector = []\n",
    "    for document in corpus:\n",
    "        doc_freq = 0\n",
    "        for word in nltk.word_tokenize(document):\n",
    "            if token == word:\n",
    "                  doc_freq += 1\n",
    "        word_tf = doc_freq/len(nltk.word_tokenize(document))\n",
    "        sent_tf_vector.append(word_tf)\n",
    "    word_tf_values[token] = sent_tf_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>of</th>\n",
       "      <th>a</th>\n",
       "      <th>to</th>\n",
       "      <th>in</th>\n",
       "      <th>and</th>\n",
       "      <th>language</th>\n",
       "      <th>is</th>\n",
       "      <th>natural</th>\n",
       "      <th>as</th>\n",
       "      <th>...</th>\n",
       "      <th>answers</th>\n",
       "      <th>non</th>\n",
       "      <th>less</th>\n",
       "      <th>amount</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>network</th>\n",
       "      <th>techniques</th>\n",
       "      <th>modeling</th>\n",
       "      <th>parsing</th>\n",
       "      <th>others</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        the        of         a        to        in       and  language  \\\n",
       "0  0.023810  0.047619  0.023810  0.047619  0.023810  0.071429  0.047619   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.062500  0.062500  0.187500   \n",
       "2  0.100000  0.050000  0.000000  0.000000  0.050000  0.000000  0.050000   \n",
       "3  0.035714  0.035714  0.035714  0.000000  0.035714  0.035714  0.000000   \n",
       "4  0.058824  0.058824  0.000000  0.000000  0.058824  0.000000  0.000000   \n",
       "\n",
       "         is   natural        as  ...  answers  non  less  amount  algorithm  \\\n",
       "0  0.023810  0.071429  0.000000  ...      0.0  0.0   0.0     0.0        0.0   \n",
       "1  0.000000  0.187500  0.000000  ...      0.0  0.0   0.0     0.0        0.0   \n",
       "2  0.000000  0.050000  0.000000  ...      0.0  0.0   0.0     0.0        0.0   \n",
       "3  0.035714  0.000000  0.035714  ...      0.0  0.0   0.0     0.0        0.0   \n",
       "4  0.000000  0.000000  0.000000  ...      0.0  0.0   0.0     0.0        0.0   \n",
       "\n",
       "   network  techniques  modeling  parsing  others  \n",
       "0      0.0         0.0       0.0      0.0     0.0  \n",
       "1      0.0         0.0       0.0      0.0     0.0  \n",
       "2      0.0         0.0       0.0      0.0     0.0  \n",
       "3      0.0         0.0       0.0      0.0     0.0  \n",
       "4      0.0         0.0       0.0      0.0     0.0  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tf = pd.DataFrame(word_tf_values)\n",
    "df_tf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_values = {}\n",
    "for token in word_tf_values.keys():\n",
    "    tfidf_sentences = []\n",
    "    for tf_sentence in word_tf_values[token]:\n",
    "        tf_idf_score = tf_sentence * word_idf_values[token]\n",
    "        tfidf_sentences.append(tf_idf_score)\n",
    "    tfidf_values[token] = tfidf_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>of</th>\n",
       "      <th>a</th>\n",
       "      <th>to</th>\n",
       "      <th>in</th>\n",
       "      <th>and</th>\n",
       "      <th>language</th>\n",
       "      <th>is</th>\n",
       "      <th>natural</th>\n",
       "      <th>as</th>\n",
       "      <th>...</th>\n",
       "      <th>answers</th>\n",
       "      <th>non</th>\n",
       "      <th>less</th>\n",
       "      <th>amount</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>network</th>\n",
       "      <th>techniques</th>\n",
       "      <th>modeling</th>\n",
       "      <th>parsing</th>\n",
       "      <th>others</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.006577</td>\n",
       "      <td>0.017694</td>\n",
       "      <td>0.014916</td>\n",
       "      <td>0.031393</td>\n",
       "      <td>0.017339</td>\n",
       "      <td>0.060112</td>\n",
       "      <td>0.050701</td>\n",
       "      <td>0.02922</td>\n",
       "      <td>0.096599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045515</td>\n",
       "      <td>0.052598</td>\n",
       "      <td>0.199633</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.253574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.027625</td>\n",
       "      <td>0.018578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053236</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.067620</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009866</td>\n",
       "      <td>0.013270</td>\n",
       "      <td>0.022373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026009</td>\n",
       "      <td>0.030056</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.04383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016250</td>\n",
       "      <td>0.021857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        the        of         a        to        in       and  language  \\\n",
       "0  0.006577  0.017694  0.014916  0.031393  0.017339  0.060112  0.050701   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.045515  0.052598  0.199633   \n",
       "2  0.027625  0.018578  0.000000  0.000000  0.036412  0.000000  0.053236   \n",
       "3  0.009866  0.013270  0.022373  0.000000  0.026009  0.030056  0.000000   \n",
       "4  0.016250  0.021857  0.000000  0.000000  0.042838  0.000000  0.000000   \n",
       "\n",
       "        is   natural        as  ...  answers  non  less  amount  algorithm  \\\n",
       "0  0.02922  0.096599  0.000000  ...      0.0  0.0   0.0     0.0        0.0   \n",
       "1  0.00000  0.253574  0.000000  ...      0.0  0.0   0.0     0.0        0.0   \n",
       "2  0.00000  0.067620  0.000000  ...      0.0  0.0   0.0     0.0        0.0   \n",
       "3  0.04383  0.000000  0.041788  ...      0.0  0.0   0.0     0.0        0.0   \n",
       "4  0.00000  0.000000  0.000000  ...      0.0  0.0   0.0     0.0        0.0   \n",
       "\n",
       "   network  techniques  modeling  parsing  others  \n",
       "0      0.0         0.0       0.0      0.0     0.0  \n",
       "1      0.0         0.0       0.0      0.0     0.0  \n",
       "2      0.0         0.0       0.0      0.0     0.0  \n",
       "3      0.0         0.0       0.0      0.0     0.0  \n",
       "4      0.0         0.0       0.0      0.0     0.0  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf = pd.DataFrame(tfidf_values)\n",
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00657746, 0.0176935 , 0.01491561, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.02762534, 0.01857818, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.00986619, 0.        , 0.06712026, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.01563698, 0.03505317, 0.01181992, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.03032049, 0.03171884, 0.01527941, ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.asarray(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '11', '12', '19', '1950', '1950s', '1954', '1960s', '1964', '1966', '1970s', '1975', '1976', '1977', '1978', '1979', '1980s', '1981', '1990s', '20', '2010s', '2018', '21', '22', '23', '9269353', 'able', 'about', 'accurate', 'achieve', 'acquiring', 'action', 'additional', 'advantage', 'advantages', 'after', 'aid', 'alan', 'algorithm', 'algorithms', 'alignment', 'alike', 'all', 'almost', 'along', 'alpac', 'although', 'ambiguous', 'among', 'amount', 'amounts', 'an', 'analysis', 'analyze', 'analyzed', 'and', 'annotated', 'annotations', 'another', 'answering', 'answers', 'any', 'application', 'applications', 'applied', 'approach', 'approaches', 'are', 'areas', 'art', 'article', 'artificial', 'as', 'aspect', 'assign', 'at', 'attaching', 'author', 'authors', 'automatic', 'automatically', 'available', 'base', 'based', 'be', 'became', 'been', 'before', 'began', 'behind', 'being', 'below', 'between', 'big', 'block', 'blocks', 'both', 'branch', 'build', 'but', 'by', 'cache', 'called', 'calling', 'calls', 'can', 'canada', 'capture', 'carbonell', 'categories', 'certainty', 'challenges', 'changes', 'chatterbots', 'chomskyan', 'claimed', 'clarification', 'classes', 'closely', 'coarse', 'coding', 'cognition', 'cognitive', 'combination', 'combining', 'common', 'commonly', 'comparative', 'comparison', 'complete', 'complex', 'complexity', 'complicated', 'component', 'comprising', 'computational', 'computer', 'computers', 'computing', 'conceptual', 'concerned', 'conducted', 'consider', 'contains', 'content', 'context', 'continues', 'convenience', 'corpora', 'corpus', 'corresponding', 'criterion', 'cullingford', 'data', 'day', 'days', 'deal', 'decision', 'decisions', 'deep', 'defining', 'depended', 'dependency', 'designed', 'desired', 'developed', 'devising', 'different', 'difficult', 'direct', 'directly', 'discouraged', 'distinct', 'division', 'do', 'documents', 'dominance', 'dramatically', 'due', 'during', 'each', 'earlier', 'earliest', 'early', 'effectively', 'eliza', 'embeddings', 'emotion', 'emphasizes', 'enables', 'end', 'engineering', 'english', 'enormous', 'enough', 'entailed', 'entire', 'equation', 'errors', 'especially', 'european', 'example', 'examples', 'exceeded', 'existing', 'expectations', 'experience', 'experiment', 'explained', 'express', 'fact', 'failed', 'feature', 'features', 'field', 'findings', 'first', 'five', 'flurry', 'focused', 'following', 'for', 'form', 'found', 'free', 'frequently', 'from', 'fulfill', 'fully', 'function', 'funding', 'further', 'generally', 'generated', 'generation', 'generic', 'george', 'georgetown', 'given', 'gone', 'government', 'governmental', 'gradual', 'grammar', 'grammars', 'great', 'had', 'hand', 'handwritten', 'hard', 'has', 'have', 'head', 'heavily', 'heuristic', 'hidden', 'higher', 'history', 'how', 'however', 'human', 'hurts', 'ibm', 'idea', 'if', 'implemented', 'implication', 'imply', 'importance', 'in', 'include', 'included', 'including', 'increase', 'increasingly', 'indicators', 'inference', 'inferior', 'information', 'input', 'instance', 'instead', 'integrated', 'intelligence', 'intent', 'interaction', 'interactions', 'interdisciplinary', 'intermediate', 'intertwined', 'into', 'introduced', 'introduction', 'involve', 'involved', 'is', 'it', 'its', 'jabberwacky', 'joseph', 'knowledge', 'lakoff', 'language', 'languages', 'large', 'larger', 'late', 'law', 'laws', 'leads', 'learn', 'learning', 'lehnert', 'less', 'lessening', 'level', 'like', 'likely', 'limitation', 'limited', 'linguistics', 'list', 'little', 'location', 'long', 'low', 'machine', 'machinery', 'major', 'make', 'making', 'many', 'margie', 'marketed', 'markov', 'mathematical', 'may', 'meaning', 'meanings', 'measure', 'measures', 'meehan', 'mental', 'merely', 'metaphor', 'methodology', 'methods', 'mid', 'might', 'million', 'mind', 'model', 'modeling', 'models', 'moore', 'more', 'most', 'much', 'multilingual', 'multiple', 'my', 'namely', 'natural', 'need', 'needed', 'network', 'neural', 'new', 'nlp', 'nmt', 'no', 'non', 'not', 'notable', 'notably', 'novel', 'now', 'number', 'obviating', 'occurred', 'of', 'offers', 'official', 'often', 'on', 'one', 'only', 'ontologies', 'or', 'other', 'others', 'over', 'pam', 'paradigm', 'parliament', 'parry', 'parsing', 'part', 'particular', 'patent', 'patient', 'pcfg', 'periods', 'person', 'perspective', 'pf', 'phrase', 'physically', 'piece', 'pipeline', 'plot', 'plural', 'pmm', 'politics', 'popular', 'possible', 'possibly', 'power', 'practical', 'presented', 'probabilistic', 'probability', 'probable', 'problem', 'proceedings', 'process', 'processes', 'processing', 'produce', 'produced', 'produces', 'producing', 'program', 'programmers', 'progress', 'properties', 'proposed', 'provide', 'provided', 'provides', 'psychology', 'psychotherapist', 'published', 'qualm', 'question', 'racter', 'rather', 'real', 'recent', 'recognition', 'reduced', 'refers', 'relative', 'reliable', 'relied', 'rely', 'relying', 'remain', 'report', 'representation', 'research', 'researched', 'responding', 'response', 'restricted', 'result', 'results', 'revolution', 'rmm', 'road', 'robust', 'rogerian', 'rules', 'russian', 'sam', 'say', 'schank', 'science', 'scientific', 'second', 'see', 'semantic', 'semi', 'senses', 'sentence', 'sentences', 'separate', 'sequence', 'serve', 'set', 'sets', 'she', 'shift', 'showing', 'shrdlu', 'similar', 'simulation', 'since', 'sixty', 'slower', 'small', 'smt', 'so', 'soft', 'solved', 'solving', 'some', 'sometimes', 'somewhat', 'sort', 'specific', 'specifically', 'speech', 'started', 'starting', 'startlingly', 'state', 'statement', 'statistical', 'stative', 'steady', 'stemming', 'steps', 'structured', 'study', 'style', 'subdivided', 'subfield', 'substantial', 'subtasks', 'success', 'successes', 'successful', 'successively', 'such', 'supervised', 'system', 'systems', 'tagging', 'take', 'talespin', 'task', 'tasks', 'techniques', 'ten', 'term', 'terms', 'test', 'text', 'textual', 'than', 'that', 'the', 'then', 'theoretical', 'theories', 'theory', 'there', 'these', 'they', 'things', 'this', 'though', 'thought', 'three', 'through', 'time', 'titled', 'to', 'token', 'tokens', 'tomorrow', 'transformational', 'transformations', 'translation', 'tree', 'trees', 'turing', 'typical', 'typically', 'underlies', 'underpinnings', 'understandable', 'understanding', 'unfamiliar', 'union', 'units', 'unsupervised', 'until', 'up', 'upon', 'us', 'usages', 'use', 'used', 'using', 'valued', 'verb', 'very', 'viewed', 'vocabularies', 'was', 'web', 'weights', 'weizenbaum', 'were', 'what', 'when', 'where', 'which', 'while', 'whose', 'why', 'wide', 'widespread', 'wilensky', 'will', 'with', 'within', 'without', 'word', 'words', 'work', 'working', 'world', 'worlds', 'would', 'write', 'writing', 'written', 'year', 'years', 'you', 'your']\n",
      "(58, 634)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer(*, input='content', encoding='utf-8', \n",
    "#                 decode_error='strict', strip_accents=None, lowercase=True, \n",
    "#                 preprocessor=None, tokenizer=None, analyzer='word', stop_words=None, \n",
    "#                 token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), max_df=1.0, min_df=1, \n",
    "#                 max_features=None, vocabulary=None, binary=False, \n",
    "#                 dtype=<class 'numpy.float64'>, norm='l2', use_idf=True, smooth_idf=True, \n",
    "#                 sublinear_tf=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

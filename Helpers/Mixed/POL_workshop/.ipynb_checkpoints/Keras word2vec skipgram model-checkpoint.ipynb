{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(13)\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input\n",
    "from keras.layers.merge import Dot\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing import text\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = get_file('alice.txt', origin='http://www.gutenberg.org/files/11/11-0.txt')\n",
    "corpus = open(path).readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This eBook is for the use of anyone anywhere at no cost and with\\n',\n",
       " 'almost no restrictions whatsoever.  You may copy it, give it away or\\n']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This eBook is for the use of anyone anywhere at no cost and with\\n',\n",
       " 'almost no restrictions whatsoever.  You may copy it, give it away or\\n']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [sentence for sentence in corpus if sentence.count(' ') >= 2]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "V = len(tokenizer.word_index) + 1\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_embedddings = 128\n",
    "\n",
    "# inputs\n",
    "w_inputs = Input(shape=(1, ), dtype='int32')\n",
    "w = Embedding(V, dim_embedddings)(w_inputs)\n",
    "\n",
    "# context\n",
    "c_inputs = Input(shape=(1, ), dtype='int32')\n",
    "c  = Embedding(V, dim_embedddings)(c_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_7_1/Identity:0' shape=(None, 1, 128) dtype=float32>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_6_1/Identity:0' shape=(None, 1, 128) dtype=float32>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dot_5/Identity:0' shape=(None, 1, 1) dtype=float32>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = Dot(axes=2)([w, c])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'reshape_11/Identity:0' shape=(None, 1) dtype=float32>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = Reshape((1,), input_shape=(1, 1))(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 1, 128)       3200        input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 1, 128)       3200        input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_5 (Dot)                     (None, 1, 1)         0           embedding_6[0][0]                \n",
      "                                                                 embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_11 (Reshape)            (None, 1)            0           dot_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 1)            0           reshape_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 6,400\n",
      "Trainable params: 6,400\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "output = Activation('sigmoid')(output)\n",
    "\n",
    "SkipGram = Model(inputs=[w_inputs, c_inputs], outputs=output)\n",
    "SkipGram.summary()\n",
    "SkipGram.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'no': 1,\n",
       " 'it': 2,\n",
       " 'this': 3,\n",
       " 'ebook': 4,\n",
       " 'is': 5,\n",
       " 'for': 6,\n",
       " 'the': 7,\n",
       " 'use': 8,\n",
       " 'of': 9,\n",
       " 'anyone': 10,\n",
       " 'anywhere': 11,\n",
       " 'at': 12,\n",
       " 'cost': 13,\n",
       " 'and': 14,\n",
       " 'with': 15,\n",
       " 'almost': 16,\n",
       " 'restrictions': 17,\n",
       " 'whatsoever': 18,\n",
       " 'you': 19,\n",
       " 'may': 20,\n",
       " 'copy': 21,\n",
       " 'give': 22,\n",
       " 'away': 23,\n",
       " 'or': 24}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This eBook is for the use of anyone anywhere at no cost and with\\n',\n",
       " 'almost no restrictions whatsoever.  You may copy it, give it away or\\n']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 13, 14, 15]\n",
      "1 [16, 1, 17, 18, 19, 20, 21, 2, 22, 2, 23, 24]\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(tokenizer.texts_to_sequences(corpus)):\n",
    "    print(i, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  [[7, 7], [8, 9], [8, 13], [6, 9], [5, 6], [7, 5], [15, 14], [11, 21], [9, 22], [5, 4], [1, 14], [5, 3], [13, 1], [8, 9], [15, 14], [13, 15], [4, 6], [14, 13], [4, 6], [7, 14], [11, 12], [1, 12], [5, 18], [14, 12], [1, 6], [7, 20], [10, 20], [12, 16], [5, 12], [14, 12], [9, 20], [3, 22], [15, 21], [11, 10], [13, 22], [10, 12], [12, 23], [4, 7], [8, 22], [11, 1], [13, 21], [1, 12], [13, 14], [10, 11], [6, 7], [1, 7], [9, 14], [8, 1], [11, 12], [12, 21], [5, 23], [5, 12], [11, 10], [13, 14], [1, 2], [8, 7], [14, 9], [9, 11], [15, 13], [14, 15], [1, 13], [13, 6], [10, 17], [15, 12], [6, 5], [6, 8], [4, 3], [6, 5], [6, 20], [6, 4], [13, 5], [7, 9], [1, 21], [11, 8], [8, 10], [11, 11], [14, 23], [14, 14], [12, 10], [11, 8], [7, 22], [8, 13], [14, 13], [12, 17], [13, 8], [3, 5], [9, 10], [6, 13], [8, 6], [9, 8], [13, 15], [9, 7], [1, 11], [7, 5], [1, 10], [13, 12], [12, 9], [3, 14], [10, 22], [9, 12], [6, 12], [9, 14], [5, 6], [4, 8], [1, 13], [8, 10], [10, 21], [12, 5], [9, 3], [12, 1], [3, 5], [5, 7], [1, 12], [6, 8], [4, 9], [6, 21], [7, 9], [8, 20], [12, 14], [3, 4], [8, 9], [6, 16], [7, 5], [15, 22], [5, 20], [7, 8], [12, 2], [12, 13], [12, 11], [10, 11], [10, 8], [4, 2], [13, 18], [7, 6], [11, 24], [9, 21], [10, 9], [10, 15], [3, 23], [10, 11], [4, 5], [11, 9], [10, 15], [5, 20], [4, 13], [9, 8], [14, 1], [7, 3], [5, 4], [11, 19]] \n",
      "X:  [array([ 7,  8,  8,  6,  5,  7, 15, 11,  9,  5,  1,  5, 13,  8, 15, 13,  4,\n",
      "       14,  4,  7, 11,  1,  5, 14,  1,  7, 10, 12,  5, 14,  9,  3, 15, 11,\n",
      "       13, 10, 12,  4,  8, 11, 13,  1, 13, 10,  6,  1,  9,  8, 11, 12,  5,\n",
      "        5, 11, 13,  1,  8, 14,  9, 15, 14,  1, 13, 10, 15,  6,  6,  4,  6,\n",
      "        6,  6, 13,  7,  1, 11,  8, 11, 14, 14, 12, 11,  7,  8, 14, 12, 13,\n",
      "        3,  9,  6,  8,  9, 13,  9,  1,  7,  1, 13, 12,  3, 10,  9,  6,  9,\n",
      "        5,  4,  1,  8, 10, 12,  9, 12,  3,  5,  1,  6,  4,  6,  7,  8, 12,\n",
      "        3,  8,  6,  7, 15,  5,  7, 12, 12, 12, 10, 10,  4, 13,  7, 11,  9,\n",
      "       10, 10,  3, 10,  4, 11, 10,  5,  4,  9, 14,  7,  5, 11]), array([ 7,  9, 13,  9,  6,  5, 14, 21, 22,  4, 14,  3,  1,  9, 14, 15,  6,\n",
      "       13,  6, 14, 12, 12, 18, 12,  6, 20, 20, 16, 12, 12, 20, 22, 21, 10,\n",
      "       22, 12, 23,  7, 22,  1, 21, 12, 14, 11,  7,  7, 14,  1, 12, 21, 23,\n",
      "       12, 10, 14,  2,  7,  9, 11, 13, 15, 13,  6, 17, 12,  5,  8,  3,  5,\n",
      "       20,  4,  5,  9, 21,  8, 10, 11, 23, 14, 10,  8, 22, 13, 13, 17,  8,\n",
      "        5, 10, 13,  6,  8, 15,  7, 11,  5, 10, 12,  9, 14, 22, 12, 12, 14,\n",
      "        6,  8, 13, 10, 21,  5,  3,  1,  5,  7, 12,  8,  9, 21,  9, 20, 14,\n",
      "        4,  9, 16,  5, 22, 20,  8,  2, 13, 11, 11,  8,  2, 18,  6, 24, 21,\n",
      "        9, 15, 23, 11,  5,  9, 15, 20, 13,  8,  1,  3,  4, 19])] \n",
      "Y [0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1\n",
      " 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 1 0\n",
      " 0 0]\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(tokenizer.texts_to_sequences(corpus)):\n",
    "    data, labels = skipgrams(sequence=doc, vocabulary_size=V, window_size=2, negative_samples=2.)\n",
    "    x = [np.array(x) for x in zip(*data)]\n",
    "    y = np.array(labels, dtype=np.int32)\n",
    "    print('data: ', data,'\\nX: ' , x, '\\nY' ,y)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'no',\n",
       " 2: 'it',\n",
       " 3: 'this',\n",
       " 4: 'ebook',\n",
       " 5: 'is',\n",
       " 6: 'for',\n",
       " 7: 'the',\n",
       " 8: 'use',\n",
       " 9: 'of',\n",
       " 10: 'anyone',\n",
       " 11: 'anywhere',\n",
       " 12: 'at',\n",
       " 13: 'cost',\n",
       " 14: 'and',\n",
       " 15: 'with',\n",
       " 16: 'almost',\n",
       " 17: 'restrictions',\n",
       " 18: 'whatsoever',\n",
       " 19: 'you',\n",
       " 20: 'may',\n",
       " 21: 'copy',\n",
       " 22: 'give',\n",
       " 23: 'away',\n",
       " 24: 'or'}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word = {v:k for k, v in tokenizer.word_index.items()}\n",
    "id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(of (9), the (7)) -> 1\n",
      "(at (12), anywhere (11)) -> 1\n"
     ]
    }
   ],
   "source": [
    "# generate skip-grams\n",
    "word2id = tokenizer.word_index\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in corpus]\n",
    "skip_grams = [skipgrams(wid, vocabulary_size=vocab_size, window_size=2) for wid in wids]\n",
    "\n",
    "# view sample skip-grams\n",
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(2):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
    "          id2word[pairs[i][0]], pairs[i][0], \n",
    "          id2word[pairs[i][1]], pairs[i][1], \n",
    "          labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3862591981887817\n",
      "1.385212004184723\n",
      "1.3839866518974304\n",
      "1.383117437362671\n",
      "1.3821192979812622\n",
      "1.3805545568466187\n",
      "1.379438877105713\n",
      "1.3780673146247864\n",
      "1.376146912574768\n",
      "1.373725414276123\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    loss = 0.\n",
    "    for i, doc in enumerate(tokenizer.texts_to_sequences(corpus)):\n",
    "        data, labels = skipgrams(sequence=doc, vocabulary_size=V, window_size=5, negative_samples=5.)\n",
    "        x = [np.array(x) for x in zip(*data)]\n",
    "        y = np.array(labels, dtype=np.int32)\n",
    "        if x:\n",
    "            loss += SkipGram.train_on_batch(x, y)\n",
    "\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(V-1, dim_embedddings))\n",
    "vectors = SkipGram.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01723747, -0.0421329 , -0.01856668, ...,  0.04394963,\n",
       "        -0.00677763, -0.04928473],\n",
       "       [ 0.00532074,  0.02658218,  0.03579596, ..., -0.0129492 ,\n",
       "        -0.04443016,  0.03593078],\n",
       "       [ 0.00578544, -0.02669989, -0.01933427, ...,  0.06094663,\n",
       "         0.02118776,  0.01218737],\n",
       "       ...,\n",
       "       [ 0.04260482, -0.03379558,  0.00244163, ..., -0.00258035,\n",
       "         0.01659034, -0.01577614],\n",
       "       [ 0.02773357, -0.00603765, -0.00810921, ..., -0.00518424,\n",
       "         0.02827771, -0.04466083],\n",
       "       [-0.02400018,  0.04943145,  0.05313882, ...,  0.02259539,\n",
       "         0.05670473,  0.04488812]], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 0.3456296920776367),\n",
       " ('at', 0.2521226108074188),\n",
       " ('with', 0.23469021916389465),\n",
       " ('of', 0.22455015778541565),\n",
       " ('away', 0.19185395538806915),\n",
       " ('anywhere', 0.18722069263458252),\n",
       " ('you', 0.15692223608493805),\n",
       " ('or', 0.15685653686523438),\n",
       " ('no', 0.1402404010295868),\n",
       " ('and', 0.13369552791118622)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)\n",
    "w2v.most_similar(positive=['use'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

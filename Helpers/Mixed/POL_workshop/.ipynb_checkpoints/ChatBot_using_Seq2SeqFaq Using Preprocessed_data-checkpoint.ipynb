{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UgZHR8TO0lFF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras import layers , activations , models , preprocessing, utils\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "from string import digits\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sxiGOLldKOQD"
   },
   "source": [
    "## 2) Loading Preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data_file = Path(\"/home/iftekhar/AI-system/Mixed/POL_workshop/processed_texts.txt\")\n",
    "with open(data_file, encoding='utf-8') as f:\n",
    "    data_list = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \" \".join(data_list)\n",
    "# lines = re.sub(r\"\\s+\", \" \", lines)\n",
    "\n",
    "# sentences = sent_tokenize(lines)\n",
    "# sent = np.asarray(sentences)\n",
    "#sentTwo = sent.reshape(len(sent)//2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_text_split(page_text, word_limit):\n",
    "    page_text = page_text.split()\n",
    "    chunks = [' '.join(page_text[i:i + word_limit]) for i in range(0,\n",
    "                                                                   len(page_text), word_limit)]\n",
    "    return chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = page_text_split(all_text, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mobicontrol v manual mobicontrol v manual カスタマイザー タイトル 設定 カスタマイザー',\n",
       " 'サブタイトル 設定 mobicontrol 設定 順序 mobicontrol 設定 順序 mobicontrol 設定']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = []\n",
    "for line in lines:\n",
    "    collector.append(line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['mobicontrol',\n",
       "  'v',\n",
       "  'manual',\n",
       "  'mobicontrol',\n",
       "  'v',\n",
       "  'manual',\n",
       "  'カスタマイザー',\n",
       "  'タイトル',\n",
       "  '設定',\n",
       "  'カスタマイザー'],\n",
       " ['サブタイトル',\n",
       "  '設定',\n",
       "  'mobicontrol',\n",
       "  '設定',\n",
       "  '順序',\n",
       "  'mobicontrol',\n",
       "  '設定',\n",
       "  '順序',\n",
       "  'mobicontrol',\n",
       "  '設定']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collector[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-997d229b84cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "sent = np.asarray(lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sent)%2==0:\n",
    "    sentTwo = sent.reshape(len(sent)//2,2)\n",
    "else:\n",
    "    sent = sent[:-1]\n",
    "    sentTwo = sent.reshape(len(sent)//2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9958"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentTwo[:2]\n",
    "len(sentTwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame, concat\n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-2)    var1(t-1)      var1(t)    var1(t+1)\n",
      "2  mobicontrol            v       manual  mobicontrol\n",
      "3            v       manual  mobicontrol            v\n",
      "4       manual  mobicontrol            v       manual\n",
      "5  mobicontrol            v       manual      カスタマイザー\n",
      "6            v       manual      カスタマイザー         タイトル\n",
      "7       manual      カスタマイザー         タイトル           設定\n",
      "8      カスタマイザー         タイトル           設定      カスタマイザー\n"
     ]
    }
   ],
   "source": [
    "for items in collector:\n",
    "    var = (series_to_supervised(items, 2, 2))\n",
    "    print(var)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1, col2 = create_dataset(sentTwo)\n",
    "#col1 = col1.reshape(-1)\n",
    "col2 = col2.reshape(-1,1)\n",
    "#col2.shape\n",
    "dt = np.concatenate((col1,col2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "lines = pd.DataFrame(dt, columns=['Query', 'Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mobicontrol v manual mobicontrol v manual カスタマ...</td>\n",
       "      <td>順序 端末 os 対象 オンプレミス mobicontrol サーバ 必要 設定 事項</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>順序 端末 os 対象 オンプレミス mobicontrol サーバ 必要 設定 事項</td>\n",
       "      <td>directory domain service サーバ 間 ad f・s active d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>directory domain service サーバ 間 ad f・s active d...</td>\n",
       "      <td>会社 支給 端末 ios android 複数 従業員 共用 必須 apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>会社 支給 端末 ios android 複数 従業員 共用 必須 apple</td>\n",
       "      <td>apple 製品 設定 app ストア アプリ 購入 managed google play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple 製品 設定 app ストア アプリ 購入 managed google play</td>\n",
       "      <td>設定 アカウント 不要 wns windows pc リモート 管理 企業 団体</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Query  \\\n",
       "0  mobicontrol v manual mobicontrol v manual カスタマ...   \n",
       "1        順序 端末 os 対象 オンプレミス mobicontrol サーバ 必要 設定 事項   \n",
       "2  directory domain service サーバ 間 ad f・s active d...   \n",
       "3            会社 支給 端末 ios android 複数 従業員 共用 必須 apple   \n",
       "4     apple 製品 設定 app ストア アプリ 購入 managed google play   \n",
       "\n",
       "                                          Prediction  \n",
       "0        順序 端末 os 対象 オンプレミス mobicontrol サーバ 必要 設定 事項  \n",
       "1  directory domain service サーバ 間 ad f・s active d...  \n",
       "2            会社 支給 端末 ios android 複数 従業員 共用 必須 apple  \n",
       "3     apple 製品 設定 app ストア アプリ 購入 managed google play  \n",
       "4           設定 アカウント 不要 wns windows pc リモート 管理 企業 団体  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mobicontrol v manual mobicontrol v manual カスタマ...</td>\n",
       "      <td>START_ 順序 端末 os 対象 オンプレミス mobicontrol サーバ 必要 設...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>順序 端末 os 対象 オンプレミス mobicontrol サーバ 必要 設定 事項</td>\n",
       "      <td>START_ directory domain service サーバ 間 ad f・s a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>directory domain service サーバ 間 ad f・s active d...</td>\n",
       "      <td>START_ 会社 支給 端末 ios android 複数 従業員 共用 必須 apple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>会社 支給 端末 ios android 複数 従業員 共用 必須 apple</td>\n",
       "      <td>START_ apple 製品 設定 app ストア アプリ 購入 managed goog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple 製品 設定 app ストア アプリ 購入 managed google play</td>\n",
       "      <td>START_ 設定 アカウント 不要 wns windows pc リモート 管理 企業 団...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>設定 アカウント 不要 wns windows pc リモート 管理 企業 団体</td>\n",
       "      <td>START_ ルール 作成 端末 所定 端末 グループ 登録 の 端末 登録 _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ルール 作成 端末 所定 端末 グループ 登録 の 端末 登録</td>\n",
       "      <td>START_ グループ 複数 端末 登録 ルール 設定 可能 端末 os 対象 _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>グループ 複数 端末 登録 ルール 設定 可能 端末 os 対象</td>\n",
       "      <td>START_ 結構 構成 プロファイル 作成 情報漏洩 対策 wifi vpn メール 設定...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>結構 構成 プロファイル 作成 情報漏洩 対策 wifi vpn メール 設定</td>\n",
       "      <td>START_ 端末 配付 インストール apple 製品 端末 os 対象 ドキュメント 画...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>端末 配付 インストール apple 製品 端末 os 対象 ドキュメント 画像</td>\n",
       "      <td>START_ 挙動 設定 完了 端末 登録 可能 端末 登録 項 追加 _END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Query  \\\n",
       "0  mobicontrol v manual mobicontrol v manual カスタマ...   \n",
       "1        順序 端末 os 対象 オンプレミス mobicontrol サーバ 必要 設定 事項   \n",
       "2  directory domain service サーバ 間 ad f・s active d...   \n",
       "3            会社 支給 端末 ios android 複数 従業員 共用 必須 apple   \n",
       "4     apple 製品 設定 app ストア アプリ 購入 managed google play   \n",
       "5           設定 アカウント 不要 wns windows pc リモート 管理 企業 団体   \n",
       "6                    ルール 作成 端末 所定 端末 グループ 登録 の 端末 登録   \n",
       "7                   グループ 複数 端末 登録 ルール 設定 可能 端末 os 対象   \n",
       "8            結構 構成 プロファイル 作成 情報漏洩 対策 wifi vpn メール 設定   \n",
       "9           端末 配付 インストール apple 製品 端末 os 対象 ドキュメント 画像   \n",
       "\n",
       "                                          Prediction  \n",
       "0  START_ 順序 端末 os 対象 オンプレミス mobicontrol サーバ 必要 設...  \n",
       "1  START_ directory domain service サーバ 間 ad f・s a...  \n",
       "2  START_ 会社 支給 端末 ios android 複数 従業員 共用 必須 apple...  \n",
       "3  START_ apple 製品 設定 app ストア アプリ 購入 managed goog...  \n",
       "4  START_ 設定 アカウント 不要 wns windows pc リモート 管理 企業 団...  \n",
       "5        START_ ルール 作成 端末 所定 端末 グループ 登録 の 端末 登録 _END  \n",
       "6       START_ グループ 複数 端末 登録 ルール 設定 可能 端末 os 対象 _END  \n",
       "7  START_ 結構 構成 プロファイル 作成 情報漏洩 対策 wifi vpn メール 設定...  \n",
       "8  START_ 端末 配付 インストール apple 製品 端末 os 対象 ドキュメント 画...  \n",
       "9           START_ 挙動 設定 完了 端末 登録 可能 端末 登録 項 追加 _END  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add start and end tokens to target sequences\n",
    "lines.Prediction = lines.Prediction.apply(lambda x : 'START_ '+ x + ' _END')\n",
    "lines.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE : 6229\n"
     ]
    }
   ],
   "source": [
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts( lines.Query + lines.Prediction )\n",
    "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
    "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nF1mDKD_R6Os"
   },
   "source": [
    "### B) Reading the data from the files\n",
    "\n",
    "We parse each of the `.yaml` files.\n",
    "\n",
    "*   Concatenate two or more sentences if the answer has two or more of them.\n",
    "*   Remove unwanted data types which are produced while parsing the data.\n",
    "*   Append `<START>` and `<END>` to all the `answers`.\n",
    "*   Create a `Tokenizer` and load the whole vocabulary ( `questions` + `answers` ) into it.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mzsaO1YvS-M8"
   },
   "source": [
    "\n",
    "### C) Preparing data for Seq2Seq model\n",
    "\n",
    "Our model requires three arrays namely `encoder_input_data`, `decoder_input_data` and `decoder_output_data`.\n",
    "\n",
    "For `encoder_input_data` :\n",
    "* Tokenize the `questions`. Pad them to their maximum lquestionsth.\n",
    "\n",
    "For `decoder_input_data` :\n",
    "* Tokenize the `answers`. Pad them to their maximum lquestionsth.\n",
    "\n",
    "For `decoder_output_data` :\n",
    "\n",
    "* Tokenize the `answers`. Remove the first element from all the `tokenized_answers`. This is the `<START>` element which we added earlier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a5AD9ooQKc33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9956, 12) 12\n",
      "(9956, 14) 14\n",
      "(9956, 14, 6229)\n"
     ]
    }
   ],
   "source": [
    "questions = lines.Query \n",
    "answers = lines.Prediction\n",
    "\n",
    "# encoder_input_data\n",
    "tokenized_questions = tokenizer.texts_to_sequences( questions )\n",
    "maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n",
    "padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' )\n",
    "encoder_input_data = np.array( padded_questions )\n",
    "print( encoder_input_data.shape , maxlen_questions )\n",
    "\n",
    "# decoder_input_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "maxlen_answers = max([ len(x) for x in tokenized_answers ])\n",
    "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "decoder_input_data = np.array( padded_answers )\n",
    "print( decoder_input_data.shape , maxlen_answers )\n",
    "\n",
    "# decoder_output_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "for i in range(len(tokenized_answers)) :\n",
    "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\n",
    "decoder_output_data = np.array( onehot_answers )\n",
    "print( decoder_output_data.shape )\n",
    "\n",
    "# Saving all the arrays to storage\n",
    "#np.save( 'Saved Arrays/enc_in_data.npy' , encoder_input_data )\n",
    "#np.save( 'Saved Arrays/dec_in_data.npy' , decoder_input_data )\n",
    "#np.save( 'Saved Arrays/dec_tar_data.npy' , decoder_output_data )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4SwY3T139l19"
   },
   "source": [
    "## 3) Defining the Encoder-Decoder model\n",
    "The model will have Embedding, LSTM and Dense layers. The basic configuration is as follows.\n",
    "\n",
    "\n",
    "*   2 Input Layers : One for `encoder_input_data` and another for `decoder_input_data`.\n",
    "*   Embedding layer : For converting token vectors to fix sized dense vectors. **( Note :  Don't forget the `mask_zero=True` argument here )**\n",
    "*   LSTM layer : Provide access to Long-Short Term cells.\n",
    "\n",
    "Working : \n",
    "\n",
    "1.   The `encoder_input_data` comes in the Embedding layer (  `encoder_embedding` ). \n",
    "2.   The output of the Embedding layer goes to the LSTM cell which produces 2 state vectors ( `h` and `c` which are `encoder_states` )\n",
    "3.   These states are set in the LSTM cell of the decoder.\n",
    "4.   The decoder_input_data comes in through the Embedding layer.\n",
    "5.   The Embeddings goes in LSTM cell ( which had the states ) to produce seqeunces.\n",
    "\n",
    "**Important points :**\n",
    "\n",
    "\n",
    "*   `200` is the output of the GloVe embeddings.\n",
    "*   `embedding_matrix` is the GloVe embedding which we downloaded earlier.\n",
    "\n",
    "\n",
    "<center><img style=\"float: center;\" src=\"https://cdn-images-1.medium.com/max/1600/1*bnRvZDDapHF8Gk8soACtCQ.gif\"></center>\n",
    "\n",
    "\n",
    "Image credits to [Hackernoon](https://hackernoon.com/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-gUYtOwv21rt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 100)    622900      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    622900      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 100), (None, 80400       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 100),  80400       embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 6229)   629129      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,035,729\n",
      "Trainable params: 2,035,729\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "shape = 100\n",
    "\n",
    "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, shape , mask_zero=True ) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( shape , return_state=True )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, shape , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM( shape , return_state=True , return_sequences=True )\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n9g_8sR7WWf3"
   },
   "source": [
    "## 4) Training the model\n",
    "We train the model for a number of epochs with `RMSprop` optimizer and `categorical_crossentropy` loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N74NZnfo3Id-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "996/996 [==============================] - 26s 26ms/step - loss: 4.6889\n",
      "Epoch 2/5\n",
      "996/996 [==============================] - 25s 26ms/step - loss: 4.4408\n",
      "Epoch 3/5\n",
      "996/996 [==============================] - 26s 26ms/step - loss: 4.3236\n",
      "Epoch 4/5\n",
      "996/996 [==============================] - 25s 25ms/step - loss: 4.2058\n",
      "Epoch 5/5\n",
      "996/996 [==============================] - 26s 26ms/step - loss: 4.0936\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=10, epochs=5 ) \n",
    "model.save( 'model.h5' ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3sOLQr0M-lAe"
   },
   "source": [
    "## 5) Defining inference models\n",
    "We create inference models which help in predicting answers.\n",
    "\n",
    "**Encoder inference model** : Takes the question as input and outputs LSTM states ( `h` and `c` ).\n",
    "\n",
    "**Decoder inference model** : Takes in 2 inputs, one are the LSTM states ( Output of encoder model ), second are the answer input seqeunces ( ones not having the `<start>` tag ). It will output the answers for the question which we fed to the encoder model and its state values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1u5DE4qo3Mf2"
   },
   "outputs": [],
   "source": [
    "shape = 100\n",
    "\n",
    "def make_inference_models():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( shape ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( shape ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rxZp0ZRy-6dA"
   },
   "source": [
    "## 6) Talking with our Chatbot\n",
    "\n",
    "First, we define a method `str_to_tokens` which converts `str` questions to Integer tokens with padding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5P_wDD554q9O"
   },
   "outputs": [],
   "source": [
    "\n",
    "def str_to_tokens( sentence : str ):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    #return tokens_list\n",
    "    for word in words:\n",
    "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[138, 512,   1,  67,  24,   0,   0,   0,   0,   0,   0,   0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_to_tokens(\"会社 支給 端末 ios android\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "djEPrfJBmZE-"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "1.   First, we take a question as input and predict the state values using `enc_model`.\n",
    "2.   We set the state values in the decoder's LSTM.\n",
    "3.   Then, we generate a sequence which contains the `<start>` element.\n",
    "4.   We input this sequence in the `dec_model`.\n",
    "5.   We replace the `<start>` element with the element which was predicted by the `dec_model` and update the state values.\n",
    "6.   We carry out the above steps iteratively till we hit the `<end>` tag or the maximum answer lquestionsth.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2zBmN8qB3O-e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter question : 会社 支給 端末 ios android\n",
      " 端末 グループ 端末 グループ 端末 グループ 端末 グループ 端末 end\n",
      "Enter question : mobicontrol android setting\n",
      " 端末 ユーザ ユーザ ユーザ パスワード 入力 入力 端末 end\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/amiebot/experiment_env/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/amiebot/experiment_env/lib/python3.6/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 813\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    814\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/amiebot/experiment_env/lib/python3.6/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/amiebot/experiment_env/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-f2c5b220d3db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstates_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mstr_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'Enter question : '\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mempty_target_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mempty_target_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/amiebot/experiment_env/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         )\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/amiebot/experiment_env/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "enc_model , dec_model = make_inference_models()\n",
    "\n",
    "for _ in range(10):\n",
    "    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in tokenizer.word_index.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
    "            stop_condition = True\n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "\n",
    "    print( decoded_translation )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ChatBot_using_seq2seq.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

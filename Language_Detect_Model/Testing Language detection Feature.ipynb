{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "mecab = MeCab.Tagger('-Owakati')\n",
    "\n",
    "text = 'AndroidEnteriseの登録方法は？'\n",
    "text = mecab.parse(text).strip(\"\\n\").rstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying With fastText library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import fasttext\n",
    "model = fasttext.load_model('/home/iftekhar/AI-system/Language_Detect_Model/lid.176.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = []\n",
    "\n",
    "with open('/home/iftekhar/AI-system/Language_Detect_Model/test_nihongo.txt', 'r') as f: \n",
    "    for line in f:\n",
    "        line = re.sub(r'\\n', '', line) \n",
    "        x = re.split(\"\\s\", line)\n",
    "        k=model.predict(line, k=1)\n",
    "        ln = str(k[0][0]).replace('__label__','')\n",
    "        print(ln, \" \",x[0])\n",
    "        #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__eo',), array([0.99974376]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9b4e7f9c1851>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mln1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__label__'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mln2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__label__'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mln2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "ln1 = str(k[0][0]).replace('__label__','')\n",
    "ln2 = str(k[0][1]).replace('__label__','')\n",
    "ln2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend(list, str): \n",
    "      \n",
    "    # Using format() \n",
    "    str += '{0}'\n",
    "    list = [str.format(i) for i in list] \n",
    "    return(list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "cleaned = []\n",
    "n = 50\n",
    "with open('/home/iftekhar/AI-system/Language_Detect_Model/lang_detect_data/ja.txt', 'r') as f: \n",
    "    for line in f:\n",
    "        text = re.sub(r'\\n', '', line) \n",
    "        chunks = [text[i:i+n] for i in range(0, len(text), n)]\n",
    "        c=prepend(chunks, \"__label__jpn \")\n",
    "        cleaned.append(c)\n",
    "        #print(c)\n",
    "        #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = [x for sublist in cleaned for x in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = \"__label__jpn \"\n",
    "# my_new_list = [s + x for x in cleaned]\n",
    "\n",
    "f=open('jpn_test.txt','w')\n",
    "for ele in flat:\n",
    "    f.write(ele+'\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change the label of Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = zip([all_lebels,lines])\n",
    "\n",
    "import csv\n",
    "with open('text.csv', 'w') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    writer.writerows(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Language(alpha_3='rom', name='Romany', scope='M', type='L')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pycountry.languages.get(alpha_3='rom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pycountry \n",
    "import langcodes\n",
    "\n",
    "all_lebels = []\n",
    "lines = []\n",
    "language_names = []\n",
    "exceptional = []\n",
    "\n",
    "\n",
    "with open('/home/iftekhar/AI-system/Language_Detect_Model/languages_name.txt', 'r') as file:\n",
    "    data = file.read().replace('\\n', '')\n",
    "\n",
    "language_codes_dict = eval(data)\n",
    "\n",
    "count=0\n",
    "with open('/home/iftekhar/AI-system/Language_Detect_Model/cleaned_data.txt') as f:\n",
    "    for line in f:\n",
    "        #print(count)\n",
    "        text = line.split()\n",
    "        label = text[0]\n",
    "        label = re.sub(r'__label__','',label)\n",
    "        label = re.sub(r'\\n','',label)\n",
    "        label = label.replace('cmn','lzh')\n",
    "        label = label.replace('wuu','lzh')\n",
    "        label = label.replace('yue','lzh')\n",
    "        label = label.replace('nds','deu')\n",
    "        label = label.replace('gsw','deu')\n",
    "        label = label.replace('swg','deu')\n",
    "\n",
    "        \n",
    "        #    print(label)\n",
    "        if pycountry.languages.get(alpha_3=label) is not None:\n",
    "            lang_name = pycountry.languages.get(alpha_3=label).name\n",
    "            #print(lang_name)\n",
    "            \n",
    "\n",
    "            two_digit_code = langcodes.find(lang_name)\n",
    "\n",
    "            two_digit_code = str(two_digit_code).replace('rom','ro')\n",
    "            two_digit_code = two_digit_code.replace('hak','zh')\n",
    "            two_digit_code = two_digit_code.replace('lfn','fr')\n",
    "            two_digit_code = two_digit_code.replace('orv','ru')\n",
    "            two_digit_code = two_digit_code.replace('arq','ar')\n",
    "            two_digit_code = two_digit_code.replace('gcf','fr')\n",
    "            two_digit_code = two_digit_code.replace('mhr','ru')\n",
    "            two_digit_code = two_digit_code.replace('zsm','ms')\n",
    "            two_digit_code = two_digit_code.replace('lzh','zh')\n",
    "            \n",
    "            if len(str(two_digit_code)) != 2:\n",
    "\n",
    "                for langnames in language_codes_dict.keys():\n",
    "                    if langnames == lang_name:\n",
    "                        all_lebels.append(language_codes_dict[lang_name])                        \n",
    "                        all_text = ' '.join(text[1:])\n",
    "                        lines.append(all_text)\n",
    "                        language_names.append(lang_name)\n",
    "                        \n",
    "                        \n",
    "            else:\n",
    "                all_text = ' '.join(text[1:])\n",
    "                all_lebels.append(str(two_digit_code))\n",
    "                lines.append(all_text)\n",
    "                language_names.append(lang_name)\n",
    "            \n",
    "            count+=1\n",
    "#             if count > 100:\n",
    "#                 break\n",
    "            #print(two_digit_code, all_text )\n",
    "\n",
    "\n",
    "count=0\n",
    "holder = []\n",
    "for i in all_lebels:\n",
    "    holder.append([language_names[count],i,lines[count]])\n",
    "    count+=1\n",
    "\n",
    "import pandas as pd\n",
    "d = pd.DataFrame(holder)\n",
    "d.head()\n",
    "\n",
    "\n",
    "d.to_csv(\"out.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(str(two_digit_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lebels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit3 = []\n",
    "for index, col in d.iterrows():\n",
    "    if len(col[0]) !=2:lzh\n",
    "        digit3.append([col[0],col[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['kab', 'Kabyle'],\n",
       " ['ilo', 'Iloko'],\n",
       " ['cbk', 'Chavacano'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['jbo', 'Lojban'],\n",
       " ['jbo', 'Lojban'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['tlh', 'Klingon'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['tlh', 'Klingon'],\n",
       " ['jbo', 'Lojban'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['bar', 'Bavarian'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['jbo', 'Lojban'],\n",
       " ['gos', 'Gronings'],\n",
       " ['jbo', 'Lojban'],\n",
       " ['jbo', 'Lojban'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['lij', 'Ligurian'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['tlh', 'Klingon'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['jbo', 'Lojban'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['jbo', 'Lojban'],\n",
       " ['tlh', 'Klingon'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['bua', 'Buriat'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['jbo', 'Lojban'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['jbo', 'Lojban'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['nst', 'Tase Naga'],\n",
       " ['tlh', 'Klingon'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['tlh', 'Klingon'],\n",
       " ['tlh', 'Klingon'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['hsb', 'Upper Sorbian'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['pms', 'Piemontese'],\n",
       " ['kha', 'Khasi'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['tlh', 'Klingon'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['nst', 'Tase Naga'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['lad', 'Ladino'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['tlh', 'Klingon'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['nst', 'Tase Naga'],\n",
       " ['qya', 'Quenya'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['jbo', 'Lojban'],\n",
       " ['swg', 'Swabian'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['xal', 'Kalmyk'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['ilo', 'Iloko'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['jbo', 'Lojban'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['gos', 'Gronings'],\n",
       " ['jbo', 'Lojban'],\n",
       " ['tlh', 'Klingon'],\n",
       " ['tlh', 'Klingon'],\n",
       " ['tlh', 'Klingon'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['jbo', 'Lojban'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['jbo', 'Lojban'],\n",
       " ['zza', 'Zaza'],\n",
       " ['tlh', 'Klingon'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['tlh', 'Klingon'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['jbo', 'Lojban'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['jbo', 'Lojban'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['avk', 'Kotava'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['lvs', 'Standard Latvian'],\n",
       " ['qya', 'Quenya'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['tlh', 'Klingon'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['jbo', 'Lojban'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['tlh', 'Klingon'],\n",
       " ['avk', 'Kotava'],\n",
       " ['avk', 'Kotava'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['tlh', 'Klingon'],\n",
       " ['ceb', 'Cebuano'],\n",
       " ['rom', 'Romany'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['swg', 'Swabian'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['jbo', 'Lojban'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['hoc', 'Ho'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['jbo', 'Lojban'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['gos', 'Gronings'],\n",
       " ['tlh', 'Klingon'],\n",
       " ['ckt', 'Chukot'],\n",
       " ['hrx', 'Hunsrik'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['tlh', 'Klingon'],\n",
       " ['hrx', 'Hunsrik'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['hoc', 'Ho'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['gos', 'Gronings'],\n",
       " ['pms', 'Piemontese'],\n",
       " ['thv', 'Tahaggart Tamahaq'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['min', 'Minangkabau'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['cbk', 'Chavacano'],\n",
       " ['swg', 'Swabian'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['tlh', 'Klingon'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['jbo', 'Lojban'],\n",
       " ['tlh', 'Klingon'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['jbo', 'Lojban'],\n",
       " ['mhr', 'Eastern Mari'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['pam', 'Pampanga'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['niu', 'Niuean'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['gsw', 'Swiss German'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['jbo', 'Lojban'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['avk', 'Kotava'],\n",
       " ['tpi', 'Tok Pisin'],\n",
       " ['jbo', 'Lojban'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kha', 'Khasi'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['cbk', 'Chavacano'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['swg', 'Swabian'],\n",
       " ['zza', 'Zaza'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['tlh', 'Klingon'],\n",
       " ['jbo', 'Lojban'],\n",
       " ['avk', 'Kotava'],\n",
       " ['jbo', 'Lojban'],\n",
       " ['jbo', 'Lojban'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['bzt', 'Brithenig'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['kab', 'Kabyle'],\n",
       " ['tlh', 'Klingon']]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit3[0:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eng', 'kab', 'heb', 'ita', 'fin', 'ukr', 'eng', 'fra', 'deu', 'deu']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "labels = []\n",
    "with open('/home/iftekhar/AI-system/Language_Detect_Model/label.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        line = re.sub(r'\\n','',line)\n",
    "        labels.append(line)\n",
    "        \n",
    "labels[:10]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(set(labels))\n",
    "a.sort()\n",
    "a.pop(0)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycountry import languages\n",
    "codes = a[1:]\n",
    "codes.pop(31)\n",
    "codes.pop(62)\n",
    "codes.pop(163)\n",
    "codes.pop(208)\n",
    "codes.pop(307)\n",
    "\n",
    "m = {}\n",
    "\n",
    "lang=[]\n",
    "counter = 0\n",
    "for i in codes:\n",
    "    print(counter)\n",
    "    try:\n",
    "        language_name = languages.get(alpha_3=str(i)).name\n",
    "        m[i] = language_name\n",
    "    except TypeError:\n",
    "        pass\n",
    "        print(i)\n",
    "    counter+=1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict2 = {y:x for x,y in m.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick red fox jumps over the quick dog'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"The quick brown fox jumps over the lazy dog\"\n",
    "for r in ((\"brown\", \"red\"), (\"lazy\", \"quick\")):\n",
    "    s = s.replace(*r)\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-69514a118fde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__label__zh'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.80169469\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'array' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pycountry \n",
    "import langcodes\n",
    "\n",
    "with open('/home/iftekhar/AI-system/Language_Detect_Model/languages_name.txt', 'r') as file:\n",
    "    data = file.read().replace('\\n', '')\n",
    "language_codes_dict = eval(data)\n",
    "\n",
    "label = str(k[0][0]).replace('__label__','')\n",
    "\n",
    "if pycountry.languages.get(alpha_3=label) is not None:\n",
    "    lang_name = pycountry.languages.get(alpha_3=label).name\n",
    "    two_digit_code = langcodes.find(lang_name)\n",
    "\n",
    "    if len(str(two_digit_code)) > 2:\n",
    "\n",
    "        for langnames in language_codes_dict.keys():\n",
    "            if langnames == lang_name:\n",
    "                label = language_codes_dict[lang_name]\n",
    "    else:\n",
    "        \n",
    "        label = label.replace('cmn','zh')\n",
    "        label = label.replace('wuu','zh')\n",
    "        label = label.replace('yue','zh')\n",
    "        label = label.replace('nds','de')\n",
    "        label = label.replace('gsw','de')\n",
    "        label = label.replace('swg','de')\n",
    "        label = label.replace('rom','ro')\n",
    "        label = label.replace('hak','zh')\n",
    "        label = label.replace('lfn','fr')\n",
    "        label = label.replace('orv','ru')\n",
    "        label = label.replace('arq','ar')\n",
    "        label = label.replace('gcf','fr')\n",
    "        label = label.replace('mhr','ru')\n",
    "        label = label.replace('zsm','ms')\n",
    "        label = label.replace('lzh','zh')\n",
    "else:\n",
    "    print(\"Language code is not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pycountry \n",
    "import langcodes\n",
    "\n",
    "all_lebels = []\n",
    "lines = []\n",
    "language_names = []\n",
    "exceptional = []\n",
    "\n",
    "\n",
    "with open('/home/iftekhar/AI-system/Language_Detect_Model/languages_name.txt', 'r') as file:\n",
    "    data = file.read().replace('\\n', '')\n",
    "\n",
    "language_codes_dict = eval(data)\n",
    "\n",
    "count=0\n",
    "with open('/home/iftekhar/AI-system/Language_Detect_Model/cleaned_data.txt') as f:\n",
    "    for line in f:\n",
    "        #print(count)\n",
    "        text = line.split()\n",
    "        label = text[0]\n",
    "        label = re.sub(r'__label__','',label)\n",
    "        label = re.sub(r'\\n','',label)\n",
    "        label = label.replace('cmn','lzh')\n",
    "        label = label.replace('wuu','lzh')\n",
    "        label = label.replace('yue','lzh')\n",
    "        label = label.replace('nds','deu')\n",
    "        label = label.replace('gsw','deu')\n",
    "        label = label.replace('swg','deu')\n",
    "\n",
    "        \n",
    "        #    print(label)\n",
    "        if pycountry.languages.get(alpha_3=label) is not None:\n",
    "            lang_name = pycountry.languages.get(alpha_3=label).name\n",
    "            #print(lang_name)\n",
    "            \n",
    "\n",
    "            two_digit_code = langcodes.find(lang_name)\n",
    "\n",
    "            two_digit_code = str(two_digit_code).replace('rom','ro')\n",
    "            two_digit_code = two_digit_code.replace('hak','zh')\n",
    "            two_digit_code = two_digit_code.replace('lfn','fr')\n",
    "            two_digit_code = two_digit_code.replace('orv','ru')\n",
    "            two_digit_code = two_digit_code.replace('arq','ar')\n",
    "            two_digit_code = two_digit_code.replace('gcf','fr')\n",
    "            two_digit_code = two_digit_code.replace('mhr','ru')\n",
    "            two_digit_code = two_digit_code.replace('zsm','ms')\n",
    "            two_digit_code = two_digit_code.replace('lzh','zh')\n",
    "            \n",
    "            if len(str(two_digit_code)) != 2:\n",
    "\n",
    "                for langnames in language_codes_dict.keys():\n",
    "                    if langnames == lang_name:\n",
    "                        all_lebels.append(language_codes_dict[lang_name])                        \n",
    "                        all_text = ' '.join(text[1:])\n",
    "                        lines.append(all_text)\n",
    "                        language_names.append(lang_name)\n",
    "                        \n",
    "                        \n",
    "            else:\n",
    "                all_text = ' '.join(text[1:])\n",
    "                all_lebels.append(str(two_digit_code))\n",
    "                lines.append(all_text)\n",
    "                language_names.append(lang_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

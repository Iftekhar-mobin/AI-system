{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ifte/amiebot_project/MyAI/AI-system/chatbot/Models/Seq2Seq Models/Seq2SeqWordModel'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to install mecab\n",
    "# sudo apt install mecab mecab-ipadic-utf8\n",
    "import MeCab\n",
    "wakati = MeCab.Tagger(\"-Owakati\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "http://www.manythings.org/anki/  (Download and unzip jap-eng.zip file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ifte/amiebot_project/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ifte/amiebot_project/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ifte/amiebot_project/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ifte/amiebot_project/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ifte/amiebot_project/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ifte/amiebot_project/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ifte/amiebot_project/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ifte/amiebot_project/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ifte/amiebot_project/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ifte/amiebot_project/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ifte/amiebot_project/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ifte/amiebot_project/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines= pd.read_table('jpn.txt', names=['eng', 'jap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45093, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45088    「恋人にさよならのキスくらいしないと、キスしてあばよ、ということになるぞ」とトムは言ったが、...\n",
       "45089    日本ではリヒター・スケールでマグニチュード5.0以上の余震が今週5回ありました。しかし科学者...\n",
       "45090    ただ今到着のバスは、国内線第1ターミナル行きです。国際線ターミナルにお越しの方は、しばらくそ...\n",
       "45091    ネイティブの子どもは、何年も学んだ非ネイティブが知らず今後も知り得ないたくさんのことを自身の...\n",
       "45092    色々並行してやってるから芥川ばかり読んでるのでもないのだよ。今は英語読んでる時間が増えてる。...\n",
       "Name: jap, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.tail(5)\n",
    "lines.jap.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase all characters\n",
    "lines.eng=lines.eng.apply(lambda x: x.lower())\n",
    "#lines.jap=lines.jap.apply(lambda x: x.lower())\n",
    "lines.jap = lines.jap.apply(lambda x: wakati.parse(x).strip(\"\\n\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45088    「 恋人 に さよなら の キス くらい し ない と 、 キス し て あ ば よ 、 と...\n",
       "45089    日本 で は リヒター ・ スケール で マグニチュード 5 . 0 以上 の 余震 が 今...\n",
       "45090    ただ 今 到着 の バス は 、 国内線 第 1 ターミナル 行き です 。 国際線 ターミ...\n",
       "45091    ネイティブ の 子ども は 、 何 年 も 学ん だ 非 ネイティブ が 知ら ず 今後 も...\n",
       "45092    色々 並行 し て やっ てる から 芥川 ばかり 読ん でる の で も ない の だ よ...\n",
       "Name: jap, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.tail(5)\n",
    "lines.jap.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove quotes\n",
    "lines.eng=lines.eng.apply(lambda x: re.sub(\"'\", '', x))\n",
    "lines.jap=lines.jap.apply(lambda x: re.sub(\"'\", '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set(string.punctuation) # Set of all special characters\n",
    "# Remove all the special characters\n",
    "lines.eng=lines.eng.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "lines.jap=lines.jap.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all numbers from text\n",
    "#remove_digits = str.maketrans('', '', digits)\n",
    "#lines.eng=lines.eng.apply(lambda x: x.translate(remove_digits))\n",
    "#lines.jap = lines.jap.apply(lambda x: re.sub(\"[123456789]\", \"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extra spaces\n",
    "lines.eng=lines.eng.apply(lambda x: x.strip())\n",
    "lines.jap=lines.jap.apply(lambda x: x.strip())\n",
    "lines.eng=lines.eng.apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "lines.jap=lines.jap.apply(lambda x: re.sub(\" +\", \" \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add start and end tokens to target sequences\n",
    "lines.jap = lines.jap.apply(lambda x : 'START '+ x + ' END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>jap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8877</td>\n",
       "      <td>it only costs 1000</td>\n",
       "      <td>START たった １ ０ ドル だ ぜ 。 END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4215</td>\n",
       "      <td>i bought a watch</td>\n",
       "      <td>START 私 は 腕時計 を 買っ た 。 END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6357</td>\n",
       "      <td>i was having lunch</td>\n",
       "      <td>START 昼 ごはん を 食べ て た ん です 。 END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3581</td>\n",
       "      <td>im very sleepy</td>\n",
       "      <td>START すごく 眠い 。 END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11395</td>\n",
       "      <td>i feel guilty about it</td>\n",
       "      <td>START 私 は その こと で 気 が とがめ て いる 。 END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2906</td>\n",
       "      <td>im not afraid</td>\n",
       "      <td>START 怖く なんて ない さ 。 END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28370</td>\n",
       "      <td>she went to the art museum by taxi</td>\n",
       "      <td>START 彼女 は タクシー で 美術館 に 行っ た 。 END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36040</td>\n",
       "      <td>unfortunately the information is accurate</td>\n",
       "      <td>START その 報告 は 残念 ながら 事実 だ 。 END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5767</td>\n",
       "      <td>tomorrows payday</td>\n",
       "      <td>START 明日 は 給料 日 だ 。 END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22636</td>\n",
       "      <td>i lived overseas for ten years</td>\n",
       "      <td>START 私 は 十 年間 海外 で 生活 し た 。 END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             eng  \\\n",
       "8877                          it only costs 1000   \n",
       "4215                            i bought a watch   \n",
       "6357                          i was having lunch   \n",
       "3581                              im very sleepy   \n",
       "11395                     i feel guilty about it   \n",
       "2906                               im not afraid   \n",
       "28370         she went to the art museum by taxi   \n",
       "36040  unfortunately the information is accurate   \n",
       "5767                            tomorrows payday   \n",
       "22636             i lived overseas for ten years   \n",
       "\n",
       "                                        jap  \n",
       "8877             START たった １ ０ ドル だ ぜ 。 END  \n",
       "4215             START 私 は 腕時計 を 買っ た 。 END  \n",
       "6357        START 昼 ごはん を 食べ て た ん です 。 END  \n",
       "3581                     START すごく 眠い 。 END  \n",
       "11395  START 私 は その こと で 気 が とがめ て いる 。 END  \n",
       "2906                START 怖く なんて ない さ 。 END  \n",
       "28370    START 彼女 は タクシー で 美術館 に 行っ た 。 END  \n",
       "36040       START その 報告 は 残念 ながら 事実 だ 。 END  \n",
       "5767                START 明日 は 給料 日 だ 。 END  \n",
       "22636      START 私 は 十 年間 海外 で 生活 し た 。 END  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary of English\n",
    "all_eng_words=set()\n",
    "for eng in lines.eng:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "\n",
    "# Vocabulary of Japanese \n",
    "all_japanese_words=set()\n",
    "for jap in lines.jap:\n",
    "    for word in jap.split():\n",
    "        if word not in all_japanese_words:\n",
    "            all_japanese_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45093,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.jap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max Length of source sequence\n",
    "lenght_list=[]\n",
    "for l in lines.eng:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "max_length_src = np.max(lenght_list)\n",
    "max_length_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max Length of target sequence\n",
    "lenght_list=[]\n",
    "for l in lines.jap:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "max_length_tar = np.max(lenght_list)\n",
    "max_length_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9386, 13905)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_japanese_words))\n",
    "num_encoder_tokens = len(all_eng_words)\n",
    "num_decoder_tokens = len(all_japanese_words)\n",
    "num_encoder_tokens, num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13906"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_decoder_tokens += 1 # For zero padding\n",
    "num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>jap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8124</td>\n",
       "      <td>where are you going</td>\n",
       "      <td>START あなた は どこ に 行く の です か ？ END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39770</td>\n",
       "      <td>since id never met her before i didnt know her</td>\n",
       "      <td>START 以前 会っ た こと が なかっ た ので 、 私 は 彼女 が わから なかっ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12111</td>\n",
       "      <td>tom likes swiss cheese</td>\n",
       "      <td>START トム は エメンタールチーズ が 好き だ 。 END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38579</td>\n",
       "      <td>an old man was resting in the shade of the tree</td>\n",
       "      <td>START おじいさん が 木陰 で 休ん で い まし た 。 END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13884</td>\n",
       "      <td>he appeared from nowhere</td>\n",
       "      <td>START 彼 は どこ から とも なく 現れ た 。 END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12646</td>\n",
       "      <td>he stamped out the fire</td>\n",
       "      <td>START 彼 は 火 を 踏み 消し た 。 END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4998</td>\n",
       "      <td>he got well again</td>\n",
       "      <td>START 彼 は また 元気 に なっ た 。 END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3955</td>\n",
       "      <td>youre so bossy</td>\n",
       "      <td>START お前 、 随分 偉 そう だ な 。 END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19993</td>\n",
       "      <td>i would rather have a coffee</td>\n",
       "      <td>START 私 は 、 むしろ コーヒー を 飲み たい 。 END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3075</td>\n",
       "      <td>she sings well</td>\n",
       "      <td>START 彼女 は 歌 が うまい 。 END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   eng  \\\n",
       "8124                               where are you going   \n",
       "39770   since id never met her before i didnt know her   \n",
       "12111                           tom likes swiss cheese   \n",
       "38579  an old man was resting in the shade of the tree   \n",
       "13884                         he appeared from nowhere   \n",
       "12646                          he stamped out the fire   \n",
       "4998                                 he got well again   \n",
       "3955                                    youre so bossy   \n",
       "19993                     i would rather have a coffee   \n",
       "3075                                    she sings well   \n",
       "\n",
       "                                                     jap  \n",
       "8124                    START あなた は どこ に 行く の です か ？ END  \n",
       "39770  START 以前 会っ た こと が なかっ た ので 、 私 は 彼女 が わから なかっ...  \n",
       "12111                  START トム は エメンタールチーズ が 好き だ 。 END  \n",
       "38579               START おじいさん が 木陰 で 休ん で い まし た 。 END  \n",
       "13884                   START 彼 は どこ から とも なく 現れ た 。 END  \n",
       "12646                        START 彼 は 火 を 踏み 消し た 。 END  \n",
       "4998                        START 彼 は また 元気 に なっ た 。 END  \n",
       "3955                        START お前 、 随分 偉 そう だ な 。 END  \n",
       "19993                 START 私 は 、 むしろ コーヒー を 飲み たい 。 END  \n",
       "3075                            START 彼女 は 歌 が うまい 。 END  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = shuffle(lines)\n",
    "lines.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40583,), (4510,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train - Test Split\n",
    "X, y = lines.eng, lines.jap\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the train and test dataframes for reproducing the results later, as they are shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_pickle('Weights/X_train.pkl')\n",
    "X_test.to_pickle('Weights/X_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
    "    ''' Generate a batch of data '''\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n",
    "                    if t>0:\n",
    "                        # decoder target sequence (one hot encoded)\n",
    "                        # does not include the START_ token\n",
    "                        # Offset by one timestep\n",
    "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder - Decoder Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ifte/amiebot_project/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(num_encoder_tokens, latent_dim, mask_zero = True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import Image\n",
    "#Image(retina=True, filename='train_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "batch_size = 128\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ifte/amiebot_project/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/50\n",
      "  2/317 [..............................] - ETA: 36:34 - loss: 1.7535"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-b22c3ec5e69e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     validation_steps = val_samples//batch_size)\n\u001b[0m",
      "\u001b[0;32m~/amiebot_project/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/amiebot_project/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/amiebot_project/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/amiebot_project/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/amiebot_project/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/amiebot_project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n",
    "                    steps_per_epoch = train_samples//batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "                    validation_steps = val_samples//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Always remember to save the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('nmt_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the weights, if you close the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('nmt_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the \"thought vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(retina=True, filename='encoder_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(retina=True, filename='decoder_model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decode sample sequeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_END' or\n",
    "           len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
    "k=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gen = generate_batch(X_test, y_test, batch_size = 1)\n",
    "k=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Japanese Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Japanese Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'English_for_Today.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "npwordsList = np.asarray(wordsList) \n",
    "\n",
    "encoded = LabelEncoder()\n",
    "e = encoded.fit_transform(npwordsList)\n",
    "\n",
    "# we can also use padding technique here to make the length of number of token even number\n",
    "# shaping according to desired pattern samples, Timestep, Predicted output length \n",
    "d = e[:-2].reshape(npwordsList[:len(npwordsList)-2].shape[0]//10,10)\n",
    "d\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "#Index(['Col1(C-3)', 'Col2(C-3)', 'Col3(C-3)', 'Col4(C-3)', 'Col5(C-3)',\n",
    "#       'Col6(C-3)', 'Col7(C-3)', 'Col8(C-3)', 'Col9(C-3)', 'Col10(C-3)',\n",
    "#       'Col1(C-2)', 'Col2(C-2)', 'Col3(C-2)', 'Col4(C-2)', 'Col5(C-2)',\n",
    "#       'Col6(C-2)', 'Col7(C-2)', 'Col8(C-2)', 'Col9(C-2)', 'Col10(C-2)',\n",
    "#       'Col1(C-1)', 'Col2(C-1)', 'Col3(C-1)', 'Col4(C-1)', 'Col5(C-1)',\n",
    "#       'Col6(C-1)', 'Col7(C-1)', 'Col8(C-1)', 'Col9(C-1)', 'Col10(C-1)',\n",
    "#       'Col1(C)', 'Col2(C)', 'Col3(C)', 'Col4(C)', 'Col5(C)', 'Col6(C)',\n",
    "#       'Col7(C)', 'Col8(C)', 'Col9(C)', 'Col10(C)', 'Col1(C+1)', 'Col2(C+1)',\n",
    "#       'Col3(C+1)', 'Col4(C+1)', 'Col5(C+1)', 'Col6(C+1)', 'Col7(C+1)',\n",
    "#       'Col8(C+1)', 'Col9(C+1)', 'Col10(C+1)', 'Col1(C+2)', 'Col2(C+2)',\n",
    "#       'Col3(C+2)', 'Col4(C+2)', 'Col5(C+2)', 'Col6(C+2)', 'Col7(C+2)',\n",
    "#       'Col8(C+2)', 'Col9(C+2)', 'Col10(C+2)', 'Col1(C+3)', 'Col2(C+3)',\n",
    "#       'Col3(C+3)', 'Col4(C+3)', 'Col5(C+3)', 'Col6(C+3)', 'Col7(C+3)',\n",
    "#       'Col8(C+3)', 'Col9(C+3)', 'Col10(C+3)', 'Col1(C+4)', 'Col2(C+4)',\n",
    "#       'Col3(C+4)', 'Col4(C+4)', 'Col5(C+4)', 'Col6(C+4)', 'Col7(C+4)',\n",
    "#       'Col8(C+4)', 'Col9(C+4)', 'Col10(C+4)'],\n",
    "#      dtype='object')\n",
    "        \n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = pd.DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('Col%d(C-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('Col%d(C)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('Col%d(C+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = pd.concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    "\n",
    "n_hours = 3\n",
    "n_features = 5\n",
    "n_ahead = 10\n",
    "\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(d, n_hours, n_features)\n",
    "\n",
    "st = (n_hours+1)*n_ahead + 1  \n",
    "\n",
    "# drop columns we don't want to predict\n",
    "deletedcol =    list(reframed.columns)[st           :  st+n_ahead-1] +                 list(reframed.columns)[st+n_ahead   :  st+n_ahead*2-1] +                 list(reframed.columns)[st+n_ahead*2 :  st+n_ahead*3-1] +                 list(reframed.columns)[st+n_ahead*3 : ] \n",
    "#print(\"deleted column\",deletedcol)\n",
    "\n",
    "reframed.drop(deletedcol, axis=1, inplace=True)\n",
    "#print(reframed.head())\n",
    "\n",
    "#reframed.to_csv(r'outNum.csv')\n",
    "\n",
    "\n",
    "# In[52]:\n",
    "\n",
    "\n",
    "# decoding to see the formation\n",
    "\n",
    "\n",
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "train_size = int(len(values) * 0.8)\n",
    "test_size = len(values) - train_size\n",
    "train, test = values[0:train_size,:], values[train_size:,:]\n",
    "\n",
    "input_col = (n_hours+1)*n_ahead\n",
    "\n",
    "## split into input and outputs\n",
    "train_X, train_y = train[:, :input_col], train[:, -4:]\n",
    "test_X, test_y = test[:, :input_col], test[:, -4:]\n",
    "#print(train_X, train_X.shape, train_y, train_y.shape)\n",
    "\n",
    "\n",
    "# In[53]:\n",
    "\n",
    "\n",
    "print(train_X.shape, test_y.shape)\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "\n",
    "\n",
    "# In[57]:\n",
    "\n",
    "\n",
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(160, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(4))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=100, batch_size=40, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "\n",
    "\n",
    "# In[62]:\n",
    "\n",
    "\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "\n",
    "# In[63]:\n",
    "\n",
    "\n",
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "yhat\n",
    "\n",
    "\n",
    "# In[66]:\n",
    "\n",
    "\n",
    "yhat = yhat.reshape(-1)\n",
    "print(yhat.shape)\n",
    "encoded.inverse_transform()\n",
    "\n",
    "\n",
    "# In[48]:\n",
    "\n",
    "\n",
    "\n",
    "#test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))\n",
    "# invert scaling for forecast\n",
    "#inv_yhat = concatenate((yhat, test_X[:, -7:]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "#inv_y = concatenate((test_y, test_X[:, -7:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
